====== JOB SCRIPT ======
#!/bin/bash
#SBATCH -A LRN070
#SBATCH -J HydraGNN
#SBATCH -o SC25-multibranch-%j.out
#SBATCH -e SC25-multibranch-%j.out
#SBATCH -t 01:30:00
#SBATCH -p batch
#SBATCH -C nvme
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
##SBATCH -q debug
#SBATCH -N 5
##SBATCH -S 1

echo "====== JOB SCRIPT ======"
cat "$0"
echo "====== END OF JOB SCRIPT ======"
 
# Load conda environemnt
source /lustre/orion/lrn070/world-shared/mlupopa/module-to-load-frontier-rocm624.sh
source /lustre/orion/lrn070/world-shared/mlupopa/max_conda_envs_frontier/bin/activate
conda activate hydragnn_rocm624
 
#export python path to use ADIOS2 v.2.9.2
export PYTHONPATH=/lustre/orion/lrn070/world-shared/mlupopa/ADIOS_ROCm624/adios2-install/lib/python3.11/site-packages/:$PYTHONPATH

which python
python -c "import numpy; print(numpy.__version__)"


echo $LD_LIBRARY_PATH  | tr ':' '\n'

export MPICH_ENV_DISPLAY=0
export MPICH_VERSION_DISPLAY=0
export MIOPEN_DISABLE_CACHE=1
export NCCL_PROTO=Simple

export OMP_NUM_THREADS=7
export HYDRAGNN_NUM_WORKERS=0
export HYDRAGNN_USE_VARIABLE_GRAPH_SIZE=1
export HYDRAGNN_AGGR_BACKEND=mpi
export HYDRAGNN_VALTEST=0

export NCCL_P2P_LEVEL=NVL
export NCCL_P2P_DISABLE=1

export HYDRAGNN_VALTEST=0

## Checking
env | grep ROCM
env | grep ^MI
env | grep ^MPICH
env | grep ^HYDRA


# export datadir0=/lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/ANI1x-v3.bp
# export datadir1=/lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/qm7x-v3.bp
# export datadir2=/lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/MPTrj-v3.bp
# export datadir3=/lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/Alexandria-v3.bp
# export datadir4=/lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/transition1x-v3.bp


set -x
srun -N $SLURM_JOB_NUM_NODES -n $SLURM_JOB_NUM_NODES rm -rf /mnt/bb/kmehta/*
time srun -N $SLURM_JOB_NUM_NODES -n $SLURM_JOB_NUM_NODES cp -r /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/*-v3.bp /mnt/bb/kmehta/.
set +x
if [ $? -ne 0 ]; then
    echo "Could not load data into memory. Exiting"
    exit 1
fi

export datadir0=/mnt/bb/kmehta/ANI1x-v3.bp
export datadir1=/mnt/bb/kmehta/qm7x-v3.bp
export datadir2=/mnt/bb/kmehta/MPTrj-v3.bp
export datadir3=/mnt/bb/kmehta/Alexandria-v3.bp
export datadir4=/mnt/bb/kmehta/transition1x-v3.bp


echo "Launching HydraGNN for baseline tests at `date`"
for batch_size in 32; do
    for count in 1 2 3 4 5; do
        export NTOTPROCS=$((SLURM_JOB_NUM_NODES*8))
        export HYDRAGNN_MAX_NUM_BATCH=50
        export BATCH_SIZE=$batch_size
        export EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE* NTOTPROCS))
        export LOGDIRNAME="SC25_multibranch_weakscaling_JOB${SLURM_JOB_ID}_N${SLURM_JOB_NUM_NODES}_NPROCS${NTOTPROCS}_EBS${EFFECTIVE_BATCH_SIZE}_LBS${BATCH_SIZE}_MaxNumBatch${HYDRAGNN_MAX_NUM_BATCH}_TP0_BASELINE_${count}"
        GPSUMMARY=/lustre/orion/lrn070/world-shared/kmehta/hydragnn-sc25/HydraGNN-pzhang/logs/$LOGDIRNAME
        
        if [ "$BATCH_SIZE" -lt 16 ] || [ "$BATCH_SIZE" -gt 2048 ]; then
            continue
        fi
        echo "Node count: $SLURM_JOB_NUM_NODES, num_processes: $NTOTPROCS, effective batch size: $EFFECTIVE_BATCH_SIZE, local batch size: $BATCH_SIZE"
        # this test was already performed
        if [ -e $GPSUMMARY ]; then
            echo "Skipping $GPSUMMARY as it is already done."
            continue
        fi

        echo "Node count: $SLURM_JOB_NUM_NODES, num_processes: $NTOTPROCS, effective batch size: $EFFECTIVE_BATCH_SIZE, local batch size: $BATCH_SIZE"
        echo "Experiment started at `date`"

        set -x

        timeout --signal=TERM --kill-after=10s 10m srun -n$((SLURM_JOB_NUM_NODES*8)) --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=$LOGDIRNAME \
        --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=$datadir0,$datadir1,$datadir2,$datadir3,$datadir4 --num_samples=$((EFFECTIVE_BATCH_SIZE*HYDRAGNN_MAX_NUM_BATCH)) \
        --everyone --batch_size=${BATCH_SIZE} --num_epoch=5

        set +x
        echo "Experiment ended at `date`"
    done
done
echo "Baseline tests with HydraGNN complete at `date`"


echo "Launching gaussian calculations at `date`"
bash /lustre/orion/lrn070/world-shared/kmehta/pubchem-interference-test/pubchem-gaussian/src/gaussian/run.sl &
GAUSSIAN_PID=$!

sleep 600

echo "Launching HydraGNN for interference tests at `date`"
for batch_size in 32; do
    for count in 1 2 3 4 5; do
        export NTOTPROCS=$((SLURM_JOB_NUM_NODES*8))
        export HYDRAGNN_MAX_NUM_BATCH=50
        export BATCH_SIZE=$batch_size
        export EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE* NTOTPROCS))
        export LOGDIRNAME="SC25_multibranch_weakscaling_JOB${SLURM_JOB_ID}_N${SLURM_JOB_NUM_NODES}_NPROCS${NTOTPROCS}_EBS${EFFECTIVE_BATCH_SIZE}_LBS${BATCH_SIZE}_MaxNumBatch${HYDRAGNN_MAX_NUM_BATCH}_TP0_INTERFERENCE_${count}"
        GPSUMMARY=/lustre/orion/lrn070/world-shared/kmehta/hydragnn-sc25/HydraGNN-pzhang/logs/$LOGDIRNAME
        
        if [ "$BATCH_SIZE" -lt 16 ] || [ "$BATCH_SIZE" -gt 2048 ]; then
            continue
        fi
        echo "Node count: $SLURM_JOB_NUM_NODES, num_processes: $NTOTPROCS, effective batch size: $EFFECTIVE_BATCH_SIZE, local batch size: $BATCH_SIZE"
        # this test was already performed
        if [ -e $GPSUMMARY ]; then
            echo "Skipping $GPSUMMARY as it is already done."
            continue
        fi

        echo "Node count: $SLURM_JOB_NUM_NODES, num_processes: $NTOTPROCS, effective batch size: $EFFECTIVE_BATCH_SIZE, local batch size: $BATCH_SIZE"
        echo "Experiment started at `date`"

        set -x

        timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n$((SLURM_JOB_NUM_NODES*8)) --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=$LOGDIRNAME \
        --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=$datadir0,$datadir1,$datadir2,$datadir3,$datadir4 --num_samples=$((EFFECTIVE_BATCH_SIZE*HYDRAGNN_MAX_NUM_BATCH)) \
        --everyone --batch_size=${BATCH_SIZE} --num_epoch=5 

        set +x
        echo "Experiment ended at `date`"
    done
done
echo "Interference tests with HydraGNN complete at `date`"

echo "Killing gaussian"
kill $GAUSSIAN_PID

echo "ALL DONE"

====== END OF JOB SCRIPT ======
Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: /opt/cray/pe/lmod/modulefiles/mpi/amd/4.0/ofi/1.0/cray-mpich/8.0
-------------------------------------------------------------------------------
There are messages associated with the following module(s):
-------------------------------------------------------------------------------

DefApps:
   Join us for the 2025 OLCF User Meeting, taking place August 5th and 6th
   at Oak Ridge National Laboratory and virtually. Register by July 25th:
   https://www.olcf.ornl.gov/olcf-user-meeting/register/

-------------------------------------------------------------------------------


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31


The following have been reloaded with a version change:
  1) PrgEnv-cray/8.6.0 => PrgEnv-cray/8.5.0
  2) cce/18.0.1 => cce/18.0.0
  3) cray-libsci/24.11.0 => cray-libsci/24.07.0
  4) cray-mpich/8.1.31 => cray-mpich/8.1.30
  5) cray-pmi/6.1.15 => cray-pmi/6.1.15.21
  6) craype/2.7.33 => craype/2.7.32
  7) darshan-runtime/3.4.6-mpi => darshan-runtime/3.4.4-mpi
  8) perftools-base/24.11.0 => perftools-base/24.07.0

Lmod Warning:
================================================================================

    Mixed compiler modules are being deprecated in a future release.
    For more information run: module help amd-mixed

================================================================================


While processing the following module(s):
    Module fullname  Module Filename
    ---------------  ---------------
    amd-mixed/6.2.4  /opt/cray/pe/lmod/modulefiles/mix_compilers/amd-mixed/6.2.4.lua


Lmod is automatically replacing "cce/18.0.0" with "gcc-native/13.2".


Lmod is automatically replacing "PrgEnv-cray/8.5.0" with "PrgEnv-gnu/8.5.0".

Lmod Warning:
================================================================================

    Mixed compiler modules are being deprecated in a future release.
    For more information run: module help amd-mixed

================================================================================


While processing the following module(s):
    Module fullname  Module Filename
    ---------------  ---------------
    amd-mixed/6.2.4  /opt/cray/pe/lmod/modulefiles/mix_compilers/amd-mixed/6.2.4.lua


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.07.0     2) cray-mpich/8.1.30     3) darshan-runtime/3.4.4-mpi

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "cmake/3.27.9"
   Try: "module spider cmake/3.27.9" to see how to load the module(s).



Lmod Warning:
================================================================================

    Mixed compiler modules are being deprecated in a future release.
    For more information run: module help amd-mixed

================================================================================


While processing the following module(s):
    Module fullname  Module Filename
    ---------------  ---------------
    amd-mixed/6.2.4  /opt/cray/pe/lmod/modulefiles/mix_compilers/amd-mixed/6.2.4.lua

/lustre/orion/lrn070/world-shared/mlupopa/max_conda_envs_frontier/envs/hydragnn_rocm624/bin/python
1.26.4
/opt/cray/pe/libsci/24.07.0/GNU/12.3/x86_64/lib
/opt/cray/pe/mpich/8.1.30/ofi/gnu/12.3/lib
/opt/cray/pe/mpich/8.1.30/gtl/lib
/opt/cray/pe/dsmml/0.3.0/dsmml/lib
/opt/cray/pe/perftools/24.07.0/lib64
/opt/cray/pe/pmi/6.1.15.21/lib
/opt/rocm-6.2.4/lib
/opt/rocm-6.2.4/llvm/lib
/opt/rocm-6.2.4/lib/roctracer
/opt/rocm-6.2.4/lib/rocprofiler
/opt/cray/pe/papi/7.1.0.2/lib64
/opt/cray/libfabric/1.22.0/lib64
PE_PRODUCT_LIST=CRAYPE:CRAY_PMI:CRAYPE_X86_TRENTO:PERFTOOLS:CRAYPAT:CRAY_ROCM:CRAY_ACCEL
ROCM_PATH=/opt/rocm-6.2.4
CRAY_ROCM_POST_LINK_OPTS= -L/opt/rocm-6.2.4/lib -L/opt/rocm-6.2.4/lib/rocprofiler -L/opt/rocm-6.2.4/lib/roctracer -lamdhip64
CRAY_ROCM_PREFIX=/opt/rocm-6.2.4
CRAY_ROCM_DIR=/opt/rocm-6.2.4
CRAY_ROCM_VERSION=6.2.4
CRAY_ROCM_INCLUDE_OPTS=-I/opt/rocm-6.2.4/include -I/opt/rocm-6.2.4/include/rocprofiler -I/opt/rocm-6.2.4/include/roctracer -I/opt/rocm-6.2.4/include/hip -D__HIP_PLATFORM_AMD__
__LMOD_REF_COUNT_PE_PRODUCT_LIST=CRAYPE:1;CRAY_PMI:1;CRAYPE_X86_TRENTO:1;PERFTOOLS:1;CRAYPAT:1;CRAY_ROCM:1;CRAY_ACCEL:1
MINICOM=-c on
MIOPEN_DISABLE_CACHE=1
MPICH_ENV_DISPLAY=0
MPICH_OFI_NIC_POLICY=NUMA
MPICH_VERSION_DISPLAY=0
MPICH_DIR=/opt/cray/pe/mpich/8.1.30/ofi/gnu/12.3
HYDRA_LAUNCHER_EXTRA_ARGS=--external-launcher
HYDRAGNN_VALTEST=0
HYDRAGNN_AGGR_BACKEND=mpi
HYDRAGNN_NUM_WORKERS=0
HYDRA_BOOTSTRAP=slurm
HYDRAGNN_USE_VARIABLE_GRAPH_SIZE=1
+ srun -N 5 -n 5 rm -rf '/mnt/bb/kmehta/*'
+ srun -N 5 -n 5 cp -r /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/Alexandria-v3.bp /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/ANI1x-v3.bp /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/MPTrj-v3.bp /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/qm7x-v3.bp /lustre/orion/world-shared/lrn070/HydraGNN-sc25-comm/transition1x-v3.bp /mnt/bb/kmehta/.

real	1m52.227s
user	0m0.012s
sys	0m0.017s
+ set +x
Launching HydraGNN for baseline tests at Thu Apr 24 05:39:35 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 05:39:35 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_1 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 17:39:57.924037105 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.924035102 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.924043267 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.924040642 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.926019507 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.926056086 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.927588303 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.927841714 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.272232540 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.272236497 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.272231718 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.275181928 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.277659883 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.280110616 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.283931826 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.285826787 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.039978687 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.039980721 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.042838247 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.046737102 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.049268320 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.051620108 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.053119184 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:57.054702750 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 17:39:58.788350157 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.789087879 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.792245018 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.796279546 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.797410605 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.798814283 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.800022348 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.800298203 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556636123 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556620253 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556621715 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556628308 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556616756 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556624030 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.556704462 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:39:58.621810144 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_1 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Read attr time (sec):  0.0034515857696533203
0: read and bcast: trainset/x/variable_count 0.20077991485595703
0: read and bcast: trainset/x/variable_offset 0.4271817207336426
0: read and bcast: trainset/x/variable_dim 0.4286355972290039
0: read and bcast: trainset/edge_index/variable_count 0.63826584815979
16 mptrj nsplit: 1422346 2 711173
17 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_offset 0.8742783069610596
0: read and bcast: trainset/edge_index/variable_dim 0.9005050659179688
0: read and bcast: trainset/edge_attr/variable_count 1.0872604846954346
0: read and bcast: trainset/edge_attr/variable_offset 1.316542148590088
0: read and bcast: trainset/edge_attr/variable_dim 1.3422784805297852
0: read and bcast: trainset/pos/variable_count 1.538269281387329
0: read and bcast: trainset/pos/variable_offset 1.7458961009979248
0: read and bcast: trainset/pos/variable_dim 1.7936418056488037
0: read and bcast: trainset/energy/variable_count 1.9746010303497314
0: read and bcast: trainset/energy/variable_offset 2.162611961364746
0: read and bcast: trainset/energy/variable_dim 2.2110869884490967
0: read and bcast: trainset/forces/variable_count 2.3897178173065186
0: read and bcast: trainset/forces/variable_offset 2.5882303714752197
0: read and bcast: trainset/forces/variable_dim 2.6128666400909424
0: read and bcast: trainset/y/variable_count 2.79571533203125
0: read and bcast: trainset/y/variable_offset 3.0326459407806396
0: read and bcast: trainset/y/variable_dim 3.064009428024292
0: Overall time (sec):  3.0677006244659424
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  3.074204921722412
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0009722709655761719
0: read and bcast: valset/x/variable_count 0.008437156677246094
0: read and bcast: valset/x/variable_offset 0.015560626983642578
0: read and bcast: valset/x/variable_dim 0.015734434127807617
0: read and bcast: valset/edge_index/variable_count 0.022946834564208984
0: read and bcast: valset/edge_index/variable_offset 0.03002142906188965
0: read and bcast: valset/edge_index/variable_dim 0.030189037322998047
0: read and bcast: valset/edge_attr/variable_count 0.03724956512451172
0: read and bcast: valset/edge_attr/variable_offset 0.044428348541259766
0: read and bcast: valset/edge_attr/variable_dim 0.044594526290893555
0: read and bcast: valset/pos/variable_count 0.051712751388549805
0: read and bcast: valset/pos/variable_offset 0.058737754821777344
0: read and bcast: valset/pos/variable_dim 0.05889487266540527
0: read and bcast: valset/energy/variable_count 0.06595110893249512
0: read and bcast: valset/energy/variable_offset 0.07275509834289551
0: read and bcast: valset/energy/variable_dim 0.07292056083679199
0: read and bcast: valset/forces/variable_count 0.07980060577392578
0: read and bcast: valset/forces/variable_offset 0.08673334121704102
0: read and bcast: valset/forces/variable_dim 0.0868995189666748
0: read and bcast: valset/y/variable_count 0.09371089935302734
0: read and bcast: valset/y/variable_offset 0.10050225257873535
0: read and bcast: valset/y/variable_dim 0.10065770149230957
0: Overall time (sec):  0.10182952880859375
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10462832450866699
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007302761077880859
0: read and bcast: testset/x/variable_count 0.006997346878051758
0: read and bcast: testset/x/variable_offset 0.014247655868530273
0: read and bcast: testset/x/variable_dim 0.01442861557006836
0: read and bcast: testset/edge_index/variable_count 0.02138686180114746
0: read and bcast: testset/edge_index/variable_offset 0.02839803695678711
0: read and bcast: testset/edge_index/variable_dim 0.02858567237854004
0: read and bcast: testset/edge_attr/variable_count 0.0354764461517334
0: read and bcast: testset/edge_attr/variable_offset 0.04231381416320801
0: read and bcast: testset/edge_attr/variable_dim 0.042474985122680664
0: read and bcast: testset/pos/variable_count 0.04938864707946777
0: read and bcast: testset/pos/variable_offset 0.05626225471496582
0: read and bcast: testset/pos/variable_dim 0.05643486976623535
0: read and bcast: testset/energy/variable_count 0.06326508522033691
0: read and bcast: testset/energy/variable_offset 0.07003211975097656
0: read and bcast: testset/energy/variable_dim 0.07018756866455078
0: read and bcast: testset/forces/variable_count 0.07684087753295898
0: read and bcast: testset/forces/variable_offset 0.0835871696472168
0: read and bcast: testset/forces/variable_dim 0.08376026153564453
0: read and bcast: testset/y/variable_count 0.09053826332092285
0: read and bcast: testset/y/variable_offset 0.09742283821105957
0: read and bcast: testset/y/variable_dim 0.09759664535522461
0: Overall time (sec):  0.09851598739624023
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10133814811706543
0 ani1x nsplit: 4460384 6 743398
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
8 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
32 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
34 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
36 transition1x nsplit: 8680250 11 789113
29 transition1x nsplit: 8680250 11 789114
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
20 alexandria nsplit: 9705384 11 882308
21 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
23 alexandria nsplit: 9705384 11 882308
19 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
26 alexandria nsplit: 9705384 11 882307
28 alexandria nsplit: 9705384 11 882307
24 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
27 alexandria nsplit: 9705384 11 882307
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
0: Adios reading time (sec):  0.60416579246521
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
0: Adios reading time (sec):  0.274489164352417
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.2829608917236328
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 85/64000 [00:00<01:15, 847.52it/s]Loading:   1%|          | 343/64000 [00:00<00:34, 1863.00it/s]Loading:   1%|          | 606/64000 [00:00<00:28, 2209.51it/s]Loading:   1%|▏         | 870/64000 [00:00<00:26, 2378.51it/s]Loading:   2%|▏         | 1131/64000 [00:00<00:25, 2460.77it/s]Loading:   2%|▏         | 1390/64000 [00:00<00:25, 2503.81it/s]Loading:   3%|▎         | 1645/64000 [00:00<00:24, 2516.40it/s]Loading:   3%|▎         | 1902/64000 [00:00<00:24, 2530.96it/s]Loading:   3%|▎         | 2160/64000 [00:00<00:24, 2543.54it/s]Loading:   4%|▍         | 2415/64000 [00:01<00:24, 2545.05it/s]Loading:   4%|▍         | 2670/64000 [00:01<00:24, 2545.34it/s]Loading:   5%|▍         | 2926/64000 [00:01<00:23, 2546.76it/s]Loading:   5%|▍         | 3183/64000 [00:01<00:23, 2552.61it/s]Loading:   5%|▌         | 3440/64000 [00:01<00:23, 2555.39it/s]Loading:   6%|▌         | 3696/64000 [00:01<00:23, 2553.60it/s]Loading:   6%|▌         | 3952/64000 [00:01<00:35, 1687.18it/s]Loading:   7%|▋         | 4203/64000 [00:01<00:32, 1867.57it/s]Loading:   7%|▋         | 4458/64000 [00:01<00:29, 2030.59it/s]Loading:   7%|▋         | 4714/64000 [00:02<00:27, 2165.08it/s]Loading:   8%|▊         | 4967/64000 [00:02<00:26, 2260.99it/s]Loading:   8%|▊         | 5221/64000 [00:02<00:25, 2337.42it/s]Loading:   9%|▊         | 5476/64000 [00:02<00:24, 2397.08it/s]Loading:   9%|▉         | 5730/64000 [00:02<00:23, 2436.33it/s]Loading:   9%|▉         | 5985/64000 [00:02<00:23, 2467.73it/s]Loading:  10%|▉         | 6240/64000 [00:02<00:23, 2489.68it/s]Loading:  10%|█         | 6495/64000 [00:02<00:22, 2506.97it/s]Loading:  11%|█         | 6750/64000 [00:02<00:22, 2518.90it/s]Loading:  11%|█         | 7007/64000 [00:02<00:22, 2532.35it/s]Loading:  11%|█▏        | 7262/64000 [00:03<00:22, 2525.89it/s]Loading:  12%|█▏        | 7518/64000 [00:03<00:22, 2532.84it/s]Loading:  12%|█▏        | 7772/64000 [00:03<00:39, 1409.24it/s]Loading:  12%|█▏        | 7988/64000 [00:03<00:36, 1552.48it/s]Loading:  13%|█▎        | 8245/64000 [00:03<00:31, 1770.91it/s]Loading:  13%|█▎        | 8503/64000 [00:03<00:28, 1959.66it/s]Loading:  14%|█▎        | 8761/64000 [00:03<00:26, 2115.05it/s]Loading:  14%|█▍        | 9020/64000 [00:04<00:24, 2239.88it/s]Loading:  14%|█▍        | 9280/64000 [00:04<00:23, 2336.35it/s]Loading:  15%|█▍        | 9536/64000 [00:04<00:22, 2397.29it/s]Loading:  15%|█▌        | 9796/64000 [00:04<00:22, 2452.73it/s]Loading:  16%|█▌        | 10056/64000 [00:04<00:21, 2493.34it/s]Loading:  16%|█▌        | 10316/64000 [00:04<00:21, 2523.18it/s]Loading:  17%|█▋        | 10575/64000 [00:04<00:21, 2542.54it/s]Loading:  17%|█▋        | 10835/64000 [00:04<00:20, 2557.82it/s]Loading:  17%|█▋        | 11093/64000 [00:04<00:20, 2562.68it/s]Loading:  18%|█▊        | 11353/64000 [00:04<00:20, 2573.05it/s]Loading:  18%|█▊        | 11612/64000 [00:05<00:20, 2575.92it/s]Loading:  19%|█▊        | 11871/64000 [00:05<00:20, 2579.97it/s]Loading:  19%|█▉        | 12130/64000 [00:05<00:20, 2577.97it/s]Loading:  19%|█▉        | 12389/64000 [00:05<00:28, 1787.54it/s]Loading:  20%|█▉        | 12644/64000 [00:05<00:26, 1959.99it/s]Loading:  20%|██        | 12900/64000 [00:05<00:24, 2106.47it/s]Loading:  21%|██        | 13157/64000 [00:05<00:22, 2226.63it/s]Loading:  21%|██        | 13414/64000 [00:05<00:21, 2319.00it/s]Loading:  21%|██▏       | 13672/64000 [00:05<00:21, 2391.70it/s]Loading:  22%|██▏       | 13931/64000 [00:06<00:20, 2445.56it/s]Loading:  22%|██▏       | 14191/64000 [00:06<00:20, 2488.54it/s]Loading:  23%|██▎       | 14448/64000 [00:06<00:19, 2511.26it/s]Loading:  23%|██▎       | 14708/64000 [00:06<00:19, 2534.85it/s]Loading:  23%|██▎       | 14967/64000 [00:06<00:19, 2548.21it/s]Loading:  24%|██▍       | 15225/64000 [00:06<00:19, 2555.17it/s]Loading:  24%|██▍       | 15482/64000 [00:06<00:18, 2558.38it/s]Loading:  25%|██▍       | 15739/64000 [00:06<00:18, 2561.32it/s]Loading:  25%|██▍       | 15997/64000 [00:06<00:18, 2566.88it/s]Loading:  25%|██▌       | 16255/64000 [00:07<00:18, 2536.33it/s]Loading:  26%|██▌       | 16513/64000 [00:07<00:18, 2547.44it/s]Loading:  26%|██▌       | 16771/64000 [00:07<00:18, 2554.17it/s]Loading:  27%|██▋       | 17028/64000 [00:07<00:18, 2557.18it/s]Loading:  27%|██▋       | 17287/64000 [00:07<00:18, 2566.76it/s]Loading:  27%|██▋       | 17544/64000 [00:07<00:26, 1761.78it/s]Loading:  28%|██▊       | 17755/64000 [00:07<00:25, 1838.85it/s]Loading:  28%|██▊       | 18002/64000 [00:07<00:23, 1991.99it/s]Loading:  29%|██▊       | 18257/64000 [00:07<00:21, 2134.22it/s]Loading:  29%|██▉       | 18512/64000 [00:08<00:20, 2246.13it/s]Loading:  29%|██▉       | 18767/64000 [00:08<00:19, 2330.39it/s]Loading:  30%|██▉       | 19021/64000 [00:08<00:18, 2387.56it/s]Loading:  30%|███       | 19277/64000 [00:08<00:18, 2435.25it/s]Loading:  31%|███       | 19531/64000 [00:08<00:18, 2465.34it/s]Loading:  31%|███       | 19784/64000 [00:08<00:17, 2482.32it/s]Loading:  31%|███▏      | 20037/64000 [00:08<00:17, 2493.66it/s]Loading:  32%|███▏      | 20292/64000 [00:08<00:17, 2510.02it/s]Loading:  32%|███▏      | 20545/64000 [00:08<00:17, 2512.34it/s]Loading:  33%|███▎      | 20801/64000 [00:08<00:17, 2523.86it/s]Loading:  33%|███▎      | 21056/64000 [00:09<00:16, 2530.43it/s]Loading:  33%|███▎      | 21312/64000 [00:09<00:16, 2538.93it/s]Loading:  34%|███▎      | 21567/64000 [00:09<00:16, 2539.05it/s]Loading:  34%|███▍      | 21822/64000 [00:09<00:16, 2536.44it/s]Loading:  34%|███▍      | 22076/64000 [00:09<00:16, 2522.20it/s]Loading:  35%|███▍      | 22329/64000 [00:09<00:24, 1668.56it/s]Loading:  35%|███▌      | 22534/64000 [00:09<00:28, 1479.63it/s]Loading:  36%|███▌      | 22746/64000 [00:10<00:25, 1612.37it/s]Loading:  36%|███▌      | 22996/64000 [00:10<00:22, 1816.53it/s]Loading:  36%|███▋      | 23254/64000 [00:10<00:20, 2004.70it/s]Loading:  37%|███▋      | 23510/64000 [00:10<00:18, 2148.89it/s]Loading:  37%|███▋      | 23769/64000 [00:10<00:17, 2267.15it/s]Loading:  38%|███▊      | 24028/64000 [00:10<00:16, 2355.67it/s]Loading:  38%|███▊      | 24287/64000 [00:10<00:16, 2421.04it/s]Loading:  38%|███▊      | 24544/64000 [00:10<00:16, 2463.68it/s]Loading:  39%|███▉      | 24802/64000 [00:10<00:15, 2496.94it/s]Loading:  39%|███▉      | 25059/64000 [00:10<00:15, 2517.61it/s]Loading:  40%|███▉      | 25317/64000 [00:11<00:15, 2533.46it/s]Loading:  40%|███▉      | 25573/64000 [00:11<00:15, 2540.62it/s]Loading:  40%|████      | 25832/64000 [00:11<00:14, 2553.33it/s]Loading:  41%|████      | 26089/64000 [00:11<00:14, 2553.50it/s]Loading:  41%|████      | 26346/64000 [00:11<00:14, 2558.20it/s]Loading:  42%|████▏     | 26604/64000 [00:11<00:14, 2562.62it/s]Loading:  42%|████▏     | 26861/64000 [00:11<00:14, 2564.38it/s]Loading:  42%|████▏     | 27118/64000 [00:11<00:14, 2553.98it/s]Loading:  43%|████▎     | 27375/64000 [00:11<00:14, 2557.19it/s]Loading:  43%|████▎     | 27632/64000 [00:11<00:14, 2559.53it/s]Loading:  44%|████▎     | 27889/64000 [00:12<00:14, 2560.35it/s]Loading:  44%|████▍     | 28146/64000 [00:12<00:14, 2549.29it/s]Loading:  44%|████▍     | 28401/64000 [00:12<00:24, 1469.95it/s]Loading:  45%|████▍     | 28613/64000 [00:12<00:22, 1597.53it/s]Loading:  45%|████▌     | 28855/64000 [00:12<00:19, 1777.90it/s]Loading:  45%|████▌     | 29110/64000 [00:12<00:17, 1962.31it/s]Loading:  46%|████▌     | 29365/64000 [00:12<00:16, 2111.11it/s]Loading:  46%|████▋     | 29622/64000 [00:12<00:15, 2233.01it/s]Loading:  47%|████▋     | 29879/64000 [00:13<00:14, 2325.14it/s]Loading:  47%|████▋     | 30135/64000 [00:13<00:14, 2390.01it/s]Loading:  47%|████▋     | 30393/64000 [00:13<00:13, 2442.84it/s]Loading:  48%|████▊     | 30650/64000 [00:13<00:13, 2477.85it/s]Loading:  48%|████▊     | 30909/64000 [00:13<00:13, 2508.77it/s]Loading:  49%|████▊     | 31167/64000 [00:13<00:12, 2527.28it/s]Loading:  49%|████▉     | 31426/64000 [00:13<00:12, 2544.52it/s]Loading:  50%|████▉     | 31683/64000 [00:13<00:12, 2550.66it/s]Loading:  50%|████▉     | 31941/64000 [00:13<00:12, 2558.93it/s]Loading:  50%|█████     | 32199/64000 [00:13<00:12, 2563.65it/s]Loading:  51%|█████     | 32457/64000 [00:14<00:12, 2567.29it/s]Loading:  51%|█████     | 32716/64000 [00:14<00:12, 2571.67it/s]Loading:  52%|█████▏    | 32974/64000 [00:14<00:12, 2573.74it/s]Loading:  52%|█████▏    | 33232/64000 [00:14<00:11, 2573.05it/s]Loading:  52%|█████▏    | 33490/64000 [00:14<00:11, 2574.85it/s]Loading:  53%|█████▎    | 33748/64000 [00:14<00:11, 2573.26it/s]Loading:  53%|█████▎    | 34007/64000 [00:14<00:11, 2576.60it/s]Loading:  54%|█████▎    | 34267/64000 [00:14<00:11, 2581.40it/s]Loading:  54%|█████▍    | 34526/64000 [00:14<00:11, 2577.16it/s]Loading:  54%|█████▍    | 34784/64000 [00:14<00:11, 2558.16it/s]Loading:  55%|█████▍    | 35040/64000 [00:15<00:18, 1574.94it/s]Loading:  55%|█████▌    | 35297/64000 [00:15<00:16, 1780.35it/s]Loading:  56%|█████▌    | 35552/64000 [00:15<00:14, 1954.55it/s]Loading:  56%|█████▌    | 35808/64000 [00:15<00:13, 2102.29it/s]Loading:  56%|█████▋    | 36064/64000 [00:15<00:12, 2219.23it/s]Loading:  57%|█████▋    | 36322/64000 [00:15<00:11, 2314.74it/s]Loading:  57%|█████▋    | 36576/64000 [00:15<00:11, 2376.93it/s]Loading:  58%|█████▊    | 36832/64000 [00:15<00:11, 2428.13it/s]Loading:  58%|█████▊    | 37088/64000 [00:16<00:10, 2463.78it/s]Loading:  58%|█████▊    | 37344/64000 [00:16<00:10, 2490.45it/s]Loading:  59%|█████▊    | 37599/64000 [00:16<00:10, 2507.46it/s]Loading:  59%|█████▉    | 37854/64000 [00:16<00:10, 2517.11it/s]Loading:  60%|█████▉    | 38109/64000 [00:16<00:10, 2524.81it/s]Loading:  60%|█████▉    | 38363/64000 [00:16<00:15, 1675.94it/s]Loading:  60%|██████    | 38588/64000 [00:16<00:14, 1801.09it/s]Loading:  61%|██████    | 38838/64000 [00:16<00:12, 1967.91it/s]Loading:  61%|██████    | 39091/64000 [00:17<00:11, 2109.84it/s]Loading:  61%|██████▏   | 39341/64000 [00:17<00:11, 2212.94it/s]Loading:  62%|██████▏   | 39592/64000 [00:17<00:10, 2294.40it/s]Loading:  62%|██████▏   | 39845/64000 [00:17<00:10, 2360.39it/s]Loading:  63%|██████▎   | 40095/64000 [00:17<00:09, 2398.77it/s]Loading:  63%|██████▎   | 40348/64000 [00:17<00:09, 2435.22it/s]Loading:  63%|██████▎   | 40599/64000 [00:17<00:09, 2456.94it/s]Loading:  64%|██████▍   | 40853/64000 [00:17<00:09, 2479.11it/s]Loading:  64%|██████▍   | 41104/64000 [00:17<00:09, 2484.97it/s]Loading:  65%|██████▍   | 41358/64000 [00:17<00:09, 2498.09it/s]Loading:  65%|██████▌   | 41609/64000 [00:18<00:08, 2501.06it/s]Loading:  65%|██████▌   | 41861/64000 [00:18<00:08, 2506.10it/s]Loading:  66%|██████▌   | 42113/64000 [00:18<00:08, 2503.88it/s]Loading:  66%|██████▌   | 42364/64000 [00:18<00:08, 2503.56it/s]Loading:  67%|██████▋   | 42615/64000 [00:18<00:08, 2500.58it/s]Loading:  67%|██████▋   | 42866/64000 [00:18<00:08, 2497.21it/s]Loading:  67%|██████▋   | 43118/64000 [00:18<00:08, 2503.70it/s]Loading:  68%|██████▊   | 43369/64000 [00:18<00:08, 2504.25it/s]Loading:  68%|██████▊   | 43621/64000 [00:18<00:08, 2507.58it/s]Loading:  69%|██████▊   | 43872/64000 [00:18<00:08, 2501.95it/s]Loading:  69%|██████▉   | 44124/64000 [00:19<00:07, 2506.16it/s]Loading:  69%|██████▉   | 44375/64000 [00:19<00:07, 2506.73it/s]Loading:  70%|██████▉   | 44626/64000 [00:19<00:07, 2479.97it/s]Loading:  70%|███████   | 44875/64000 [00:19<00:07, 2476.21it/s]Loading:  71%|███████   | 45129/64000 [00:19<00:07, 2492.31it/s]Loading:  71%|███████   | 45380/64000 [00:19<00:07, 2495.12it/s]Loading:  71%|███████▏  | 45631/64000 [00:19<00:07, 2499.11it/s]Loading:  72%|███████▏  | 45885/64000 [00:19<00:07, 2509.75it/s]Loading:  72%|███████▏  | 46136/64000 [00:19<00:07, 2501.57it/s]Loading:  72%|███████▏  | 46389/64000 [00:19<00:07, 2507.28it/s]Loading:  73%|███████▎  | 46640/64000 [00:20<00:06, 2502.06it/s]Loading:  73%|███████▎  | 46891/64000 [00:20<00:06, 2503.17it/s]Loading:  74%|███████▎  | 47142/64000 [00:20<00:06, 2493.91it/s]Loading:  74%|███████▍  | 47392/64000 [00:20<00:09, 1823.22it/s]Loading:  74%|███████▍  | 47619/64000 [00:20<00:08, 1929.14it/s]Loading:  75%|███████▍  | 47872/64000 [00:20<00:07, 2080.71it/s]Loading:  75%|███████▌  | 48127/64000 [00:20<00:07, 2204.66it/s]Loading:  76%|███████▌  | 48384/64000 [00:20<00:06, 2303.44it/s]Loading:  76%|███████▌  | 48640/64000 [00:21<00:06, 2373.70it/s]Loading:  76%|███████▋  | 48897/64000 [00:21<00:06, 2427.34it/s]Loading:  77%|███████▋  | 49154/64000 [00:21<00:06, 2468.54it/s]Loading:  77%|███████▋  | 49413/64000 [00:21<00:05, 2501.81it/s]Loading:  78%|███████▊  | 49670/64000 [00:21<00:05, 2519.57it/s]Loading:  78%|███████▊  | 49927/64000 [00:21<00:05, 2532.05it/s]Loading:  78%|███████▊  | 50182/64000 [00:21<00:05, 2531.46it/s]Loading:  79%|███████▉  | 50437/64000 [00:21<00:05, 2513.77it/s]Loading:  79%|███████▉  | 50695/64000 [00:21<00:05, 2530.68it/s]Loading:  80%|███████▉  | 50949/64000 [00:22<00:09, 1437.26it/s]Loading:  80%|████████  | 51206/64000 [00:22<00:07, 1656.18it/s]Loading:  80%|████████  | 51461/64000 [00:22<00:06, 1849.70it/s]Loading:  81%|████████  | 51717/64000 [00:22<00:06, 2016.34it/s]Loading:  81%|████████  | 51972/64000 [00:22<00:05, 2150.94it/s]Loading:  82%|████████▏ | 52227/64000 [00:22<00:05, 2255.01it/s]Loading:  82%|████████▏ | 52473/64000 [00:22<00:07, 1635.39it/s]Loading:  82%|████████▏ | 52692/64000 [00:23<00:06, 1755.71it/s]Loading:  83%|████████▎ | 52944/64000 [00:23<00:05, 1935.94it/s]Loading:  83%|████████▎ | 53193/64000 [00:23<00:05, 2075.85it/s]Loading:  84%|████████▎ | 53445/64000 [00:23<00:04, 2191.61it/s]Loading:  84%|████████▍ | 53697/64000 [00:23<00:04, 2276.91it/s]Loading:  84%|████████▍ | 53948/64000 [00:23<00:04, 2340.98it/s]Loading:  85%|████████▍ | 54199/64000 [00:23<00:04, 2387.87it/s]Loading:  85%|████████▌ | 54449/64000 [00:23<00:03, 2418.16it/s]Loading:  85%|████████▌ | 54700/64000 [00:23<00:03, 2444.91it/s]Loading:  86%|████████▌ | 54951/64000 [00:23<00:03, 2463.12it/s]Loading:  86%|████████▋ | 55203/64000 [00:24<00:03, 2478.63it/s]Loading:  87%|████████▋ | 55455/64000 [00:24<00:03, 2489.21it/s]Loading:  87%|████████▋ | 55708/64000 [00:24<00:03, 2500.65it/s]Loading:  87%|████████▋ | 55959/64000 [00:24<00:03, 2496.54it/s]Loading:  88%|████████▊ | 56211/64000 [00:24<00:03, 2502.21it/s]Loading:  88%|████████▊ | 56463/64000 [00:24<00:03, 2505.86it/s]Loading:  89%|████████▊ | 56714/64000 [00:24<00:02, 2505.55it/s]Loading:  89%|████████▉ | 56965/64000 [00:24<00:02, 2501.71it/s]Loading:  89%|████████▉ | 57217/64000 [00:24<00:02, 2505.71it/s]Loading:  90%|████████▉ | 57469/64000 [00:24<00:02, 2509.19it/s]Loading:  90%|█████████ | 57720/64000 [00:25<00:02, 2508.23it/s]Loading:  91%|█████████ | 57971/64000 [00:25<00:04, 1497.43it/s]Loading:  91%|█████████ | 58181/64000 [00:25<00:03, 1619.42it/s]Loading:  91%|█████████▏| 58422/64000 [00:25<00:03, 1796.40it/s]Loading:  92%|█████████▏| 58672/64000 [00:25<00:02, 1966.88it/s]Loading:  92%|█████████▏| 58927/64000 [00:25<00:02, 2116.47it/s]Loading:  92%|█████████▏| 59183/64000 [00:25<00:02, 2235.67it/s]Loading:  93%|█████████▎| 59438/64000 [00:25<00:01, 2321.64it/s]Loading:  93%|█████████▎| 59693/64000 [00:26<00:01, 2386.46it/s]Loading:  94%|█████████▎| 59949/64000 [00:26<00:01, 2434.41it/s]Loading:  94%|█████████▍| 60205/64000 [00:26<00:01, 2469.73it/s]Loading:  94%|█████████▍| 60459/64000 [00:26<00:01, 2490.26it/s]Loading:  95%|█████████▍| 60717/64000 [00:26<00:01, 2515.87it/s]Loading:  95%|█████████▌| 60972/64000 [00:26<00:01, 2524.30it/s]Loading:  96%|█████████▌| 61229/64000 [00:26<00:01, 2536.80it/s]Loading:  96%|█████████▌| 61484/64000 [00:26<00:00, 2531.11it/s]Loading:  96%|█████████▋| 61742/64000 [00:26<00:00, 2543.14it/s]Loading:  97%|█████████▋| 61998/64000 [00:26<00:00, 2546.18it/s]Loading:  97%|█████████▋| 62254/64000 [00:27<00:00, 2544.86it/s]Loading:  98%|█████████▊| 62509/64000 [00:27<00:00, 2545.39it/s]Loading:  98%|█████████▊| 62765/64000 [00:27<00:00, 2546.92it/s]Loading:  98%|█████████▊| 63020/64000 [00:27<00:00, 2538.34it/s]Loading:  99%|█████████▉| 63276/64000 [00:27<00:00, 2542.59it/s]Loading:  99%|█████████▉| 63531/64000 [00:27<00:00, 2543.32it/s]Loading: 100%|█████████▉| 63786/64000 [00:27<00:00, 2543.90it/s]Loading: 100%|██████████| 64000/64000 [00:27<00:00, 2306.60it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 140/49018 [00:00<00:34, 1397.84it/s]Loading:   1%|          | 405/49018 [00:00<00:22, 2132.28it/s]Loading:   1%|▏         | 672/49018 [00:00<00:20, 2377.17it/s]Loading:   2%|▏         | 938/49018 [00:00<00:19, 2486.35it/s]Loading:   2%|▏         | 1207/49018 [00:00<00:18, 2557.13it/s]Loading:   3%|▎         | 1475/49018 [00:00<00:18, 2596.96it/s]Loading:   4%|▎         | 1744/49018 [00:00<00:18, 2625.60it/s]Loading:   4%|▍         | 2013/49018 [00:00<00:17, 2644.67it/s]Loading:   5%|▍         | 2282/49018 [00:00<00:17, 2656.44it/s]Loading:   5%|▌         | 2550/49018 [00:01<00:17, 2662.50it/s]Loading:   6%|▌         | 2820/49018 [00:01<00:17, 2671.28it/s]Loading:   6%|▋         | 3090/49018 [00:01<00:17, 2677.37it/s]Loading:   7%|▋         | 3359/49018 [00:01<00:17, 2678.60it/s]Loading:   7%|▋         | 3628/49018 [00:01<00:16, 2681.95it/s]Loading:   8%|▊         | 3897/49018 [00:01<00:16, 2684.29it/s]Loading:   8%|▊         | 4166/49018 [00:01<00:16, 2682.05it/s]Loading:   9%|▉         | 4436/49018 [00:01<00:16, 2686.13it/s]Loading:  10%|▉         | 4705/49018 [00:01<00:16, 2680.92it/s]Loading:  10%|█         | 4974/49018 [00:01<00:16, 2653.71it/s]Loading:  11%|█         | 5243/49018 [00:02<00:16, 2662.09it/s]Loading:  11%|█         | 5512/49018 [00:02<00:16, 2668.44it/s]Loading:  12%|█▏        | 5781/49018 [00:02<00:16, 2674.41it/s]Loading:  12%|█▏        | 6049/49018 [00:02<00:16, 2675.50it/s]Loading:  13%|█▎        | 6318/49018 [00:02<00:15, 2678.48it/s]Loading:  13%|█▎        | 6586/49018 [00:02<00:15, 2676.73it/s]Loading:  14%|█▍        | 6854/49018 [00:02<00:15, 2672.66it/s]Loading:  15%|█▍        | 7122/49018 [00:02<00:15, 2672.18it/s]Loading:  15%|█▌        | 7390/49018 [00:02<00:15, 2673.33it/s]Loading:  16%|█▌        | 7658/49018 [00:02<00:15, 2672.50it/s]Loading:  16%|█▌        | 7926/49018 [00:03<00:15, 2672.78it/s]Loading:  17%|█▋        | 8194/49018 [00:03<00:15, 2667.79it/s]Loading:  17%|█▋        | 8461/49018 [00:03<00:15, 2661.42it/s]Loading:  18%|█▊        | 8728/49018 [00:03<00:15, 2662.65it/s]Loading:  18%|█▊        | 8996/49018 [00:03<00:15, 2665.34it/s]Loading:  19%|█▉        | 9263/49018 [00:03<00:14, 2656.18it/s]Loading:  19%|█▉        | 9529/49018 [00:03<00:14, 2657.28it/s]Loading:  20%|█▉        | 9797/49018 [00:03<00:14, 2661.79it/s]Loading:  21%|██        | 10065/49018 [00:03<00:14, 2667.17it/s]Loading:  21%|██        | 10332/49018 [00:03<00:14, 2662.95it/s]Loading:  22%|██▏       | 10600/49018 [00:04<00:14, 2666.27it/s]Loading:  22%|██▏       | 10867/49018 [00:04<00:14, 2662.94it/s]Loading:  23%|██▎       | 11135/49018 [00:04<00:14, 2666.13it/s]Loading:  23%|██▎       | 11402/49018 [00:04<00:14, 2657.39it/s]Loading:  24%|██▍       | 11669/49018 [00:04<00:14, 2660.59it/s]Loading:  24%|██▍       | 11936/49018 [00:04<00:13, 2662.15it/s]Loading:  25%|██▍       | 12203/49018 [00:04<00:13, 2658.70it/s]Loading:  25%|██▌       | 12469/49018 [00:04<00:13, 2657.91it/s]Loading:  26%|██▌       | 12735/49018 [00:04<00:13, 2655.97it/s]Loading:  27%|██▋       | 13001/49018 [00:04<00:13, 2655.15it/s]Loading:  27%|██▋       | 13267/49018 [00:05<00:13, 2648.68it/s]Loading:  28%|██▊       | 13532/49018 [00:05<00:13, 2622.07it/s]Loading:  28%|██▊       | 13795/49018 [00:05<00:13, 2616.23it/s]Loading:  29%|██▊       | 14057/49018 [00:05<00:13, 2613.55it/s]Loading:  29%|██▉       | 14319/49018 [00:05<00:13, 2607.95it/s]Loading:  30%|██▉       | 14580/49018 [00:05<00:13, 2607.99it/s]Loading:  30%|███       | 14841/49018 [00:05<00:13, 2602.31it/s]Loading:  31%|███       | 15102/49018 [00:05<00:13, 2583.25it/s]Loading:  31%|███▏      | 15362/49018 [00:05<00:13, 2586.90it/s]Loading:  32%|███▏      | 15623/49018 [00:05<00:12, 2592.32it/s]Loading:  32%|███▏      | 15883/49018 [00:06<00:12, 2592.59it/s]Loading:  33%|███▎      | 16143/49018 [00:06<00:12, 2588.88it/s]Loading:  33%|███▎      | 16403/49018 [00:06<00:12, 2589.87it/s]Loading:  34%|███▍      | 16662/49018 [00:06<00:12, 2581.90it/s]Loading:  35%|███▍      | 16921/49018 [00:06<00:12, 2578.33it/s]Loading:  35%|███▌      | 17182/49018 [00:06<00:12, 2587.55it/s]Loading:  36%|███▌      | 17441/49018 [00:06<00:12, 2587.87it/s]Loading:  36%|███▌      | 17700/49018 [00:06<00:12, 2585.59it/s]Loading:  37%|███▋      | 17959/49018 [00:06<00:12, 2559.77it/s]Loading:  37%|███▋      | 18216/49018 [00:07<00:24, 1232.69it/s]Loading:  38%|███▊      | 18476/49018 [00:07<00:20, 1464.40it/s]Loading:  38%|███▊      | 18734/49018 [00:07<00:18, 1681.84it/s]Loading:  39%|███▊      | 18993/49018 [00:07<00:15, 1878.88it/s]Loading:  39%|███▉      | 19251/49018 [00:07<00:14, 2043.47it/s]Loading:  40%|███▉      | 19511/49018 [00:07<00:13, 2183.06it/s]Loading:  40%|████      | 19768/49018 [00:07<00:12, 2284.13it/s]Loading:  41%|████      | 20027/49018 [00:08<00:12, 2366.55it/s]Loading:  41%|████▏     | 20285/49018 [00:08<00:11, 2426.59it/s]Loading:  42%|████▏     | 20545/49018 [00:08<00:11, 2475.47it/s]Loading:  42%|████▏     | 20804/49018 [00:08<00:11, 2507.84it/s]Loading:  43%|████▎     | 21066/49018 [00:08<00:11, 2538.21it/s]Loading:  44%|████▎     | 21325/49018 [00:08<00:10, 2552.67it/s]Loading:  44%|████▍     | 21584/49018 [00:08<00:10, 2563.56it/s]Loading:  45%|████▍     | 21844/49018 [00:08<00:10, 2571.60it/s]Loading:  45%|████▌     | 22104/49018 [00:08<00:10, 2579.98it/s]Loading:  46%|████▌     | 22364/49018 [00:08<00:10, 2584.72it/s]Loading:  46%|████▌     | 22624/49018 [00:09<00:10, 2582.80it/s]Loading:  47%|████▋     | 22885/49018 [00:09<00:10, 2588.60it/s]Loading:  47%|████▋     | 23145/49018 [00:09<00:10, 2584.66it/s]Loading:  48%|████▊     | 23404/49018 [00:09<00:09, 2581.51it/s]Loading:  48%|████▊     | 23663/49018 [00:09<00:09, 2577.08it/s]Loading:  49%|████▉     | 23923/49018 [00:09<00:09, 2582.78it/s]Loading:  49%|████▉     | 24183/49018 [00:09<00:09, 2586.61it/s]Loading:  50%|████▉     | 24444/49018 [00:09<00:09, 2591.16it/s]Loading:  50%|█████     | 24704/49018 [00:09<00:09, 2581.50it/s]Loading:  51%|█████     | 24963/49018 [00:09<00:09, 2577.84it/s]Loading:  51%|█████▏    | 25221/49018 [00:10<00:09, 2576.04it/s]Loading:  52%|█████▏    | 25480/49018 [00:10<00:09, 2579.69it/s]Loading:  53%|█████▎    | 25739/49018 [00:10<00:09, 2582.29it/s]Loading:  53%|█████▎    | 25998/49018 [00:10<00:08, 2581.78it/s]Loading:  54%|█████▎    | 26258/49018 [00:10<00:08, 2584.50it/s]Loading:  54%|█████▍    | 26518/49018 [00:10<00:08, 2586.91it/s]Loading:  55%|█████▍    | 26778/49018 [00:10<00:08, 2589.91it/s]Loading:  55%|█████▌    | 27037/49018 [00:10<00:08, 2589.90it/s]Loading:  56%|█████▌    | 27296/49018 [00:10<00:08, 2577.71it/s]Loading:  56%|█████▌    | 27554/49018 [00:10<00:08, 2578.22it/s]Loading:  57%|█████▋    | 27813/49018 [00:11<00:08, 2579.93it/s]Loading:  57%|█████▋    | 28072/49018 [00:11<00:08, 2581.67it/s]Loading:  58%|█████▊    | 28331/49018 [00:11<00:08, 2583.66it/s]Loading:  58%|█████▊    | 28590/49018 [00:11<00:07, 2576.55it/s]Loading:  59%|█████▉    | 28852/49018 [00:11<00:07, 2587.17it/s]Loading:  59%|█████▉    | 29111/49018 [00:11<00:07, 2586.22it/s]Loading:  60%|█████▉    | 29374/49018 [00:11<00:07, 2596.35it/s]Loading:  60%|██████    | 29634/49018 [00:11<00:07, 2589.59it/s]Loading:  61%|██████    | 29893/49018 [00:11<00:07, 2583.91it/s]Loading:  62%|██████▏   | 30152/49018 [00:11<00:07, 2577.41it/s]Loading:  62%|██████▏   | 30410/49018 [00:12<00:07, 2576.72it/s]Loading:  63%|██████▎   | 30669/49018 [00:12<00:07, 2577.93it/s]Loading:  63%|██████▎   | 30929/49018 [00:12<00:06, 2584.16it/s]Loading:  64%|██████▎   | 31188/49018 [00:12<00:06, 2582.67it/s]Loading:  64%|██████▍   | 31447/49018 [00:12<00:06, 2580.31it/s]Loading:  65%|██████▍   | 31708/49018 [00:12<00:06, 2586.51it/s]Loading:  65%|██████▌   | 31967/49018 [00:12<00:06, 2583.38it/s]Loading:  66%|██████▌   | 32226/49018 [00:12<00:06, 2581.71it/s]Loading:  66%|██████▋   | 32485/49018 [00:12<00:06, 2572.82it/s]Loading:  67%|██████▋   | 32744/49018 [00:12<00:06, 2577.36it/s]Loading:  67%|██████▋   | 33002/49018 [00:13<00:06, 2578.12it/s]Loading:  68%|██████▊   | 33260/49018 [00:13<00:06, 2559.17it/s]Loading:  68%|██████▊   | 33518/49018 [00:13<00:06, 2564.49it/s]Loading:  69%|██████▉   | 33777/49018 [00:13<00:05, 2571.53it/s]Loading:  69%|██████▉   | 34036/49018 [00:13<00:05, 2574.04it/s]Loading:  70%|██████▉   | 34296/49018 [00:13<00:05, 2580.53it/s]Loading:  70%|███████   | 34556/49018 [00:13<00:05, 2584.33it/s]Loading:  71%|███████   | 34818/49018 [00:13<00:05, 2593.39it/s]Loading:  72%|███████▏  | 35078/49018 [00:13<00:05, 2586.86it/s]Loading:  72%|███████▏  | 35337/49018 [00:13<00:05, 2581.82it/s]Loading:  73%|███████▎  | 35596/49018 [00:14<00:05, 2577.47it/s]Loading:  73%|███████▎  | 35856/49018 [00:14<00:05, 2582.44it/s]Loading:  74%|███████▎  | 36117/49018 [00:14<00:04, 2587.91it/s]Loading:  74%|███████▍  | 36376/49018 [00:14<00:04, 2582.71it/s]Loading:  75%|███████▍  | 36635/49018 [00:14<00:04, 2578.43it/s]Loading:  75%|███████▌  | 36893/49018 [00:14<00:04, 2572.66it/s]Loading:  76%|███████▌  | 37153/49018 [00:14<00:04, 2578.75it/s]Loading:  76%|███████▋  | 37412/49018 [00:14<00:04, 2581.28it/s]Loading:  77%|███████▋  | 37671/49018 [00:14<00:04, 2580.72it/s]Loading:  77%|███████▋  | 37930/49018 [00:14<00:04, 2578.22it/s]Loading:  78%|███████▊  | 38188/49018 [00:15<00:04, 2577.26it/s]Loading:  78%|███████▊  | 38448/49018 [00:15<00:04, 2581.64it/s]Loading:  79%|███████▉  | 38709/49018 [00:15<00:03, 2587.26it/s]Loading:  79%|███████▉  | 38968/49018 [00:15<00:03, 2580.56it/s]Loading:  80%|████████  | 39227/49018 [00:15<00:03, 2579.83it/s]Loading:  81%|████████  | 39487/49018 [00:15<00:03, 2584.93it/s]Loading:  81%|████████  | 39746/49018 [00:15<00:03, 2585.13it/s]Loading:  82%|████████▏ | 40006/49018 [00:15<00:03, 2586.86it/s]Loading:  82%|████████▏ | 40265/49018 [00:15<00:03, 2579.90it/s]Loading:  83%|████████▎ | 40523/49018 [00:15<00:03, 2578.31it/s]Loading:  83%|████████▎ | 40782/49018 [00:16<00:03, 2579.96it/s]Loading:  84%|████████▎ | 41042/49018 [00:16<00:03, 2584.13it/s]Loading:  84%|████████▍ | 41301/49018 [00:16<00:02, 2581.54it/s]Loading:  85%|████████▍ | 41560/49018 [00:16<00:02, 2577.07it/s]Loading:  85%|████████▌ | 41819/49018 [00:16<00:02, 2578.83it/s]Loading:  86%|████████▌ | 42080/49018 [00:16<00:02, 2587.72it/s]Loading:  86%|████████▋ | 42339/49018 [00:16<00:02, 2582.43it/s]Loading:  87%|████████▋ | 42598/49018 [00:16<00:02, 2584.33it/s]Loading:  87%|████████▋ | 42857/49018 [00:16<00:02, 2583.26it/s]Loading:  88%|████████▊ | 43116/49018 [00:16<00:02, 2579.46it/s]Loading:  88%|████████▊ | 43374/49018 [00:17<00:02, 2576.54it/s]Loading:  89%|████████▉ | 43634/49018 [00:17<00:02, 2582.08it/s]Loading:  90%|████████▉ | 43895/49018 [00:17<00:01, 2588.46it/s]Loading:  90%|█████████ | 44154/49018 [00:17<00:01, 2587.16it/s]Loading:  91%|█████████ | 44414/49018 [00:17<00:01, 2590.64it/s]Loading:  91%|█████████ | 44674/49018 [00:17<00:01, 2586.02it/s]Loading:  92%|█████████▏| 44935/49018 [00:17<00:01, 2590.88it/s]Loading:  92%|█████████▏| 45195/49018 [00:17<00:01, 2586.62it/s]Loading:  93%|█████████▎| 45456/49018 [00:17<00:01, 2592.66it/s]Loading:  93%|█████████▎| 45716/49018 [00:18<00:02, 1104.58it/s]Loading:  94%|█████████▍| 45974/49018 [00:18<00:02, 1330.95it/s]Loading:  94%|█████████▍| 46234/49018 [00:18<00:01, 1558.76it/s]Loading:  95%|█████████▍| 46493/49018 [00:18<00:01, 1768.54it/s]Loading:  95%|█████████▌| 46751/49018 [00:18<00:01, 1951.59it/s]Loading:  96%|█████████▌| 47011/49018 [00:18<00:00, 2107.71it/s]Loading:  96%|█████████▋| 47270/49018 [00:18<00:00, 2230.52it/s]Loading:  97%|█████████▋| 47529/49018 [00:19<00:00, 2326.62it/s]Loading:  97%|█████████▋| 47787/49018 [00:19<00:00, 2396.75it/s]Loading:  98%|█████████▊| 48044/49018 [00:19<00:00, 2445.87it/s]Loading:  99%|█████████▊| 48303/49018 [00:19<00:00, 2487.39it/s]Loading:  99%|█████████▉| 48562/49018 [00:19<00:00, 2515.47it/s]Loading: 100%|█████████▉| 48820/49018 [00:19<00:00, 2529.90it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2491.06it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 147/49018 [00:00<00:33, 1464.81it/s]Loading:   1%|          | 411/49018 [00:00<00:22, 2152.67it/s]Loading:   1%|▏         | 678/49018 [00:00<00:20, 2385.89it/s]Loading:   2%|▏         | 943/49018 [00:00<00:19, 2487.78it/s]Loading:   2%|▏         | 1212/49018 [00:00<00:18, 2557.33it/s]Loading:   3%|▎         | 1478/49018 [00:00<00:18, 2590.79it/s]Loading:   4%|▎         | 1746/49018 [00:00<00:18, 2618.10it/s]Loading:   4%|▍         | 2008/49018 [00:01<00:44, 1045.23it/s]Loading:   5%|▍         | 2274/49018 [00:01<00:36, 1292.46it/s]Loading:   5%|▌         | 2538/49018 [00:01<00:30, 1533.87it/s]Loading:   6%|▌         | 2795/49018 [00:01<00:26, 1744.16it/s]Loading:   6%|▌         | 3058/49018 [00:01<00:23, 1943.60it/s]Loading:   7%|▋         | 3322/49018 [00:01<00:21, 2111.65it/s]Loading:   7%|▋         | 3587/49018 [00:01<00:20, 2250.40it/s]Loading:   8%|▊         | 3853/49018 [00:01<00:19, 2358.70it/s]Loading:   8%|▊         | 4113/49018 [00:02<00:18, 2425.48it/s]Loading:   9%|▉         | 4377/49018 [00:02<00:17, 2484.33it/s]Loading:   9%|▉         | 4638/49018 [00:02<00:17, 2519.73it/s]Loading:  10%|▉         | 4901/49018 [00:02<00:17, 2549.39it/s]Loading:  11%|█         | 5162/49018 [00:02<00:17, 2565.76it/s]Loading:  11%|█         | 5425/49018 [00:02<00:16, 2583.30it/s]Loading:  12%|█▏        | 5687/49018 [00:02<00:16, 2589.86it/s]Loading:  12%|█▏        | 5949/49018 [00:02<00:16, 2593.56it/s]Loading:  13%|█▎        | 6212/49018 [00:02<00:16, 2602.06it/s]Loading:  13%|█▎        | 6476/49018 [00:02<00:16, 2611.41it/s]Loading:  14%|█▎        | 6739/49018 [00:03<00:16, 2615.88it/s]Loading:  14%|█▍        | 7002/49018 [00:03<00:16, 2617.84it/s]Loading:  15%|█▍        | 7265/49018 [00:03<00:15, 2618.95it/s]Loading:  15%|█▌        | 7528/49018 [00:03<00:15, 2618.48it/s]Loading:  16%|█▌        | 7791/49018 [00:03<00:15, 2621.07it/s]Loading:  16%|█▋        | 8055/49018 [00:03<00:15, 2625.09it/s]Loading:  17%|█▋        | 8318/49018 [00:03<00:15, 2622.10it/s]Loading:  18%|█▊        | 8581/49018 [00:03<00:15, 2622.82it/s]Loading:  18%|█▊        | 8844/49018 [00:03<00:15, 2622.15it/s]Loading:  19%|█▊        | 9108/49018 [00:03<00:15, 2624.79it/s]Loading:  19%|█▉        | 9371/49018 [00:04<00:15, 2625.35it/s]Loading:  20%|█▉        | 9634/49018 [00:04<00:15, 2624.69it/s]Loading:  20%|██        | 9898/49018 [00:04<00:14, 2627.88it/s]Loading:  21%|██        | 10161/49018 [00:04<00:14, 2625.75it/s]Loading:  21%|██▏       | 10424/49018 [00:04<00:14, 2626.41it/s]Loading:  22%|██▏       | 10687/49018 [00:04<00:14, 2621.69it/s]Loading:  22%|██▏       | 10951/49018 [00:04<00:14, 2624.53it/s]Loading:  23%|██▎       | 11215/49018 [00:04<00:14, 2626.72it/s]Loading:  23%|██▎       | 11478/49018 [00:04<00:14, 2621.39it/s]Loading:  24%|██▍       | 11743/49018 [00:04<00:14, 2627.25it/s]Loading:  24%|██▍       | 12006/49018 [00:05<00:21, 1742.81it/s]Loading:  25%|██▍       | 12247/49018 [00:05<00:19, 1888.91it/s]Loading:  26%|██▌       | 12509/49018 [00:05<00:17, 2063.91it/s]Loading:  26%|██▌       | 12773/49018 [00:05<00:16, 2209.73it/s]Loading:  27%|██▋       | 13035/49018 [00:05<00:15, 2318.11it/s]Loading:  27%|██▋       | 13299/49018 [00:05<00:14, 2405.36it/s]Loading:  28%|██▊       | 13563/49018 [00:05<00:14, 2469.42it/s]Loading:  28%|██▊       | 13827/49018 [00:05<00:13, 2518.13it/s]Loading:  29%|██▊       | 14088/49018 [00:06<00:13, 2544.68it/s]Loading:  29%|██▉       | 14351/49018 [00:06<00:13, 2569.66it/s]Loading:  30%|██▉       | 14614/49018 [00:06<00:13, 2585.08it/s]Loading:  30%|███       | 14878/49018 [00:06<00:13, 2599.20it/s]Loading:  31%|███       | 15141/49018 [00:06<00:12, 2607.57it/s]Loading:  31%|███▏      | 15406/49018 [00:06<00:12, 2619.32it/s]Loading:  32%|███▏      | 15669/49018 [00:06<00:12, 2604.42it/s]Loading:  33%|███▎      | 15931/49018 [00:06<00:12, 2601.06it/s]Loading:  33%|███▎      | 16192/49018 [00:06<00:12, 2586.62it/s]Loading:  34%|███▎      | 16455/49018 [00:06<00:12, 2598.48it/s]Loading:  34%|███▍      | 16718/49018 [00:07<00:12, 2605.74it/s]Loading:  35%|███▍      | 16979/49018 [00:07<00:12, 2606.30it/s]Loading:  35%|███▌      | 17242/49018 [00:07<00:12, 2613.16it/s]Loading:  36%|███▌      | 17504/49018 [00:07<00:12, 2613.47it/s]Loading:  36%|███▌      | 17766/49018 [00:07<00:11, 2613.58it/s]Loading:  37%|███▋      | 18028/49018 [00:07<00:11, 2608.21it/s]Loading:  37%|███▋      | 18289/49018 [00:07<00:11, 2593.59it/s]Loading:  38%|███▊      | 18549/49018 [00:07<00:11, 2578.07it/s]Loading:  38%|███▊      | 18807/49018 [00:07<00:11, 2569.30it/s]Loading:  39%|███▉      | 19064/49018 [00:07<00:11, 2561.86it/s]Loading:  39%|███▉      | 19321/49018 [00:08<00:11, 2561.16it/s]Loading:  40%|███▉      | 19578/49018 [00:08<00:11, 2551.76it/s]Loading:  40%|████      | 19835/49018 [00:08<00:11, 2556.54it/s]Loading:  41%|████      | 20091/49018 [00:08<00:11, 2550.75it/s]Loading:  42%|████▏     | 20347/49018 [00:08<00:11, 2550.05it/s]Loading:  42%|████▏     | 20603/49018 [00:08<00:15, 1842.54it/s]Loading:  42%|████▏     | 20832/49018 [00:08<00:14, 1946.57it/s]Loading:  43%|████▎     | 21084/49018 [00:08<00:13, 2089.83it/s]Loading:  44%|████▎     | 21334/49018 [00:09<00:12, 2196.47it/s]Loading:  44%|████▍     | 21586/49018 [00:09<00:12, 2283.80it/s]Loading:  45%|████▍     | 21837/49018 [00:09<00:11, 2346.53it/s]Loading:  45%|████▌     | 22090/49018 [00:09<00:11, 2399.07it/s]Loading:  46%|████▌     | 22343/49018 [00:09<00:10, 2436.22it/s]Loading:  46%|████▌     | 22595/49018 [00:09<00:10, 2460.28it/s]Loading:  47%|████▋     | 22848/49018 [00:09<00:10, 2479.45it/s]Loading:  47%|████▋     | 23099/49018 [00:09<00:10, 2486.74it/s]Loading:  48%|████▊     | 23353/49018 [00:09<00:10, 2500.57it/s]Loading:  48%|████▊     | 23605/49018 [00:09<00:10, 2502.21it/s]Loading:  49%|████▊     | 23858/49018 [00:10<00:10, 2507.88it/s]Loading:  49%|████▉     | 24110/49018 [00:10<00:09, 2505.41it/s]Loading:  50%|████▉     | 24362/49018 [00:10<00:09, 2509.32it/s]Loading:  50%|█████     | 24614/49018 [00:10<00:09, 2510.96it/s]Loading:  51%|█████     | 24867/49018 [00:10<00:09, 2513.34it/s]Loading:  51%|█████     | 25119/49018 [00:10<00:09, 2509.62it/s]Loading:  52%|█████▏    | 25372/49018 [00:10<00:09, 2513.71it/s]Loading:  52%|█████▏    | 25624/49018 [00:10<00:09, 2512.12it/s]Loading:  53%|█████▎    | 25876/49018 [00:11<00:15, 1462.21it/s]Loading:  53%|█████▎    | 26090/49018 [00:11<00:14, 1596.77it/s]Loading:  54%|█████▎    | 26337/49018 [00:11<00:12, 1789.46it/s]Loading:  54%|█████▍    | 26594/49018 [00:11<00:11, 1975.88it/s]Loading:  55%|█████▍    | 26850/49018 [00:11<00:10, 2123.02it/s]Loading:  55%|█████▌    | 27106/49018 [00:11<00:09, 2238.20it/s]Loading:  56%|█████▌    | 27362/49018 [00:11<00:09, 2326.07it/s]Loading:  56%|█████▋    | 27618/49018 [00:11<00:08, 2390.15it/s]Loading:  57%|█████▋    | 27874/49018 [00:11<00:08, 2438.39it/s]Loading:  57%|█████▋    | 28132/49018 [00:11<00:08, 2478.48it/s]Loading:  58%|█████▊    | 28387/49018 [00:12<00:08, 2499.03it/s]Loading:  58%|█████▊    | 28641/49018 [00:12<00:08, 2510.55it/s]Loading:  59%|█████▉    | 28895/49018 [00:12<00:08, 2513.07it/s]Loading:  59%|█████▉    | 29150/49018 [00:12<00:07, 2522.34it/s]Loading:  60%|█████▉    | 29404/49018 [00:12<00:07, 2524.79it/s]Loading:  61%|██████    | 29658/49018 [00:12<00:07, 2502.17it/s]Loading:  61%|██████    | 29915/49018 [00:12<00:07, 2521.01it/s]Loading:  62%|██████▏   | 30170/49018 [00:12<00:07, 2529.40it/s]Loading:  62%|██████▏   | 30425/49018 [00:12<00:07, 2534.92it/s]Loading:  63%|██████▎   | 30679/49018 [00:12<00:07, 2535.70it/s]Loading:  63%|██████▎   | 30933/49018 [00:13<00:07, 2535.21it/s]Loading:  64%|██████▎   | 31187/49018 [00:13<00:07, 2533.66it/s]Loading:  64%|██████▍   | 31444/49018 [00:13<00:06, 2543.59it/s]Loading:  65%|██████▍   | 31700/49018 [00:13<00:06, 2545.91it/s]Loading:  65%|██████▌   | 31955/49018 [00:13<00:06, 2536.86it/s]Loading:  66%|██████▌   | 32209/49018 [00:13<00:06, 2528.51it/s]Loading:  66%|██████▌   | 32463/49018 [00:13<00:06, 2530.72it/s]Loading:  67%|██████▋   | 32717/49018 [00:13<00:06, 2529.61it/s]Loading:  67%|██████▋   | 32970/49018 [00:13<00:06, 2524.84it/s]Loading:  68%|██████▊   | 33225/49018 [00:13<00:06, 2530.55it/s]Loading:  68%|██████▊   | 33479/49018 [00:14<00:10, 1539.16it/s]Loading:  69%|██████▊   | 33699/49018 [00:14<00:09, 1675.02it/s]Loading:  69%|██████▉   | 33951/49018 [00:14<00:08, 1866.54it/s]Loading:  70%|██████▉   | 34206/49018 [00:14<00:07, 2031.95it/s]Loading:  70%|███████   | 34460/49018 [00:14<00:06, 2162.07it/s]Loading:  71%|███████   | 34715/49018 [00:14<00:06, 2264.49it/s]Loading:  71%|███████▏  | 34958/49018 [00:14<00:06, 2309.50it/s]Loading:  72%|███████▏  | 35212/49018 [00:14<00:05, 2374.07it/s]Loading:  72%|███████▏  | 35463/49018 [00:15<00:05, 2411.98it/s]Loading:  73%|███████▎  | 35715/49018 [00:15<00:05, 2442.73it/s]Loading:  73%|███████▎  | 35968/49018 [00:15<00:05, 2468.23it/s]Loading:  74%|███████▍  | 36219/49018 [00:15<00:05, 2478.33it/s]Loading:  74%|███████▍  | 36472/49018 [00:15<00:05, 2492.83it/s]Loading:  75%|███████▍  | 36724/49018 [00:15<00:04, 2499.69it/s]Loading:  75%|███████▌  | 36977/49018 [00:15<00:04, 2507.99it/s]Loading:  76%|███████▌  | 37229/49018 [00:15<00:04, 2511.56it/s]Loading:  76%|███████▋  | 37481/49018 [00:15<00:04, 2494.96it/s]Loading:  77%|███████▋  | 37733/49018 [00:15<00:04, 2501.17it/s]Loading:  77%|███████▋  | 37984/49018 [00:16<00:11, 928.45it/s] Loading:  78%|███████▊  | 38235/49018 [00:16<00:09, 1144.06it/s]Loading:  79%|███████▊  | 38486/49018 [00:16<00:07, 1366.47it/s]Loading:  79%|███████▉  | 38738/49018 [00:16<00:06, 1583.97it/s]Loading:  80%|███████▉  | 38989/49018 [00:17<00:05, 1780.39it/s]Loading:  80%|████████  | 39242/49018 [00:17<00:05, 1954.06it/s]Loading:  81%|████████  | 39493/49018 [00:17<00:04, 2092.65it/s]Loading:  81%|████████  | 39744/49018 [00:17<00:04, 2201.94it/s]Loading:  82%|████████▏ | 39995/49018 [00:17<00:03, 2284.20it/s]Loading:  82%|████████▏ | 40248/49018 [00:17<00:03, 2352.74it/s]Loading:  83%|████████▎ | 40500/49018 [00:17<00:03, 2399.38it/s]Loading:  83%|████████▎ | 40753/49018 [00:17<00:03, 2435.40it/s]Loading:  84%|████████▎ | 41006/49018 [00:17<00:03, 2460.92it/s]Loading:  84%|████████▍ | 41260/49018 [00:17<00:03, 2483.02it/s]Loading:  85%|████████▍ | 41513/49018 [00:18<00:03, 2496.37it/s]Loading:  85%|████████▌ | 41766/49018 [00:18<00:02, 2501.56it/s]Loading:  86%|████████▌ | 42020/49018 [00:18<00:02, 2511.70it/s]Loading:  86%|████████▌ | 42273/49018 [00:18<00:02, 2515.34it/s]Loading:  87%|████████▋ | 42526/49018 [00:18<00:02, 2514.66it/s]Loading:  87%|████████▋ | 42779/49018 [00:18<00:02, 2516.50it/s]Loading:  88%|████████▊ | 43032/49018 [00:18<00:02, 2518.56it/s]Loading:  88%|████████▊ | 43286/49018 [00:18<00:02, 2522.68it/s]Loading:  89%|████████▉ | 43539/49018 [00:18<00:02, 2522.68it/s]Loading:  89%|████████▉ | 43793/49018 [00:18<00:02, 2526.60it/s]Loading:  90%|████████▉ | 44046/49018 [00:19<00:01, 2519.10it/s]Loading:  90%|█████████ | 44300/49018 [00:19<00:01, 2524.85it/s]Loading:  91%|█████████ | 44554/49018 [00:19<00:01, 2526.94it/s]Loading:  91%|█████████▏| 44809/49018 [00:19<00:01, 2532.09it/s]Loading:  92%|█████████▏| 45063/49018 [00:19<00:01, 2525.86it/s]Loading:  92%|█████████▏| 45317/49018 [00:19<00:01, 2529.01it/s]Loading:  93%|█████████▎| 45570/49018 [00:19<00:01, 2522.50it/s]Loading:  93%|█████████▎| 45824/49018 [00:19<00:01, 2524.96it/s]Loading:  94%|█████████▍| 46077/49018 [00:19<00:01, 2525.58it/s]Loading:  95%|█████████▍| 46331/49018 [00:19<00:01, 2527.19it/s]Loading:  95%|█████████▌| 46584/49018 [00:20<00:00, 2525.50it/s]Loading:  96%|█████████▌| 46837/49018 [00:20<00:00, 2523.29it/s]Loading:  96%|█████████▌| 47090/49018 [00:20<00:00, 2524.86it/s]Loading:  97%|█████████▋| 47344/49018 [00:20<00:00, 2526.71it/s]Loading:  97%|█████████▋| 47597/49018 [00:20<00:00, 2526.26it/s]Loading:  98%|█████████▊| 47851/49018 [00:20<00:00, 2528.26it/s]Loading:  98%|█████████▊| 48105/49018 [00:20<00:00, 2531.60it/s]Loading:  99%|█████████▊| 48359/49018 [00:20<00:00, 2530.36it/s]Loading:  99%|█████████▉| 48613/49018 [00:20<00:00, 2531.26it/s]Loading: 100%|█████████▉| 48867/49018 [00:20<00:00, 2527.76it/s]Loading: 100%|██████████| 49018/49018 [00:21<00:00, 2331.01it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 17:42:02.965279973 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 17:42:02.965294120 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 17:42:02.965285394 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 17:42:02.965278420 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 17:42:02.965282959 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 17:42:02.965275986 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 17:42:09.737796380 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 17:42:09.737798965 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 17:42:09.737792773 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 17:42:09.737795639 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 17:42:09.737800167 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 17:42:09.737788385 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 17:42:11.230837609 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 17:42:11.230841306 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 17:42:11.230844943 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 17:42:11.230833942 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 17:42:11.230835716 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 17:42:11.230825847 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 17:42:11.230829955 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 17:42:12.772111677 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 17:42:12.772115915 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 17:42:12.772102049 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 17:42:12.772094144 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 17:42:12.772121175 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 17:42:12.772098041 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 17:42:12.772106106 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 17:42:12.772108912 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 17:42:12.686552465 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 17:42:12.686545231 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 17:42:12.686547706 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 17:42:12.686550301 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 17:42:14.060807390 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 17:42:14.061074778 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 17:42:14.061115976 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 17:42:14.705802892 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 17:42:14.705799516 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 17:42:14.818511545 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 17:42:14.028509150 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 17:42:14.289628820 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 17:42:15.197157720 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:36<30:11, 36.97s/it]Train:   4%|▍         | 2/50 [00:37<12:18, 15.39s/it]Train:   6%|▌         | 3/50 [00:37<06:37,  8.46s/it]Train:   8%|▊         | 4/50 [00:37<03:59,  5.20s/it]Train:  10%|█         | 5/50 [00:37<02:32,  3.40s/it]Train:  12%|█▏        | 6/50 [00:38<01:41,  2.31s/it]Train:  14%|█▍        | 7/50 [00:38<01:09,  1.62s/it]Train:  16%|█▌        | 8/50 [00:38<00:49,  1.17s/it]Train:  18%|█▊        | 9/50 [00:38<00:35,  1.15it/s]Train:  20%|██        | 10/50 [00:38<00:26,  1.51it/s]Train:  22%|██▏       | 11/50 [00:39<00:20,  1.91it/s]Train:  24%|██▍       | 12/50 [00:39<00:16,  2.35it/s]Train:  26%|██▌       | 13/50 [00:39<00:13,  2.79it/s]Train:  28%|██▊       | 14/50 [00:39<00:11,  3.22it/s]Train:  30%|███       | 15/50 [00:39<00:09,  3.60it/s]Train:  32%|███▏      | 16/50 [00:40<00:08,  3.93it/s]Train:  34%|███▍      | 17/50 [00:40<00:07,  4.17it/s]Train:  36%|███▌      | 18/50 [00:40<00:07,  4.37it/s]Train:  38%|███▊      | 19/50 [00:40<00:06,  4.53it/s]Train:  40%|████      | 20/50 [00:40<00:06,  4.64it/s]Train:  42%|████▏     | 21/50 [00:41<00:06,  4.73it/s]Train:  44%|████▍     | 22/50 [00:41<00:05,  4.80it/s]Train:  46%|████▌     | 23/50 [00:41<00:05,  4.84it/s]Train:  48%|████▊     | 24/50 [00:41<00:05,  4.86it/s]Train:  50%|█████     | 25/50 [00:41<00:05,  4.90it/s]Train:  52%|█████▏    | 26/50 [00:42<00:04,  4.90it/s]Train:  54%|█████▍    | 27/50 [00:42<00:04,  4.91it/s]Train:  56%|█████▌    | 28/50 [00:42<00:04,  4.92it/s]Train:  58%|█████▊    | 29/50 [00:42<00:04,  4.93it/s]Train:  60%|██████    | 30/50 [00:42<00:04,  4.93it/s]Train:  62%|██████▏   | 31/50 [00:43<00:03,  4.93it/s]Train:  64%|██████▍   | 32/50 [00:43<00:03,  4.94it/s]Train:  66%|██████▌   | 33/50 [00:43<00:03,  4.93it/s]Train:  68%|██████▊   | 34/50 [00:43<00:03,  4.94it/s]Train:  70%|███████   | 35/50 [00:43<00:03,  4.95it/s]Train:  72%|███████▏  | 36/50 [00:44<00:02,  4.96it/s]Train:  74%|███████▍  | 37/50 [00:44<00:02,  4.96it/s]Train:  76%|███████▌  | 38/50 [00:44<00:02,  4.95it/s]Train:  78%|███████▊  | 39/50 [00:44<00:02,  4.95it/s]Train:  80%|████████  | 40/50 [00:44<00:02,  4.97it/s]Train:  82%|████████▏ | 41/50 [00:45<00:01,  4.96it/s]Train:  84%|████████▍ | 42/50 [00:45<00:01,  4.97it/s]Train:  86%|████████▌ | 43/50 [00:45<00:01,  4.97it/s]Train:  88%|████████▊ | 44/50 [00:45<00:01,  4.95it/s]Train:  90%|█████████ | 45/50 [00:45<00:01,  4.97it/s]Train:  92%|█████████▏| 46/50 [00:46<00:00,  4.97it/s]Train:  94%|█████████▍| 47/50 [00:46<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:46<00:00,  4.98it/s]Train:  98%|█████████▊| 49/50 [00:46<00:00,  4.98it/s]Train: 100%|██████████| 50/50 [00:46<00:00,  4.96it/s]Train: 100%|██████████| 50/50 [00:47<00:00,  1.06it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:01<01:12,  1.48s/it]Train:   4%|▍         | 2/50 [00:01<00:35,  1.37it/s]Train:   6%|▌         | 3/50 [00:01<00:23,  2.04it/s]Train:   8%|▊         | 4/50 [00:02<00:17,  2.66it/s]Train:  10%|█         | 5/50 [00:02<00:14,  3.20it/s]Train:  12%|█▏        | 6/50 [00:02<00:12,  3.63it/s]Train:  14%|█▍        | 7/50 [00:02<00:10,  4.00it/s]Train:  16%|█▌        | 8/50 [00:02<00:09,  4.27it/s]Train:  18%|█▊        | 9/50 [00:03<00:09,  4.48it/s]Train:  20%|██        | 10/50 [00:03<00:08,  4.61it/s]Train:  22%|██▏       | 11/50 [00:03<00:08,  4.71it/s]Train:  24%|██▍       | 12/50 [00:03<00:07,  4.79it/s]Train:  26%|██▌       | 13/50 [00:03<00:07,  4.83it/s]Train:  28%|██▊       | 14/50 [00:04<00:07,  4.86it/s]Train:  30%|███       | 15/50 [00:04<00:07,  4.91it/s]Train:  32%|███▏      | 16/50 [00:04<00:06,  4.91it/s]Train:  34%|███▍      | 17/50 [00:04<00:06,  4.93it/s]Train:  36%|███▌      | 18/50 [00:04<00:06,  4.94it/s]Train:  38%|███▊      | 19/50 [00:05<00:06,  4.94it/s]Train:  40%|████      | 20/50 [00:05<00:06,  4.94it/s]Train:  42%|████▏     | 21/50 [00:05<00:05,  4.95it/s]Train:  44%|████▍     | 22/50 [00:05<00:05,  4.96it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.96it/s]Train:  48%|████▊     | 24/50 [00:06<00:05,  4.95it/s]Train:  50%|█████     | 25/50 [00:06<00:05,  4.97it/s]Train:  52%|█████▏    | 26/50 [00:06<00:04,  4.99it/s]Train:  54%|█████▍    | 27/50 [00:06<00:04,  4.98it/s]Train:  56%|█████▌    | 28/50 [00:07<00:06,  3.56it/s]Train:  58%|█████▊    | 29/50 [00:08<00:13,  1.55it/s]Train:  60%|██████    | 30/50 [00:08<00:10,  1.92it/s]Train:  62%|██████▏   | 31/50 [00:09<00:08,  2.32it/s]Train:  64%|██████▍   | 32/50 [00:09<00:06,  2.74it/s]Train:  66%|██████▌   | 33/50 [00:09<00:05,  3.15it/s]Train:  68%|██████▊   | 34/50 [00:09<00:04,  3.54it/s]Train:  70%|███████   | 35/50 [00:09<00:03,  3.87it/s]Train:  72%|███████▏  | 36/50 [00:10<00:03,  4.15it/s]Train:  74%|███████▍  | 37/50 [00:10<00:03,  4.31it/s]Train:  76%|███████▌  | 38/50 [00:10<00:02,  4.49it/s]Train:  78%|███████▊  | 39/50 [00:10<00:02,  4.62it/s]Train:  80%|████████  | 40/50 [00:10<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:11<00:01,  4.80it/s]Train:  84%|████████▍ | 42/50 [00:11<00:01,  4.86it/s]Train:  86%|████████▌ | 43/50 [00:11<00:01,  4.89it/s]Train:  88%|████████▊ | 44/50 [00:11<00:01,  4.91it/s]Train:  90%|█████████ | 45/50 [00:11<00:01,  4.92it/s]Train:  92%|█████████▏| 46/50 [00:12<00:00,  4.94it/s]Train:  94%|█████████▍| 47/50 [00:12<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:12<00:00,  4.97it/s]Train:  98%|█████████▊| 49/50 [00:12<00:00,  4.98it/s]Train: 100%|██████████| 50/50 [00:12<00:00,  4.97it/s]Train: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.45it/s]Train:   4%|▍         | 2/50 [00:00<00:11,  4.20it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.53it/s]Train:   8%|▊         | 4/50 [00:03<00:49,  1.07s/it]Train:  10%|█         | 5/50 [00:03<00:35,  1.28it/s]Train:  12%|█▏        | 6/50 [00:03<00:26,  1.63it/s]Train:  14%|█▍        | 7/50 [00:03<00:21,  1.97it/s]Train:  16%|█▌        | 8/50 [00:04<00:17,  2.33it/s]Train:  18%|█▊        | 9/50 [00:04<00:15,  2.73it/s]Train:  20%|██        | 10/50 [00:04<00:12,  3.11it/s]Train:  22%|██▏       | 11/50 [00:04<00:11,  3.47it/s]Train:  24%|██▍       | 12/50 [00:05<00:10,  3.78it/s]Train:  26%|██▌       | 13/50 [00:05<00:09,  4.05it/s]Train:  28%|██▊       | 14/50 [00:05<00:08,  4.29it/s]Train:  30%|███       | 15/50 [00:05<00:07,  4.47it/s]Train:  32%|███▏      | 16/50 [00:05<00:07,  4.61it/s]Train:  34%|███▍      | 17/50 [00:06<00:06,  4.72it/s]Train:  36%|███▌      | 18/50 [00:06<00:06,  4.80it/s]Train:  38%|███▊      | 19/50 [00:06<00:06,  4.86it/s]Train:  40%|████      | 20/50 [00:06<00:06,  4.89it/s]Train:  42%|████▏     | 21/50 [00:06<00:05,  4.92it/s]Train:  44%|████▍     | 22/50 [00:07<00:05,  4.93it/s]Train:  46%|████▌     | 23/50 [00:07<00:05,  4.95it/s]Train:  48%|████▊     | 24/50 [00:07<00:05,  4.95it/s]Train:  50%|█████     | 25/50 [00:07<00:05,  4.95it/s]Train:  52%|█████▏    | 26/50 [00:07<00:04,  4.95it/s]Train:  54%|█████▍    | 27/50 [00:08<00:04,  4.94it/s]Train:  56%|█████▌    | 28/50 [00:08<00:04,  4.94it/s]Train:  58%|█████▊    | 29/50 [00:08<00:04,  4.94it/s]Train:  60%|██████    | 30/50 [00:08<00:04,  4.95it/s]Train:  62%|██████▏   | 31/50 [00:08<00:03,  4.96it/s]Train:  64%|██████▍   | 32/50 [00:09<00:03,  4.97it/s]Train:  66%|██████▌   | 33/50 [00:09<00:03,  4.97it/s]Train:  68%|██████▊   | 34/50 [00:09<00:03,  4.98it/s]Train:  70%|███████   | 35/50 [00:09<00:03,  4.96it/s]Train:  72%|███████▏  | 36/50 [00:09<00:02,  4.97it/s]Train:  74%|███████▍  | 37/50 [00:10<00:02,  4.96it/s]Train:  76%|███████▌  | 38/50 [00:10<00:02,  4.96it/s]Train:  78%|███████▊  | 39/50 [00:10<00:02,  4.99it/s]Train:  80%|████████  | 40/50 [00:10<00:02,  4.98it/s]Train:  82%|████████▏ | 41/50 [00:10<00:01,  4.97it/s]Train:  84%|████████▍ | 42/50 [00:11<00:02,  3.64it/s]Train:  86%|████████▌ | 43/50 [00:13<00:06,  1.11it/s]Train:  88%|████████▊ | 44/50 [00:13<00:04,  1.42it/s]Train:  90%|█████████ | 45/50 [00:14<00:02,  1.75it/s]Train:  92%|█████████▏| 46/50 [00:14<00:01,  2.12it/s]Train:  94%|█████████▍| 47/50 [00:14<00:01,  2.51it/s]Train:  96%|█████████▌| 48/50 [00:14<00:00,  2.89it/s]Train:  98%|█████████▊| 49/50 [00:15<00:00,  3.26it/s]Train: 100%|██████████| 50/50 [00:15<00:00,  3.58it/s]Train: 100%|██████████| 50/50 [00:15<00:00,  3.25it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.10it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.96it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.35it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.57it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.71it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.79it/s]Train:  14%|█▍        | 7/50 [00:01<00:08,  4.86it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.90it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.91it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.93it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.94it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.95it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.95it/s]Train:  28%|██▊       | 14/50 [00:02<00:07,  4.94it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.95it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.96it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.94it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.97it/s]Train:  38%|███▊      | 19/50 [00:03<00:06,  4.97it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.96it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.97it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.97it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.97it/s]Train:  48%|████▊     | 24/50 [00:04<00:05,  4.97it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.96it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.95it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.95it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.94it/s]Train:  58%|█████▊    | 29/50 [00:05<00:04,  4.94it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.95it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.96it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.95it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.96it/s]Train:  68%|██████▊   | 34/50 [00:06<00:03,  4.95it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.96it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.96it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.97it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.99it/s]Train:  78%|███████▊  | 39/50 [00:07<00:02,  4.97it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.97it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.97it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.94it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.94it/s]Train:  88%|████████▊ | 44/50 [00:08<00:01,  4.95it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.95it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.96it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.97it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.97it/s]Train:  98%|█████████▊| 49/50 [00:09<00:00,  4.96it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.95it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.89it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.42it/s]Train:   4%|▍         | 2/50 [00:00<00:11,  4.22it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.53it/s]Train:   8%|▊         | 4/50 [00:00<00:09,  4.67it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.77it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.83it/s]Train:  14%|█▍        | 7/50 [00:01<00:08,  4.86it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.86it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.90it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.91it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.94it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.95it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.95it/s]Train:  28%|██▊       | 14/50 [00:02<00:07,  4.95it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.95it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.95it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.97it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.96it/s]Train:  38%|███▊      | 19/50 [00:03<00:06,  4.97it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.97it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.97it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.96it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.96it/s]Train:  48%|████▊     | 24/50 [00:04<00:05,  4.97it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.97it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.97it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.98it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.98it/s]Train:  58%|█████▊    | 29/50 [00:05<00:04,  4.95it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.96it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.98it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.98it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.95it/s]Train:  68%|██████▊   | 34/50 [00:06<00:03,  4.97it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.96it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.97it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.97it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.97it/s]Train:  78%|███████▊  | 39/50 [00:07<00:02,  4.97it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.97it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.94it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.94it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.94it/s]Train:  88%|████████▊ | 44/50 [00:08<00:01,  4.94it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.95it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.95it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.96it/s]Train:  98%|█████████▊| 49/50 [00:09<00:00,  4.95it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.96it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.90it/s]
0: Process 0 - Local timer:  train_validate_test  :  95.98
0: Process 0 - Local timer:  load_data  :  93.45
0: Process 0 - Local timer:  create_model  :  1.39
0: Minimum timers: 
0: train_validate_test  :  95.94
0: load_data  :  93.45
0: create_model  :  1.27
0: Maximum timers: 
0: train_validate_test  :  95.99
0: load_data  :  93.94
0: create_model  :  1.51
0: Average timers: 
0: train_validate_test  :  95.96
0: load_data  :  93.73
0: create_model  :  1.37
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 17:43:15.792217666 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 17:43:16.396535156 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 17:43:16.285226208 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 17:43:16.285227470 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 17:43:16.396775302 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 17:43:16.285209837 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 17:43:16.396815698 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 17:43:16.285211350 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 17:43:16.285547587 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 17:43:16.285211681 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 17:43:16.397221718 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 17:43:16.397930511 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 17:43:16.286581937 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 17:43:16.398309139 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 17:43:16.956209900 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 17:43:16.956200492 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 17:43:16.956236440 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 17:43:16.956207685 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 17:43:16.956198678 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 17:43:16.956216402 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 17:43:16.956229356 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 17:43:16.288053796 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 17:43:16.956246579 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 17:43:16.000832396 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 17:43:16.000821215 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 17:43:16.659964275 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 17:43:16.000824140 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 17:43:16.659961229 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 17:43:16.000839279 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 17:43:16.659944367 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 17:43:16.000816646 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 17:43:16.659952082 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 17:43:16.000846863 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 17:43:16.659976518 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 17:43:16.000835602 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 17:43:16.659966559 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 17:43:16.000830242 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 17:43:16.659965006 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 17:43:16.400438776 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 17:43:16.660767101 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 05:43:19 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 05:43:19 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_2 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
Distributed data parallel: nccl master at frontier07676:8889
[W424 17:43:33.261260580 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261386007 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261406346 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261521333 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549298951 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261531382 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261556330 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549388801 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549364255 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549354145 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217833802 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549398730 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217859060 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.549411675 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217926989 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217924654 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.261768481 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217869319 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.217909035 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.218188946 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.262394004 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.218908501 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.550569901 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.550868658 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.666641989 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.666629616 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.666653171 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.666674691 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.666844323 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.668686886 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.669016991 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.683948894 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.494614115 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.494859822 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.495200550 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.495197965 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.495202484 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.496175042 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.496221610 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:43:33.496744574 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_2 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Read attr time (sec):  0.0014255046844482422
0: read and bcast: trainset/x/variable_count 0.17041826248168945
0: read and bcast: trainset/x/variable_offset 0.3439157009124756
0: read and bcast: trainset/x/variable_dim 0.34429287910461426
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.51963210105896
0: read and bcast: trainset/edge_index/variable_offset 0.7233459949493408
0: read and bcast: trainset/edge_index/variable_dim 0.7335331439971924
0: read and bcast: trainset/edge_attr/variable_count 0.9106273651123047
0: read and bcast: trainset/edge_attr/variable_offset 1.085219383239746
0: read and bcast: trainset/edge_attr/variable_dim 1.0947520732879639
0: read and bcast: trainset/pos/variable_count 1.273611068725586
0: read and bcast: trainset/pos/variable_offset 1.4520554542541504
0: read and bcast: trainset/pos/variable_dim 1.462883710861206
0: read and bcast: trainset/energy/variable_count 1.6361479759216309
0: read and bcast: trainset/energy/variable_offset 1.8088891506195068
0: read and bcast: trainset/energy/variable_dim 1.819868564605713
0: read and bcast: trainset/forces/variable_count 1.9941065311431885
0: read and bcast: trainset/forces/variable_offset 2.1655337810516357
0: read and bcast: trainset/forces/variable_dim 2.176389217376709
0: read and bcast: trainset/y/variable_count 2.350487232208252
0: read and bcast: trainset/y/variable_offset 2.5234851837158203
0: read and bcast: trainset/y/variable_dim 2.5343005657196045
0: Overall time (sec):  2.535946846008301
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.540295124053955
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007524490356445312
0: read and bcast: valset/x/variable_count 0.007692098617553711
0: read and bcast: valset/x/variable_offset 0.014794111251831055
0: read and bcast: valset/x/variable_dim 0.014949798583984375
0: read and bcast: valset/edge_index/variable_count 0.022019386291503906
0: read and bcast: valset/edge_index/variable_offset 0.029233217239379883
0: read and bcast: valset/edge_index/variable_dim 0.029413938522338867
0: read and bcast: valset/edge_attr/variable_count 0.03678178787231445
0: read and bcast: valset/edge_attr/variable_offset 0.043840646743774414
0: read and bcast: valset/edge_attr/variable_dim 0.0439908504486084
0: read and bcast: valset/pos/variable_count 0.05084657669067383
0: read and bcast: valset/pos/variable_offset 0.05799055099487305
0: read and bcast: valset/pos/variable_dim 0.0581669807434082
0: read and bcast: valset/energy/variable_count 0.06506991386413574
0: read and bcast: valset/energy/variable_offset 0.0720064640045166
0: read and bcast: valset/energy/variable_dim 0.0721592903137207
0: read and bcast: valset/forces/variable_count 0.07900714874267578
0: read and bcast: valset/forces/variable_offset 0.08597445487976074
0: read and bcast: valset/forces/variable_dim 0.08612561225891113
0: read and bcast: valset/y/variable_count 0.09308481216430664
0: read and bcast: valset/y/variable_offset 0.10003662109375
0: read and bcast: valset/y/variable_dim 0.1002042293548584
0: Overall time (sec):  0.10115957260131836
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10397768020629883
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007491111755371094
0: read and bcast: testset/x/variable_count 0.007384538650512695
0: read and bcast: testset/x/variable_offset 0.014406442642211914
0: read and bcast: testset/x/variable_dim 0.014573097229003906
0: read and bcast: testset/edge_index/variable_count 0.0215761661529541
0: read and bcast: testset/edge_index/variable_offset 0.028751850128173828
0: read and bcast: testset/edge_index/variable_dim 0.028957605361938477
0: read and bcast: testset/edge_attr/variable_count 0.03584098815917969
0: read and bcast: testset/edge_attr/variable_offset 0.04302215576171875
0: read and bcast: testset/edge_attr/variable_dim 0.0431981086730957
0: read and bcast: testset/pos/variable_count 0.05005311965942383
0: read and bcast: testset/pos/variable_offset 0.05713486671447754
0: read and bcast: testset/pos/variable_dim 0.057308197021484375
0: read and bcast: testset/energy/variable_count 0.06434202194213867
0: read and bcast: testset/energy/variable_offset 0.0713191032409668
0: read and bcast: testset/energy/variable_dim 0.07148146629333496
0: read and bcast: testset/forces/variable_count 0.07845211029052734
0: read and bcast: testset/forces/variable_offset 0.08574175834655762
0: read and bcast: testset/forces/variable_dim 0.08592987060546875
0: read and bcast: testset/y/variable_count 0.0928030014038086
0: read and bcast: testset/y/variable_offset 0.10005331039428711
0: read and bcast: testset/y/variable_dim 0.10023117065429688
0: Overall time (sec):  0.10117220878601074
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10398697853088379
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
9 qm7x nsplit: 7551412 10 755141
6 qm7x nsplit: 7551412 10 755142
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
29 transition1x nsplit: 8680250 11 789114
34 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
19 alexandria nsplit: 9705384 11 882308
20 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
22 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
26 alexandria nsplit: 9705384 11 882307
18 alexandria nsplit: 9705384 11 882308
24 alexandria nsplit: 9705384 11 882308
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
0: Adios reading time (sec):  0.5384795665740967
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.14109396934509277
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.1397862434387207
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 213/64000 [00:00<00:30, 2125.17it/s]Loading:   1%|          | 462/64000 [00:00<00:27, 2338.28it/s]Loading:   1%|          | 719/64000 [00:00<00:25, 2442.59it/s]Loading:   2%|▏         | 986/64000 [00:00<00:24, 2529.78it/s]Loading:   2%|▏         | 1247/64000 [00:00<00:24, 2558.05it/s]Loading:   2%|▏         | 1505/64000 [00:00<00:24, 2562.63it/s]Loading:   3%|▎         | 1762/64000 [00:00<00:24, 2559.82it/s]Loading:   3%|▎         | 2018/64000 [00:00<00:24, 2559.13it/s]Loading:   4%|▎         | 2274/64000 [00:00<00:24, 2553.14it/s]Loading:   4%|▍         | 2531/64000 [00:01<00:24, 2556.27it/s]Loading:   4%|▍         | 2787/64000 [00:01<00:23, 2557.26it/s]Loading:   5%|▍         | 3043/64000 [00:01<00:23, 2557.03it/s]Loading:   5%|▌         | 3299/64000 [00:01<00:23, 2555.86it/s]Loading:   6%|▌         | 3555/64000 [00:01<00:23, 2553.15it/s]Loading:   6%|▌         | 3811/64000 [00:01<00:23, 2547.07it/s]Loading:   6%|▋         | 4066/64000 [00:01<00:34, 1713.66it/s]Loading:   7%|▋         | 4321/64000 [00:01<00:31, 1900.26it/s]Loading:   7%|▋         | 4578/64000 [00:01<00:28, 2061.36it/s]Loading:   8%|▊         | 4836/64000 [00:02<00:26, 2192.63it/s]Loading:   8%|▊         | 5092/64000 [00:02<00:25, 2289.48it/s]Loading:   8%|▊         | 5349/64000 [00:02<00:24, 2365.74it/s]Loading:   9%|▉         | 5605/64000 [00:02<00:24, 2418.52it/s]Loading:   9%|▉         | 5861/64000 [00:02<00:23, 2458.95it/s]Loading:  10%|▉         | 6118/64000 [00:02<00:23, 2488.89it/s]Loading:  10%|▉         | 6375/64000 [00:02<00:22, 2511.99it/s]Loading:  10%|█         | 6631/64000 [00:02<00:22, 2525.77it/s]Loading:  11%|█         | 6888/64000 [00:02<00:22, 2536.89it/s]Loading:  11%|█         | 7145/64000 [00:02<00:22, 2545.98it/s]Loading:  12%|█▏        | 7403/64000 [00:03<00:22, 2553.34it/s]Loading:  12%|█▏        | 7660/64000 [00:03<00:22, 2557.56it/s]Loading:  12%|█▏        | 7917/64000 [00:03<00:21, 2556.01it/s]Loading:  13%|█▎        | 8173/64000 [00:03<00:21, 2546.94it/s]Loading:  13%|█▎        | 8428/64000 [00:03<00:21, 2541.13it/s]Loading:  14%|█▎        | 8683/64000 [00:03<00:21, 2541.38it/s]Loading:  14%|█▍        | 8938/64000 [00:03<00:21, 2540.38it/s]Loading:  14%|█▍        | 9196/64000 [00:03<00:21, 2549.85it/s]Loading:  15%|█▍        | 9452/64000 [00:03<00:21, 2550.38it/s]Loading:  15%|█▌        | 9708/64000 [00:03<00:21, 2551.88it/s]Loading:  16%|█▌        | 9964/64000 [00:04<00:21, 2552.00it/s]Loading:  16%|█▌        | 10221/64000 [00:04<00:21, 2556.05it/s]Loading:  16%|█▋        | 10478/64000 [00:04<00:20, 2558.98it/s]Loading:  17%|█▋        | 10734/64000 [00:04<00:20, 2556.25it/s]Loading:  17%|█▋        | 10990/64000 [00:04<00:20, 2552.56it/s]Loading:  18%|█▊        | 11246/64000 [00:04<00:20, 2545.27it/s]Loading:  18%|█▊        | 11501/64000 [00:04<00:20, 2540.77it/s]Loading:  18%|█▊        | 11756/64000 [00:04<00:20, 2539.02it/s]Loading:  19%|█▉        | 12012/64000 [00:04<00:20, 2543.01it/s]Loading:  19%|█▉        | 12267/64000 [00:05<00:29, 1752.73it/s]Loading:  20%|█▉        | 12523/64000 [00:05<00:26, 1934.80it/s]Loading:  20%|█▉        | 12776/64000 [00:05<00:24, 2080.17it/s]Loading:  20%|██        | 13032/64000 [00:05<00:23, 2203.17it/s]Loading:  21%|██        | 13287/64000 [00:05<00:22, 2296.08it/s]Loading:  21%|██        | 13544/64000 [00:05<00:21, 2372.23it/s]Loading:  22%|██▏       | 13798/64000 [00:05<00:20, 2419.88it/s]Loading:  22%|██▏       | 14053/64000 [00:05<00:20, 2457.06it/s]Loading:  22%|██▏       | 14311/64000 [00:05<00:19, 2491.28it/s]Loading:  23%|██▎       | 14568/64000 [00:06<00:19, 2513.39it/s]Loading:  23%|██▎       | 14826/64000 [00:06<00:19, 2532.66it/s]Loading:  24%|██▎       | 15083/64000 [00:06<00:19, 2542.66it/s]Loading:  24%|██▍       | 15339/64000 [00:06<00:19, 2538.70it/s]Loading:  24%|██▍       | 15595/64000 [00:06<00:19, 2542.87it/s]Loading:  25%|██▍       | 15850/64000 [00:06<00:18, 2542.20it/s]Loading:  25%|██▌       | 16105/64000 [00:06<00:18, 2537.88it/s]Loading:  26%|██▌       | 16362/64000 [00:06<00:18, 2545.59it/s]Loading:  26%|██▌       | 16617/64000 [00:06<00:18, 2544.72it/s]Loading:  26%|██▋       | 16873/64000 [00:06<00:18, 2549.26it/s]Loading:  27%|██▋       | 17132/64000 [00:07<00:18, 2560.31it/s]Loading:  27%|██▋       | 17389/64000 [00:07<00:18, 2563.16it/s]Loading:  28%|██▊       | 17648/64000 [00:07<00:18, 2570.26it/s]Loading:  28%|██▊       | 17906/64000 [00:07<00:17, 2565.59it/s]Loading:  28%|██▊       | 18164/64000 [00:07<00:17, 2567.91it/s]Loading:  29%|██▉       | 18421/64000 [00:07<00:17, 2564.98it/s]Loading:  29%|██▉       | 18678/64000 [00:07<00:17, 2563.53it/s]Loading:  30%|██▉       | 18935/64000 [00:07<00:17, 2561.37it/s]Loading:  30%|██▉       | 19193/64000 [00:07<00:17, 2566.15it/s]Loading:  30%|███       | 19450/64000 [00:07<00:17, 2550.12it/s]Loading:  31%|███       | 19706/64000 [00:08<00:17, 2550.06it/s]Loading:  31%|███       | 19962/64000 [00:08<00:17, 2550.71it/s]Loading:  32%|███▏      | 20219/64000 [00:08<00:17, 2555.53it/s]Loading:  32%|███▏      | 20475/64000 [00:08<00:17, 2549.55it/s]Loading:  32%|███▏      | 20730/64000 [00:08<00:17, 2544.08it/s]Loading:  33%|███▎      | 20985/64000 [00:08<00:16, 2545.67it/s]Loading:  33%|███▎      | 21240/64000 [00:08<00:16, 2546.00it/s]Loading:  34%|███▎      | 21495/64000 [00:08<00:16, 2546.16it/s]Loading:  34%|███▍      | 21751/64000 [00:08<00:16, 2549.76it/s]Loading:  34%|███▍      | 22007/64000 [00:08<00:16, 2551.03it/s]Loading:  35%|███▍      | 22263/64000 [00:09<00:25, 1664.26it/s]Loading:  35%|███▌      | 22519/64000 [00:09<00:22, 1857.84it/s]Loading:  36%|███▌      | 22772/64000 [00:09<00:20, 2016.84it/s]Loading:  36%|███▌      | 23027/64000 [00:09<00:19, 2151.32it/s]Loading:  36%|███▋      | 23282/64000 [00:09<00:18, 2256.87it/s]Loading:  37%|███▋      | 23536/64000 [00:09<00:17, 2333.24it/s]Loading:  37%|███▋      | 23792/64000 [00:09<00:16, 2394.81it/s]Loading:  38%|███▊      | 24046/64000 [00:09<00:16, 2433.94it/s]Loading:  38%|███▊      | 24300/64000 [00:10<00:16, 2462.77it/s]Loading:  38%|███▊      | 24553/64000 [00:10<00:15, 2479.93it/s]Loading:  39%|███▉      | 24809/64000 [00:10<00:15, 2501.89it/s]Loading:  39%|███▉      | 25063/64000 [00:10<00:15, 2512.37it/s]Loading:  40%|███▉      | 25317/64000 [00:10<00:15, 2520.20it/s]Loading:  40%|███▉      | 25571/64000 [00:10<00:15, 2504.47it/s]Loading:  40%|████      | 25826/64000 [00:10<00:15, 2516.52it/s]Loading:  41%|████      | 26080/64000 [00:10<00:15, 2522.11it/s]Loading:  41%|████      | 26336/64000 [00:10<00:14, 2532.84it/s]Loading:  42%|████▏     | 26590/64000 [00:10<00:14, 2532.79it/s]Loading:  42%|████▏     | 26845/64000 [00:11<00:14, 2535.15it/s]Loading:  42%|████▏     | 27099/64000 [00:11<00:14, 2530.45it/s]Loading:  43%|████▎     | 27353/64000 [00:11<00:14, 2533.15it/s]Loading:  43%|████▎     | 27608/64000 [00:11<00:14, 2536.06it/s]Loading:  44%|████▎     | 27862/64000 [00:11<00:14, 2531.91it/s]Loading:  44%|████▍     | 28116/64000 [00:11<00:14, 2530.30it/s]Loading:  44%|████▍     | 28370/64000 [00:11<00:14, 2531.10it/s]Loading:  45%|████▍     | 28625/64000 [00:11<00:13, 2536.00it/s]Loading:  45%|████▌     | 28879/64000 [00:11<00:13, 2532.43it/s]Loading:  46%|████▌     | 29135/64000 [00:11<00:13, 2538.24it/s]Loading:  46%|████▌     | 29391/64000 [00:12<00:13, 2543.16it/s]Loading:  46%|████▋     | 29646/64000 [00:12<00:13, 2543.84it/s]Loading:  47%|████▋     | 29902/64000 [00:12<00:13, 2545.81it/s]Loading:  47%|████▋     | 30158/64000 [00:12<00:13, 2548.93it/s]Loading:  48%|████▊     | 30413/64000 [00:12<00:13, 2543.94it/s]Loading:  48%|████▊     | 30668/64000 [00:12<00:13, 2541.09it/s]Loading:  48%|████▊     | 30925/64000 [00:12<00:12, 2547.15it/s]Loading:  49%|████▊     | 31180/64000 [00:12<00:12, 2544.36it/s]Loading:  49%|████▉     | 31435/64000 [00:12<00:12, 2540.08it/s]Loading:  50%|████▉     | 31690/64000 [00:12<00:12, 2539.77it/s]Loading:  50%|████▉     | 31944/64000 [00:13<00:12, 2538.39it/s]Loading:  50%|█████     | 32199/64000 [00:13<00:12, 2540.56it/s]Loading:  51%|█████     | 32455/64000 [00:13<00:12, 2543.57it/s]Loading:  51%|█████     | 32710/64000 [00:13<00:12, 2540.09it/s]Loading:  52%|█████▏    | 32965/64000 [00:13<00:12, 2537.46it/s]Loading:  52%|█████▏    | 33220/64000 [00:13<00:12, 2540.76it/s]Loading:  52%|█████▏    | 33478/64000 [00:13<00:11, 2550.92it/s]Loading:  53%|█████▎    | 33735/64000 [00:13<00:11, 2556.10it/s]Loading:  53%|█████▎    | 33991/64000 [00:13<00:11, 2555.23it/s]Loading:  54%|█████▎    | 34248/64000 [00:13<00:11, 2557.72it/s]Loading:  54%|█████▍    | 34504/64000 [00:14<00:11, 2554.05it/s]Loading:  54%|█████▍    | 34762/64000 [00:14<00:11, 2559.27it/s]Loading:  55%|█████▍    | 35018/64000 [00:14<00:18, 1561.70it/s]Loading:  55%|█████▌    | 35273/64000 [00:14<00:16, 1764.85it/s]Loading:  56%|█████▌    | 35529/64000 [00:14<00:14, 1945.06it/s]Loading:  56%|█████▌    | 35786/64000 [00:14<00:13, 2097.54it/s]Loading:  56%|█████▋    | 36040/64000 [00:14<00:12, 2210.32it/s]Loading:  57%|█████▋    | 36295/64000 [00:14<00:12, 2300.08it/s]Loading:  57%|█████▋    | 36549/64000 [00:15<00:11, 2366.72it/s]Loading:  58%|█████▊    | 36807/64000 [00:15<00:11, 2426.30it/s]Loading:  58%|█████▊    | 37064/64000 [00:15<00:10, 2466.55it/s]Loading:  58%|█████▊    | 37318/64000 [00:15<00:10, 2487.37it/s]Loading:  59%|█████▊    | 37575/64000 [00:15<00:10, 2509.05it/s]Loading:  59%|█████▉    | 37829/64000 [00:15<00:10, 2517.50it/s]Loading:  60%|█████▉    | 38083/64000 [00:15<00:10, 2523.56it/s]Loading:  60%|█████▉    | 38338/64000 [00:15<00:10, 2529.24it/s]Loading:  60%|██████    | 38594/64000 [00:15<00:10, 2536.65it/s]Loading:  61%|██████    | 38849/64000 [00:15<00:09, 2539.97it/s]Loading:  61%|██████    | 39107/64000 [00:16<00:09, 2551.49it/s]Loading:  62%|██████▏   | 39363/64000 [00:16<00:09, 2551.22it/s]Loading:  62%|██████▏   | 39619/64000 [00:16<00:09, 2548.33it/s]Loading:  62%|██████▏   | 39875/64000 [00:16<00:09, 2546.63it/s]Loading:  63%|██████▎   | 40131/64000 [00:16<00:09, 2550.02it/s]Loading:  63%|██████▎   | 40387/64000 [00:16<00:09, 2546.20it/s]Loading:  64%|██████▎   | 40642/64000 [00:16<00:09, 2544.43it/s]Loading:  64%|██████▍   | 40897/64000 [00:16<00:09, 2542.51it/s]Loading:  64%|██████▍   | 41152/64000 [00:16<00:08, 2540.90it/s]Loading:  65%|██████▍   | 41407/64000 [00:16<00:08, 2541.70it/s]Loading:  65%|██████▌   | 41662/64000 [00:17<00:08, 2542.10it/s]Loading:  65%|██████▌   | 41918/64000 [00:17<00:08, 2545.55it/s]Loading:  66%|██████▌   | 42173/64000 [00:17<00:08, 2544.83it/s]Loading:  66%|██████▋   | 42428/64000 [00:17<00:08, 2544.67it/s]Loading:  67%|██████▋   | 42683/64000 [00:17<00:08, 2542.29it/s]Loading:  67%|██████▋   | 42938/64000 [00:17<00:08, 2538.71it/s]Loading:  67%|██████▋   | 43194/64000 [00:17<00:08, 2543.63it/s]Loading:  68%|██████▊   | 43451/64000 [00:17<00:08, 2550.87it/s]Loading:  68%|██████▊   | 43707/64000 [00:17<00:07, 2549.08it/s]Loading:  69%|██████▊   | 43962/64000 [00:17<00:07, 2545.35it/s]Loading:  69%|██████▉   | 44218/64000 [00:18<00:07, 2548.40it/s]Loading:  69%|██████▉   | 44474/64000 [00:18<00:07, 2550.88it/s]Loading:  70%|██████▉   | 44731/64000 [00:18<00:07, 2554.41it/s]Loading:  70%|███████   | 44987/64000 [00:18<00:07, 2549.10it/s]Loading:  71%|███████   | 45243/64000 [00:18<00:07, 2551.33it/s]Loading:  71%|███████   | 45499/64000 [00:18<00:07, 2548.76it/s]Loading:  71%|███████▏  | 45755/64000 [00:18<00:07, 2552.07it/s]Loading:  72%|███████▏  | 46011/64000 [00:18<00:07, 2552.22it/s]Loading:  72%|███████▏  | 46267/64000 [00:18<00:06, 2551.59it/s]Loading:  73%|███████▎  | 46523/64000 [00:18<00:06, 2552.39it/s]Loading:  73%|███████▎  | 46781/64000 [00:19<00:06, 2558.53it/s]Loading:  73%|███████▎  | 47037/64000 [00:19<00:06, 2552.78it/s]Loading:  74%|███████▍  | 47293/64000 [00:19<00:06, 2554.34it/s]Loading:  74%|███████▍  | 47549/64000 [00:19<00:06, 2554.77it/s]Loading:  75%|███████▍  | 47805/64000 [00:19<00:06, 2555.76it/s]Loading:  75%|███████▌  | 48061/64000 [00:19<00:06, 2552.69it/s]Loading:  75%|███████▌  | 48317/64000 [00:19<00:06, 2550.98it/s]Loading:  76%|███████▌  | 48573/64000 [00:19<00:06, 2552.66it/s]Loading:  76%|███████▋  | 48829/64000 [00:19<00:05, 2547.70it/s]Loading:  77%|███████▋  | 49086/64000 [00:19<00:05, 2553.01it/s]Loading:  77%|███████▋  | 49342/64000 [00:20<00:05, 2538.11it/s]Loading:  77%|███████▋  | 49598/64000 [00:20<00:05, 2544.09it/s]Loading:  78%|███████▊  | 49853/64000 [00:20<00:05, 2542.35it/s]Loading:  78%|███████▊  | 50109/64000 [00:20<00:05, 2545.24it/s]Loading:  79%|███████▊  | 50364/64000 [00:20<00:05, 2544.48it/s]Loading:  79%|███████▉  | 50619/64000 [00:20<00:05, 2537.72it/s]Loading:  79%|███████▉  | 50873/64000 [00:20<00:05, 2537.29it/s]Loading:  80%|███████▉  | 51127/64000 [00:21<00:09, 1430.13it/s]Loading:  80%|████████  | 51381/64000 [00:21<00:07, 1644.63it/s]Loading:  81%|████████  | 51634/64000 [00:21<00:06, 1835.19it/s]Loading:  81%|████████  | 51888/64000 [00:21<00:06, 2000.93it/s]Loading:  81%|████████▏ | 52142/64000 [00:21<00:05, 2135.69it/s]Loading:  82%|████████▏ | 52396/64000 [00:21<00:05, 2242.04it/s]Loading:  82%|████████▏ | 52648/64000 [00:21<00:04, 2318.19it/s]Loading:  83%|████████▎ | 52904/64000 [00:21<00:04, 2384.14it/s]Loading:  83%|████████▎ | 53157/64000 [00:21<00:04, 2424.43it/s]Loading:  83%|████████▎ | 53411/64000 [00:21<00:04, 2455.87it/s]Loading:  84%|████████▍ | 53665/64000 [00:22<00:04, 2479.45it/s]Loading:  84%|████████▍ | 53919/64000 [00:22<00:04, 2496.32it/s]Loading:  85%|████████▍ | 54175/64000 [00:22<00:03, 2513.13it/s]Loading:  85%|████████▌ | 54430/64000 [00:22<00:03, 2522.66it/s]Loading:  85%|████████▌ | 54686/64000 [00:22<00:03, 2530.96it/s]Loading:  86%|████████▌ | 54942/64000 [00:22<00:03, 2538.01it/s]Loading:  86%|████████▋ | 55200/64000 [00:22<00:03, 2548.29it/s]Loading:  87%|████████▋ | 55456/64000 [00:22<00:03, 2537.04it/s]Loading:  87%|████████▋ | 55711/64000 [00:22<00:03, 2527.51it/s]Loading:  87%|████████▋ | 55964/64000 [00:22<00:03, 2523.36it/s]Loading:  88%|████████▊ | 56218/64000 [00:23<00:03, 2528.21it/s]Loading:  88%|████████▊ | 56473/64000 [00:23<00:02, 2533.64it/s]Loading:  89%|████████▊ | 56729/64000 [00:23<00:02, 2539.58it/s]Loading:  89%|████████▉ | 56984/64000 [00:23<00:02, 2542.08it/s]Loading:  89%|████████▉ | 57239/64000 [00:23<00:02, 2533.55it/s]Loading:  90%|████████▉ | 57493/64000 [00:23<00:02, 2531.98it/s]Loading:  90%|█████████ | 57747/64000 [00:23<00:02, 2533.58it/s]Loading:  91%|█████████ | 58001/64000 [00:23<00:02, 2534.76it/s]Loading:  91%|█████████ | 58255/64000 [00:23<00:02, 2533.45it/s]Loading:  91%|█████████▏| 58509/64000 [00:23<00:02, 2531.61it/s]Loading:  92%|█████████▏| 58763/64000 [00:24<00:02, 2528.38it/s]Loading:  92%|█████████▏| 59018/64000 [00:24<00:01, 2532.12it/s]Loading:  93%|█████████▎| 59272/64000 [00:24<00:01, 2531.05it/s]Loading:  93%|█████████▎| 59528/64000 [00:24<00:01, 2539.23it/s]Loading:  93%|█████████▎| 59782/64000 [00:24<00:01, 2538.99it/s]Loading:  94%|█████████▍| 60036/64000 [00:24<00:01, 2538.63it/s]Loading:  94%|█████████▍| 60291/64000 [00:24<00:01, 2539.44it/s]Loading:  95%|█████████▍| 60545/64000 [00:24<00:01, 2528.84it/s]Loading:  95%|█████████▍| 60798/64000 [00:24<00:01, 2526.26it/s]Loading:  95%|█████████▌| 61052/64000 [00:24<00:01, 2529.60it/s]Loading:  96%|█████████▌| 61306/64000 [00:25<00:01, 2531.91it/s]Loading:  96%|█████████▌| 61562/64000 [00:25<00:00, 2537.99it/s]Loading:  97%|█████████▋| 61817/64000 [00:25<00:00, 2538.92it/s]Loading:  97%|█████████▋| 62071/64000 [00:25<00:00, 2533.26it/s]Loading:  97%|█████████▋| 62326/64000 [00:25<00:00, 2536.69it/s]Loading:  98%|█████████▊| 62580/64000 [00:25<00:00, 2533.55it/s]Loading:  98%|█████████▊| 62834/64000 [00:25<00:00, 2532.87it/s]Loading:  99%|█████████▊| 63088/64000 [00:25<00:00, 2524.82it/s]Loading:  99%|█████████▉| 63341/64000 [00:25<00:00, 2524.11it/s]Loading:  99%|█████████▉| 63595/64000 [00:25<00:00, 2526.56it/s]Loading: 100%|█████████▉| 63848/64000 [00:26<00:00, 2526.02it/s]Loading: 100%|██████████| 64000/64000 [00:26<00:00, 2450.58it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 256/49018 [00:00<00:19, 2555.48it/s]Loading:   1%|          | 519/49018 [00:00<00:18, 2594.86it/s]Loading:   2%|▏         | 783/49018 [00:00<00:18, 2614.36it/s]Loading:   2%|▏         | 1045/49018 [00:00<00:18, 2616.49it/s]Loading:   3%|▎         | 1310/49018 [00:00<00:18, 2625.42it/s]Loading:   3%|▎         | 1573/49018 [00:00<00:18, 2626.90it/s]Loading:   4%|▎         | 1838/49018 [00:00<00:17, 2633.08it/s]Loading:   4%|▍         | 2102/49018 [00:00<00:17, 2630.49it/s]Loading:   5%|▍         | 2368/49018 [00:00<00:17, 2638.32it/s]Loading:   5%|▌         | 2632/49018 [00:01<00:17, 2637.27it/s]Loading:   6%|▌         | 2896/49018 [00:01<00:17, 2634.71it/s]Loading:   6%|▋         | 3160/49018 [00:01<00:17, 2627.02it/s]Loading:   7%|▋         | 3423/49018 [00:01<00:17, 2602.23it/s]Loading:   8%|▊         | 3687/49018 [00:01<00:17, 2613.41it/s]Loading:   8%|▊         | 3952/49018 [00:01<00:17, 2622.54it/s]Loading:   9%|▊         | 4215/49018 [00:01<00:17, 2622.11it/s]Loading:   9%|▉         | 4479/49018 [00:01<00:16, 2626.70it/s]Loading:  10%|▉         | 4742/49018 [00:01<00:16, 2625.31it/s]Loading:  10%|█         | 5008/49018 [00:01<00:16, 2633.39it/s]Loading:  11%|█         | 5272/49018 [00:02<00:16, 2631.17it/s]Loading:  11%|█▏        | 5536/49018 [00:02<00:16, 2629.19it/s]Loading:  12%|█▏        | 5800/49018 [00:02<00:16, 2630.07it/s]Loading:  12%|█▏        | 6065/49018 [00:02<00:16, 2632.81it/s]Loading:  13%|█▎        | 6331/49018 [00:02<00:16, 2639.61it/s]Loading:  13%|█▎        | 6596/49018 [00:02<00:16, 2641.27it/s]Loading:  14%|█▍        | 6861/49018 [00:02<00:15, 2641.52it/s]Loading:  15%|█▍        | 7126/49018 [00:02<00:15, 2640.02it/s]Loading:  15%|█▌        | 7391/49018 [00:02<00:15, 2641.36it/s]Loading:  16%|█▌        | 7656/49018 [00:02<00:15, 2639.79it/s]Loading:  16%|█▌        | 7920/49018 [00:03<00:15, 2636.01it/s]Loading:  17%|█▋        | 8184/49018 [00:03<00:15, 2622.01it/s]Loading:  17%|█▋        | 8447/49018 [00:03<00:15, 2619.73it/s]Loading:  18%|█▊        | 8710/49018 [00:03<00:15, 2620.00it/s]Loading:  18%|█▊        | 8974/49018 [00:03<00:15, 2625.75it/s]Loading:  19%|█▉        | 9237/49018 [00:03<00:15, 2623.19it/s]Loading:  19%|█▉        | 9501/49018 [00:03<00:15, 2626.66it/s]Loading:  20%|█▉        | 9764/49018 [00:03<00:14, 2623.89it/s]Loading:  20%|██        | 10027/49018 [00:03<00:14, 2622.62it/s]Loading:  21%|██        | 10291/49018 [00:03<00:14, 2625.28it/s]Loading:  22%|██▏       | 10555/49018 [00:04<00:14, 2629.10it/s]Loading:  22%|██▏       | 10818/49018 [00:04<00:14, 2623.80it/s]Loading:  23%|██▎       | 11081/49018 [00:04<00:14, 2625.44it/s]Loading:  23%|██▎       | 11344/49018 [00:04<00:14, 2619.31it/s]Loading:  24%|██▎       | 11608/49018 [00:04<00:14, 2622.71it/s]Loading:  24%|██▍       | 11871/49018 [00:04<00:14, 2618.22it/s]Loading:  25%|██▍       | 12133/49018 [00:04<00:14, 2615.71it/s]Loading:  25%|██▌       | 12396/49018 [00:04<00:13, 2618.74it/s]Loading:  26%|██▌       | 12658/49018 [00:04<00:13, 2614.57it/s]Loading:  26%|██▋       | 12921/49018 [00:04<00:13, 2617.71it/s]Loading:  27%|██▋       | 13183/49018 [00:05<00:13, 2614.69it/s]Loading:  27%|██▋       | 13445/49018 [00:05<00:13, 2594.01it/s]Loading:  28%|██▊       | 13705/49018 [00:05<00:13, 2583.14it/s]Loading:  28%|██▊       | 13964/49018 [00:05<00:13, 2572.03it/s]Loading:  29%|██▉       | 14222/49018 [00:05<00:13, 2565.45it/s]Loading:  30%|██▉       | 14480/49018 [00:05<00:13, 2566.63it/s]Loading:  30%|███       | 14737/49018 [00:05<00:13, 2563.91it/s]Loading:  31%|███       | 14994/49018 [00:05<00:13, 2562.65it/s]Loading:  31%|███       | 15251/49018 [00:05<00:13, 2557.86it/s]Loading:  32%|███▏      | 15507/49018 [00:05<00:13, 2556.03it/s]Loading:  32%|███▏      | 15763/49018 [00:06<00:13, 2553.89it/s]Loading:  33%|███▎      | 16019/49018 [00:06<00:12, 2554.56it/s]Loading:  33%|███▎      | 16276/49018 [00:06<00:12, 2558.40it/s]Loading:  34%|███▎      | 16532/49018 [00:06<00:12, 2555.31it/s]Loading:  34%|███▍      | 16788/49018 [00:06<00:12, 2552.67it/s]Loading:  35%|███▍      | 17044/49018 [00:06<00:12, 2554.04it/s]Loading:  35%|███▌      | 17301/49018 [00:06<00:12, 2556.78it/s]Loading:  36%|███▌      | 17557/49018 [00:06<00:12, 2552.63it/s]Loading:  36%|███▋      | 17813/49018 [00:06<00:12, 2551.76it/s]Loading:  37%|███▋      | 18069/49018 [00:07<00:24, 1250.25it/s]Loading:  37%|███▋      | 18324/49018 [00:07<00:20, 1474.17it/s]Loading:  38%|███▊      | 18579/49018 [00:07<00:18, 1686.62it/s]Loading:  38%|███▊      | 18835/49018 [00:07<00:16, 1878.95it/s]Loading:  39%|███▉      | 19090/49018 [00:07<00:14, 2038.15it/s]Loading:  39%|███▉      | 19347/49018 [00:07<00:13, 2172.88it/s]Loading:  40%|███▉      | 19601/49018 [00:07<00:12, 2270.52it/s]Loading:  41%|████      | 19856/49018 [00:07<00:12, 2347.00it/s]Loading:  41%|████      | 20111/49018 [00:08<00:12, 2403.74it/s]Loading:  42%|████▏     | 20365/49018 [00:08<00:11, 2442.46it/s]Loading:  42%|████▏     | 20622/49018 [00:08<00:11, 2478.03it/s]Loading:  43%|████▎     | 20877/49018 [00:08<00:11, 2495.82it/s]Loading:  43%|████▎     | 21131/49018 [00:08<00:11, 2487.79it/s]Loading:  44%|████▎     | 21387/49018 [00:08<00:11, 2506.31it/s]Loading:  44%|████▍     | 21645/49018 [00:08<00:10, 2525.98it/s]Loading:  45%|████▍     | 21900/49018 [00:08<00:10, 2533.02it/s]Loading:  45%|████▌     | 22157/49018 [00:08<00:10, 2541.31it/s]Loading:  46%|████▌     | 22412/49018 [00:08<00:10, 2541.91it/s]Loading:  46%|████▌     | 22668/49018 [00:09<00:10, 2545.86it/s]Loading:  47%|████▋     | 22923/49018 [00:09<00:10, 2544.48it/s]Loading:  47%|████▋     | 23178/49018 [00:09<00:10, 2544.89it/s]Loading:  48%|████▊     | 23433/49018 [00:09<00:10, 2541.73it/s]Loading:  48%|████▊     | 23688/49018 [00:09<00:09, 2541.95it/s]Loading:  49%|████▉     | 23945/49018 [00:09<00:09, 2548.54it/s]Loading:  49%|████▉     | 24201/49018 [00:09<00:09, 2550.82it/s]Loading:  50%|████▉     | 24460/49018 [00:09<00:09, 2560.25it/s]Loading:  50%|█████     | 24717/49018 [00:09<00:09, 2557.70it/s]Loading:  51%|█████     | 24974/49018 [00:09<00:09, 2560.26it/s]Loading:  51%|█████▏    | 25231/49018 [00:10<00:09, 2560.90it/s]Loading:  52%|█████▏    | 25488/49018 [00:10<00:09, 2559.41it/s]Loading:  53%|█████▎    | 25744/49018 [00:10<00:09, 2556.30it/s]Loading:  53%|█████▎    | 26001/49018 [00:10<00:08, 2558.52it/s]Loading:  54%|█████▎    | 26257/49018 [00:10<00:08, 2556.48it/s]Loading:  54%|█████▍    | 26513/49018 [00:10<00:08, 2550.66it/s]Loading:  55%|█████▍    | 26769/49018 [00:10<00:08, 2545.86it/s]Loading:  55%|█████▌    | 27024/49018 [00:10<00:08, 2514.59it/s]Loading:  56%|█████▌    | 27280/49018 [00:10<00:08, 2525.74it/s]Loading:  56%|█████▌    | 27535/49018 [00:11<00:08, 2532.31it/s]Loading:  57%|█████▋    | 27790/49018 [00:11<00:08, 2535.53it/s]Loading:  57%|█████▋    | 28046/49018 [00:11<00:08, 2539.78it/s]Loading:  58%|█████▊    | 28302/49018 [00:11<00:08, 2543.22it/s]Loading:  58%|█████▊    | 28558/49018 [00:11<00:08, 2546.99it/s]Loading:  59%|█████▉    | 28815/49018 [00:11<00:07, 2553.04it/s]Loading:  59%|█████▉    | 29071/49018 [00:11<00:07, 2555.01it/s]Loading:  60%|█████▉    | 29328/49018 [00:11<00:07, 2558.09it/s]Loading:  60%|██████    | 29584/49018 [00:11<00:07, 2550.47it/s]Loading:  61%|██████    | 29840/49018 [00:11<00:07, 2551.99it/s]Loading:  61%|██████▏   | 30096/49018 [00:12<00:07, 2554.27it/s]Loading:  62%|██████▏   | 30352/49018 [00:12<00:07, 2547.86it/s]Loading:  62%|██████▏   | 30609/49018 [00:12<00:07, 2551.96it/s]Loading:  63%|██████▎   | 30866/49018 [00:12<00:07, 2554.95it/s]Loading:  63%|██████▎   | 31124/49018 [00:12<00:06, 2560.44it/s]Loading:  64%|██████▍   | 31381/49018 [00:12<00:06, 2554.04it/s]Loading:  65%|██████▍   | 31638/49018 [00:12<00:06, 2557.95it/s]Loading:  65%|██████▌   | 31894/49018 [00:12<00:06, 2557.04it/s]Loading:  66%|██████▌   | 32150/49018 [00:12<00:06, 2556.62it/s]Loading:  66%|██████▌   | 32407/49018 [00:12<00:06, 2557.93it/s]Loading:  67%|██████▋   | 32663/49018 [00:13<00:06, 2557.94it/s]Loading:  67%|██████▋   | 32919/49018 [00:13<00:06, 2553.07it/s]Loading:  68%|██████▊   | 33175/49018 [00:13<00:06, 2552.13it/s]Loading:  68%|██████▊   | 33431/49018 [00:13<00:06, 2553.28it/s]Loading:  69%|██████▊   | 33687/49018 [00:13<00:06, 2554.11it/s]Loading:  69%|██████▉   | 33944/49018 [00:13<00:05, 2556.08it/s]Loading:  70%|██████▉   | 34201/49018 [00:13<00:05, 2557.41it/s]Loading:  70%|███████   | 34457/49018 [00:13<00:05, 2556.53it/s]Loading:  71%|███████   | 34713/49018 [00:13<00:05, 2556.37it/s]Loading:  71%|███████▏  | 34970/49018 [00:13<00:05, 2558.83it/s]Loading:  72%|███████▏  | 35226/49018 [00:14<00:05, 2557.41it/s]Loading:  72%|███████▏  | 35482/49018 [00:14<00:05, 2555.94it/s]Loading:  73%|███████▎  | 35738/49018 [00:14<00:05, 2556.68it/s]Loading:  73%|███████▎  | 35996/49018 [00:14<00:05, 2561.61it/s]Loading:  74%|███████▍  | 36253/49018 [00:14<00:04, 2558.58it/s]Loading:  74%|███████▍  | 36509/49018 [00:14<00:04, 2555.18it/s]Loading:  75%|███████▌  | 36765/49018 [00:14<00:04, 2552.74it/s]Loading:  76%|███████▌  | 37022/49018 [00:14<00:04, 2555.23it/s]Loading:  76%|███████▌  | 37278/49018 [00:14<00:04, 2555.68it/s]Loading:  77%|███████▋  | 37535/49018 [00:14<00:04, 2557.15it/s]Loading:  77%|███████▋  | 37793/49018 [00:15<00:04, 2561.50it/s]Loading:  78%|███████▊  | 38050/49018 [00:15<00:04, 2556.55it/s]Loading:  78%|███████▊  | 38306/49018 [00:15<00:04, 2557.39it/s]Loading:  79%|███████▊  | 38562/49018 [00:15<00:04, 2553.15it/s]Loading:  79%|███████▉  | 38819/49018 [00:15<00:03, 2557.24it/s]Loading:  80%|███████▉  | 39075/49018 [00:15<00:03, 2548.02it/s]Loading:  80%|████████  | 39331/49018 [00:15<00:03, 2549.12it/s]Loading:  81%|████████  | 39587/49018 [00:15<00:03, 2552.11it/s]Loading:  81%|████████▏ | 39843/49018 [00:15<00:03, 2552.08it/s]Loading:  82%|████████▏ | 40099/49018 [00:15<00:03, 2552.15it/s]Loading:  82%|████████▏ | 40355/49018 [00:16<00:03, 2553.72it/s]Loading:  83%|████████▎ | 40611/49018 [00:16<00:03, 2548.69it/s]Loading:  83%|████████▎ | 40868/49018 [00:16<00:03, 2554.45it/s]Loading:  84%|████████▍ | 41125/49018 [00:16<00:03, 2556.54it/s]Loading:  84%|████████▍ | 41381/49018 [00:16<00:02, 2556.67it/s]Loading:  85%|████████▍ | 41638/49018 [00:16<00:02, 2558.96it/s]Loading:  85%|████████▌ | 41895/49018 [00:16<00:02, 2560.23it/s]Loading:  86%|████████▌ | 42153/49018 [00:16<00:02, 2565.12it/s]Loading:  87%|████████▋ | 42410/49018 [00:16<00:02, 2559.04it/s]Loading:  87%|████████▋ | 42667/49018 [00:16<00:02, 2559.73it/s]Loading:  88%|████████▊ | 42923/49018 [00:17<00:02, 2556.56it/s]Loading:  88%|████████▊ | 43179/49018 [00:17<00:02, 2552.52it/s]Loading:  89%|████████▊ | 43435/49018 [00:17<00:02, 2553.39it/s]Loading:  89%|████████▉ | 43691/49018 [00:17<00:02, 2553.89it/s]Loading:  90%|████████▉ | 43948/49018 [00:17<00:01, 2556.66it/s]Loading:  90%|█████████ | 44205/49018 [00:17<00:01, 2560.45it/s]Loading:  91%|█████████ | 44462/49018 [00:17<00:01, 2560.58it/s]Loading:  91%|█████████ | 44719/49018 [00:17<00:01, 2560.51it/s]Loading:  92%|█████████▏| 44976/49018 [00:17<00:01, 2560.46it/s]Loading:  92%|█████████▏| 45233/49018 [00:17<00:01, 2558.24it/s]Loading:  93%|█████████▎| 45491/49018 [00:18<00:01, 2563.28it/s]Loading:  93%|█████████▎| 45748/49018 [00:18<00:02, 1129.45it/s]Loading:  94%|█████████▍| 46003/49018 [00:18<00:02, 1354.17it/s]Loading:  94%|█████████▍| 46260/49018 [00:18<00:01, 1577.45it/s]Loading:  95%|█████████▍| 46517/49018 [00:18<00:01, 1783.03it/s]Loading:  95%|█████████▌| 46773/49018 [00:18<00:01, 1959.72it/s]Loading:  96%|█████████▌| 47030/49018 [00:19<00:00, 2109.28it/s]Loading:  96%|█████████▋| 47285/49018 [00:19<00:00, 2222.86it/s]Loading:  97%|█████████▋| 47540/49018 [00:19<00:00, 2311.45it/s]Loading:  98%|█████████▊| 47798/49018 [00:19<00:00, 2384.81it/s]Loading:  98%|█████████▊| 48054/49018 [00:19<00:00, 2433.14it/s]Loading:  99%|█████████▊| 48312/49018 [00:19<00:00, 2474.66it/s]Loading:  99%|█████████▉| 48567/49018 [00:19<00:00, 2481.23it/s]Loading: 100%|█████████▉| 48824/49018 [00:19<00:00, 2507.20it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2471.72it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 261/49018 [00:00<00:18, 2607.71it/s]Loading:   1%|          | 528/49018 [00:00<00:18, 2640.63it/s]Loading:   2%|▏         | 793/49018 [00:00<00:18, 2641.47it/s]Loading:   2%|▏         | 1059/49018 [00:00<00:18, 2645.44it/s]Loading:   3%|▎         | 1324/49018 [00:00<00:18, 2641.86it/s]Loading:   3%|▎         | 1591/49018 [00:00<00:17, 2648.31it/s]Loading:   4%|▍         | 1856/49018 [00:01<00:43, 1085.08it/s]Loading:   4%|▍         | 2122/49018 [00:01<00:35, 1336.71it/s]Loading:   5%|▍         | 2386/49018 [00:01<00:29, 1578.96it/s]Loading:   5%|▌         | 2650/49018 [00:01<00:25, 1800.85it/s]Loading:   6%|▌         | 2917/49018 [00:01<00:23, 1999.74it/s]Loading:   6%|▋         | 3179/49018 [00:01<00:21, 2152.99it/s]Loading:   7%|▋         | 3445/49018 [00:01<00:19, 2283.97it/s]Loading:   8%|▊         | 3709/49018 [00:01<00:19, 2379.09it/s]Loading:   8%|▊         | 3975/49018 [00:01<00:18, 2456.71it/s]Loading:   9%|▊         | 4239/49018 [00:02<00:17, 2507.45it/s]Loading:   9%|▉         | 4501/49018 [00:02<00:17, 2525.15it/s]Loading:  10%|▉         | 4763/49018 [00:02<00:17, 2550.87it/s]Loading:  10%|█         | 5028/49018 [00:02<00:17, 2579.56it/s]Loading:  11%|█         | 5292/49018 [00:02<00:16, 2596.14it/s]Loading:  11%|█▏        | 5555/49018 [00:02<00:16, 2605.96it/s]Loading:  12%|█▏        | 5818/49018 [00:02<00:16, 2608.89it/s]Loading:  12%|█▏        | 6081/49018 [00:02<00:16, 2610.55it/s]Loading:  13%|█▎        | 6345/49018 [00:02<00:16, 2616.24it/s]Loading:  13%|█▎        | 6608/49018 [00:02<00:16, 2619.72it/s]Loading:  14%|█▍        | 6871/49018 [00:03<00:16, 2618.46it/s]Loading:  15%|█▍        | 7134/49018 [00:03<00:16, 2614.85it/s]Loading:  15%|█▌        | 7396/49018 [00:03<00:15, 2611.34it/s]Loading:  16%|█▌        | 7659/49018 [00:03<00:15, 2614.10it/s]Loading:  16%|█▌        | 7921/49018 [00:03<00:15, 2609.02it/s]Loading:  17%|█▋        | 8183/49018 [00:03<00:15, 2611.72it/s]Loading:  17%|█▋        | 8445/49018 [00:03<00:15, 2611.89it/s]Loading:  18%|█▊        | 8707/49018 [00:03<00:15, 2609.04it/s]Loading:  18%|█▊        | 8971/49018 [00:03<00:15, 2617.35it/s]Loading:  19%|█▉        | 9233/49018 [00:03<00:15, 2616.87it/s]Loading:  19%|█▉        | 9497/49018 [00:04<00:15, 2622.58it/s]Loading:  20%|█▉        | 9760/49018 [00:04<00:14, 2618.17it/s]Loading:  20%|██        | 10023/49018 [00:04<00:14, 2619.76it/s]Loading:  21%|██        | 10285/49018 [00:04<00:14, 2619.26it/s]Loading:  22%|██▏       | 10547/49018 [00:04<00:14, 2618.30it/s]Loading:  22%|██▏       | 10809/49018 [00:04<00:14, 2616.42it/s]Loading:  23%|██▎       | 11072/49018 [00:04<00:14, 2620.41it/s]Loading:  23%|██▎       | 11335/49018 [00:04<00:14, 2616.03it/s]Loading:  24%|██▎       | 11599/49018 [00:04<00:14, 2620.41it/s]Loading:  24%|██▍       | 11862/49018 [00:04<00:14, 2621.06it/s]Loading:  25%|██▍       | 12127/49018 [00:05<00:14, 2627.18it/s]Loading:  25%|██▌       | 12390/49018 [00:05<00:13, 2623.09it/s]Loading:  26%|██▌       | 12653/49018 [00:05<00:13, 2622.60it/s]Loading:  26%|██▋       | 12916/49018 [00:05<00:13, 2620.62it/s]Loading:  27%|██▋       | 13179/49018 [00:05<00:13, 2621.02it/s]Loading:  27%|██▋       | 13442/49018 [00:05<00:13, 2618.61it/s]Loading:  28%|██▊       | 13705/49018 [00:05<00:13, 2621.01it/s]Loading:  28%|██▊       | 13968/49018 [00:05<00:13, 2619.09it/s]Loading:  29%|██▉       | 14232/49018 [00:05<00:13, 2623.03it/s]Loading:  30%|██▉       | 14495/49018 [00:05<00:13, 2622.83it/s]Loading:  30%|███       | 14758/49018 [00:06<00:13, 2623.44it/s]Loading:  31%|███       | 15021/49018 [00:06<00:12, 2623.03it/s]Loading:  31%|███       | 15284/49018 [00:06<00:12, 2619.34it/s]Loading:  32%|███▏      | 15547/49018 [00:06<00:12, 2621.57it/s]Loading:  32%|███▏      | 15810/49018 [00:06<00:12, 2614.27it/s]Loading:  33%|███▎      | 16072/49018 [00:06<00:12, 2603.88it/s]Loading:  33%|███▎      | 16335/49018 [00:06<00:12, 2610.06it/s]Loading:  34%|███▍      | 16597/49018 [00:06<00:12, 2612.03it/s]Loading:  34%|███▍      | 16859/49018 [00:06<00:12, 2611.77it/s]Loading:  35%|███▍      | 17121/49018 [00:06<00:12, 2613.87it/s]Loading:  35%|███▌      | 17383/49018 [00:07<00:12, 2610.29it/s]Loading:  36%|███▌      | 17646/49018 [00:07<00:11, 2614.83it/s]Loading:  37%|███▋      | 17908/49018 [00:07<00:11, 2612.00it/s]Loading:  37%|███▋      | 18170/49018 [00:07<00:11, 2597.85it/s]Loading:  38%|███▊      | 18430/49018 [00:07<00:11, 2578.03it/s]Loading:  38%|███▊      | 18688/49018 [00:07<00:11, 2564.22it/s]Loading:  39%|███▊      | 18945/49018 [00:07<00:11, 2561.70it/s]Loading:  39%|███▉      | 19202/49018 [00:07<00:11, 2551.10it/s]Loading:  40%|███▉      | 19458/49018 [00:07<00:11, 2551.78it/s]Loading:  40%|████      | 19714/49018 [00:07<00:11, 2548.22it/s]Loading:  41%|████      | 19969/49018 [00:08<00:11, 2546.57it/s]Loading:  41%|████▏     | 20224/49018 [00:08<00:11, 2543.96it/s]Loading:  42%|████▏     | 20479/49018 [00:08<00:11, 2541.33it/s]Loading:  42%|████▏     | 20734/49018 [00:08<00:11, 2536.43it/s]Loading:  43%|████▎     | 20989/49018 [00:08<00:11, 2538.16it/s]Loading:  43%|████▎     | 21245/49018 [00:08<00:10, 2542.31it/s]Loading:  44%|████▍     | 21500/49018 [00:08<00:10, 2544.39it/s]Loading:  44%|████▍     | 21755/49018 [00:08<00:10, 2545.40it/s]Loading:  45%|████▍     | 22010/49018 [00:08<00:10, 2544.72it/s]Loading:  45%|████▌     | 22265/49018 [00:08<00:10, 2543.12it/s]Loading:  46%|████▌     | 22520/49018 [00:09<00:10, 2544.76it/s]Loading:  46%|████▋     | 22775/49018 [00:09<00:10, 2544.84it/s]Loading:  47%|████▋     | 23030/49018 [00:09<00:10, 2540.71it/s]Loading:  48%|████▊     | 23285/49018 [00:09<00:10, 2505.40it/s]Loading:  48%|████▊     | 23539/49018 [00:09<00:10, 2514.79it/s]Loading:  49%|████▊     | 23794/49018 [00:09<00:09, 2523.50it/s]Loading:  49%|████▉     | 24048/49018 [00:09<00:09, 2528.11it/s]Loading:  50%|████▉     | 24302/49018 [00:09<00:09, 2531.07it/s]Loading:  50%|█████     | 24556/49018 [00:09<00:09, 2533.28it/s]Loading:  51%|█████     | 24810/49018 [00:09<00:09, 2532.67it/s]Loading:  51%|█████     | 25065/49018 [00:10<00:09, 2536.46it/s]Loading:  52%|█████▏    | 25321/49018 [00:10<00:09, 2542.10it/s]Loading:  52%|█████▏    | 25576/49018 [00:10<00:09, 2540.79it/s]Loading:  53%|█████▎    | 25832/49018 [00:10<00:09, 2546.12it/s]Loading:  53%|█████▎    | 26087/49018 [00:10<00:09, 2546.90it/s]Loading:  54%|█████▎    | 26342/49018 [00:10<00:08, 2546.75it/s]Loading:  54%|█████▍    | 26597/49018 [00:10<00:08, 2545.84it/s]Loading:  55%|█████▍    | 26852/49018 [00:10<00:08, 2544.14it/s]Loading:  55%|█████▌    | 27107/49018 [00:10<00:08, 2542.63it/s]Loading:  56%|█████▌    | 27362/49018 [00:10<00:08, 2541.54it/s]Loading:  56%|█████▋    | 27618/49018 [00:11<00:08, 2544.92it/s]Loading:  57%|█████▋    | 27873/49018 [00:11<00:08, 2545.00it/s]Loading:  57%|█████▋    | 28128/49018 [00:11<00:08, 2545.31it/s]Loading:  58%|█████▊    | 28383/49018 [00:11<00:08, 2540.02it/s]Loading:  58%|█████▊    | 28638/49018 [00:11<00:08, 2541.62it/s]Loading:  59%|█████▉    | 28893/49018 [00:11<00:07, 2535.04it/s]Loading:  59%|█████▉    | 29149/49018 [00:11<00:07, 2541.35it/s]Loading:  60%|█████▉    | 29405/49018 [00:11<00:07, 2544.42it/s]Loading:  61%|██████    | 29660/49018 [00:11<00:07, 2539.55it/s]Loading:  61%|██████    | 29916/49018 [00:11<00:07, 2544.07it/s]Loading:  62%|██████▏   | 30171/49018 [00:12<00:07, 2543.56it/s]Loading:  62%|██████▏   | 30427/49018 [00:12<00:07, 2546.86it/s]Loading:  63%|██████▎   | 30683/49018 [00:12<00:07, 2548.73it/s]Loading:  63%|██████▎   | 30938/49018 [00:12<00:07, 2547.05it/s]Loading:  64%|██████▎   | 31194/49018 [00:12<00:06, 2548.41it/s]Loading:  64%|██████▍   | 31449/49018 [00:12<00:06, 2544.44it/s]Loading:  65%|██████▍   | 31704/49018 [00:12<00:06, 2543.34it/s]Loading:  65%|██████▌   | 31961/49018 [00:12<00:06, 2548.62it/s]Loading:  66%|██████▌   | 32216/49018 [00:12<00:06, 2542.38it/s]Loading:  66%|██████▌   | 32472/49018 [00:12<00:06, 2547.61it/s]Loading:  67%|██████▋   | 32728/49018 [00:13<00:06, 2549.73it/s]Loading:  67%|██████▋   | 32983/49018 [00:13<00:06, 2547.01it/s]Loading:  68%|██████▊   | 33239/49018 [00:13<00:06, 2550.05it/s]Loading:  68%|██████▊   | 33495/49018 [00:13<00:06, 2545.09it/s]Loading:  69%|██████▉   | 33750/49018 [00:13<00:06, 2544.40it/s]Loading:  69%|██████▉   | 34005/49018 [00:13<00:05, 2539.46it/s]Loading:  70%|██████▉   | 34262/49018 [00:13<00:05, 2546.79it/s]Loading:  70%|███████   | 34517/49018 [00:13<00:05, 2546.68it/s]Loading:  71%|███████   | 34773/49018 [00:13<00:05, 2548.41it/s]Loading:  71%|███████▏  | 35028/49018 [00:14<00:05, 2546.79it/s]Loading:  72%|███████▏  | 35284/49018 [00:14<00:05, 2548.40it/s]Loading:  73%|███████▎  | 35539/49018 [00:14<00:05, 2545.36it/s]Loading:  73%|███████▎  | 35795/49018 [00:14<00:05, 2547.79it/s]Loading:  74%|███████▎  | 36050/49018 [00:14<00:05, 2547.64it/s]Loading:  74%|███████▍  | 36305/49018 [00:14<00:04, 2546.37it/s]Loading:  75%|███████▍  | 36561/49018 [00:14<00:04, 2547.63it/s]Loading:  75%|███████▌  | 36817/49018 [00:14<00:04, 2549.70it/s]Loading:  76%|███████▌  | 37072/49018 [00:14<00:04, 2548.83it/s]Loading:  76%|███████▌  | 37327/49018 [00:14<00:04, 2547.40it/s]Loading:  77%|███████▋  | 37583/49018 [00:15<00:04, 2550.14it/s]Loading:  77%|███████▋  | 37839/49018 [00:15<00:11, 966.80it/s] Loading:  78%|███████▊  | 38092/49018 [00:15<00:09, 1184.65it/s]Loading:  78%|███████▊  | 38346/49018 [00:15<00:07, 1408.82it/s]Loading:  79%|███████▊  | 38600/49018 [00:15<00:06, 1625.06it/s]Loading:  79%|███████▉  | 38856/49018 [00:16<00:05, 1824.58it/s]Loading:  80%|███████▉  | 39110/49018 [00:16<00:04, 1992.49it/s]Loading:  80%|████████  | 39365/49018 [00:16<00:04, 2131.83it/s]Loading:  81%|████████  | 39619/49018 [00:16<00:04, 2237.75it/s]Loading:  81%|████████▏ | 39872/49018 [00:16<00:03, 2317.39it/s]Loading:  82%|████████▏ | 40123/49018 [00:16<00:03, 2370.51it/s]Loading:  82%|████████▏ | 40377/49018 [00:16<00:03, 2417.39it/s]Loading:  83%|████████▎ | 40632/49018 [00:16<00:03, 2454.05it/s]Loading:  83%|████████▎ | 40887/49018 [00:16<00:03, 2480.31it/s]Loading:  84%|████████▍ | 41141/49018 [00:16<00:03, 2497.48it/s]Loading:  84%|████████▍ | 41396/49018 [00:17<00:03, 2512.24it/s]Loading:  85%|████████▍ | 41650/49018 [00:17<00:02, 2519.74it/s]Loading:  85%|████████▌ | 41905/49018 [00:17<00:02, 2527.14it/s]Loading:  86%|████████▌ | 42161/49018 [00:17<00:02, 2535.77it/s]Loading:  87%|████████▋ | 42416/49018 [00:17<00:02, 2531.99it/s]Loading:  87%|████████▋ | 42673/49018 [00:17<00:02, 2541.40it/s]Loading:  88%|████████▊ | 42928/49018 [00:17<00:02, 2539.10it/s]Loading:  88%|████████▊ | 43184/49018 [00:17<00:02, 2542.58it/s]Loading:  89%|████████▊ | 43439/49018 [00:17<00:02, 2538.76it/s]Loading:  89%|████████▉ | 43695/49018 [00:17<00:02, 2543.06it/s]Loading:  90%|████████▉ | 43950/49018 [00:18<00:01, 2544.84it/s]Loading:  90%|█████████ | 44206/49018 [00:18<00:01, 2547.14it/s]Loading:  91%|█████████ | 44461/49018 [00:18<00:01, 2546.44it/s]Loading:  91%|█████████ | 44718/49018 [00:18<00:01, 2551.63it/s]Loading:  92%|█████████▏| 44974/49018 [00:18<00:01, 2544.66it/s]Loading:  92%|█████████▏| 45229/49018 [00:18<00:01, 2545.86it/s]Loading:  93%|█████████▎| 45485/49018 [00:18<00:01, 2547.00it/s]Loading:  93%|█████████▎| 45740/49018 [00:18<00:01, 2546.86it/s]Loading:  94%|█████████▍| 45995/49018 [00:18<00:01, 2545.08it/s]Loading:  94%|█████████▍| 46250/49018 [00:18<00:01, 2540.56it/s]Loading:  95%|█████████▍| 46505/49018 [00:19<00:00, 2538.81it/s]Loading:  95%|█████████▌| 46761/49018 [00:19<00:00, 2542.56it/s]Loading:  96%|█████████▌| 47017/49018 [00:19<00:00, 2547.06it/s]Loading:  96%|█████████▋| 47272/49018 [00:19<00:00, 2545.60it/s]Loading:  97%|█████████▋| 47527/49018 [00:19<00:00, 2543.70it/s]Loading:  97%|█████████▋| 47782/49018 [00:19<00:00, 2539.26it/s]Loading:  98%|█████████▊| 48038/49018 [00:19<00:00, 2543.32it/s]Loading:  99%|█████████▊| 48293/49018 [00:19<00:00, 2539.86it/s]Loading:  99%|█████████▉| 48549/49018 [00:19<00:00, 2545.22it/s]Loading: 100%|█████████▉| 48804/49018 [00:19<00:00, 2546.40it/s]Loading: 100%|██████████| 49018/49018 [00:20<00:00, 2444.51it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank32]:[W424 17:45:28.601329530 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 17:45:28.601364927 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 17:45:28.601347644 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 17:45:28.601345230 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 17:45:28.601332195 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 17:45:28.601356341 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 17:45:28.601370708 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 17:45:28.601367652 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 17:45:29.822106191 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 17:45:29.822097745 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 17:45:29.822103596 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 17:45:29.822110119 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 17:45:29.690090684 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 17:45:29.690099240 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 17:45:29.690112154 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 17:45:29.690085865 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 17:45:29.690092377 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 17:45:29.690077208 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 17:45:29.933482156 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 17:45:29.933487426 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 17:45:29.933480573 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 17:45:29.933476255 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 17:45:29.933469362 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 17:45:29.933464974 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 17:45:29.603965296 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 17:45:29.603961749 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 17:45:29.603954506 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 17:45:29.603956920 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 17:45:29.603947152 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 17:45:29.603963222 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 17:45:29.603949185 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 17:45:30.771066524 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 17:45:30.034538683 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 17:45:30.034908115 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 17:45:30.034888328 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 17:45:30.673048276 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 17:45:30.673050881 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 17:45:30.016090861 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 17:45:30.277287235 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 17:45:31.241444982 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:27<22:04, 27.04s/it]Train:   4%|▍         | 2/50 [00:27<09:02, 11.31s/it]Train:   6%|▌         | 3/50 [00:27<04:53,  6.25s/it]Train:   8%|▊         | 4/50 [00:27<02:57,  3.87s/it]Train:  10%|█         | 5/50 [00:28<01:54,  2.55s/it]Train:  12%|█▏        | 6/50 [00:28<01:17,  1.76s/it]Train:  14%|█▍        | 7/50 [00:28<00:54,  1.26s/it]Train:  16%|█▌        | 8/50 [00:28<00:39,  1.08it/s]Train:  18%|█▊        | 9/50 [00:28<00:29,  1.41it/s]Train:  20%|██        | 10/50 [00:29<00:22,  1.79it/s]Train:  22%|██▏       | 11/50 [00:29<00:17,  2.20it/s]Train:  24%|██▍       | 12/50 [00:29<00:14,  2.60it/s]Train:  26%|██▌       | 13/50 [00:29<00:12,  2.98it/s]Train:  28%|██▊       | 14/50 [00:30<00:10,  3.33it/s]Train:  30%|███       | 15/50 [00:30<00:09,  3.63it/s]Train:  32%|███▏      | 16/50 [00:30<00:08,  3.87it/s]Train:  34%|███▍      | 17/50 [00:30<00:08,  4.04it/s]Train:  36%|███▌      | 18/50 [00:30<00:07,  4.18it/s]Train:  38%|███▊      | 19/50 [00:31<00:07,  4.28it/s]Train:  40%|████      | 20/50 [00:31<00:06,  4.37it/s]Train:  42%|████▏     | 21/50 [00:31<00:06,  4.43it/s]Train:  44%|████▍     | 22/50 [00:31<00:06,  4.46it/s]Train:  46%|████▌     | 23/50 [00:31<00:06,  4.50it/s]Train:  48%|████▊     | 24/50 [00:32<00:05,  4.52it/s]Train:  50%|█████     | 25/50 [00:32<00:05,  4.51it/s]Train:  52%|█████▏    | 26/50 [00:32<00:05,  4.51it/s]Train:  54%|█████▍    | 27/50 [00:32<00:05,  4.53it/s]Train:  56%|█████▌    | 28/50 [00:33<00:04,  4.54it/s]Train:  58%|█████▊    | 29/50 [00:33<00:04,  4.53it/s]Train:  60%|██████    | 30/50 [00:33<00:04,  4.55it/s]Train:  62%|██████▏   | 31/50 [00:33<00:04,  4.55it/s]Train:  64%|██████▍   | 32/50 [00:33<00:03,  4.54it/s]Train:  66%|██████▌   | 33/50 [00:34<00:03,  4.56it/s]Train:  68%|██████▊   | 34/50 [00:34<00:03,  4.56it/s]Train:  70%|███████   | 35/50 [00:34<00:03,  4.56it/s]Train:  72%|███████▏  | 36/50 [00:34<00:03,  4.58it/s]Train:  74%|███████▍  | 37/50 [00:35<00:02,  4.58it/s]Train:  76%|███████▌  | 38/50 [00:35<00:02,  4.55it/s]Train:  78%|███████▊  | 39/50 [00:35<00:02,  4.56it/s]Train:  80%|████████  | 40/50 [00:35<00:02,  4.56it/s]Train:  82%|████████▏ | 41/50 [00:35<00:01,  4.53it/s]Train:  84%|████████▍ | 42/50 [00:36<00:01,  4.55it/s]Train:  86%|████████▌ | 43/50 [00:36<00:01,  4.56it/s]Train:  88%|████████▊ | 44/50 [00:36<00:01,  4.57it/s]Train:  90%|█████████ | 45/50 [00:36<00:01,  4.56it/s]Train:  92%|█████████▏| 46/50 [00:37<00:00,  4.57it/s]Train:  94%|█████████▍| 47/50 [00:37<00:00,  4.56it/s]Train:  96%|█████████▌| 48/50 [00:37<00:00,  4.55it/s]Train:  98%|█████████▊| 49/50 [00:37<00:00,  4.57it/s]Train: 100%|██████████| 50/50 [00:37<00:00,  4.54it/s]Train: 100%|██████████| 50/50 [00:37<00:00,  1.32it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.21it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.89it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.12it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.29it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.39it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.44it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.47it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.51it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.53it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.51it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.54it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.54it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.54it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.54it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.55it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.55it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.55it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.55it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.54it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.55it/s]Train:  42%|████▏     | 21/50 [00:04<00:07,  3.70it/s]Train:  44%|████▍     | 22/50 [00:05<00:07,  3.52it/s]Train:  46%|████▌     | 23/50 [00:05<00:07,  3.77it/s]Train:  48%|████▊     | 24/50 [00:05<00:06,  3.96it/s]Train:  50%|█████     | 25/50 [00:05<00:06,  4.13it/s]Train:  52%|█████▏    | 26/50 [00:06<00:05,  4.24it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.34it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.42it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.48it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.52it/s]Train:  62%|██████▏   | 31/50 [00:07<00:04,  4.54it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.53it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.53it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.55it/s]Train:  70%|███████   | 35/50 [00:08<00:03,  4.56it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.58it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.57it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.58it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.60it/s]Train:  80%|████████  | 40/50 [00:09<00:02,  4.57it/s]Train:  82%|████████▏ | 41/50 [00:09<00:01,  4.54it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.57it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.57it/s]Train:  88%|████████▊ | 44/50 [00:10<00:01,  4.58it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.57it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.58it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.57it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.58it/s]Train:  98%|█████████▊| 49/50 [00:11<00:00,  4.55it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.55it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.40it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.27it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.90it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.19it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.31it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.41it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.45it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.51it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.51it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.54it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.56it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.56it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.56it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.57it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.58it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.57it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.57it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.55it/s]Train:  36%|███▌      | 18/50 [00:04<00:06,  4.58it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.56it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.56it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.55it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.53it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.51it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.51it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.52it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.50it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.50it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.52it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.54it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.55it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.54it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.53it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.55it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.57it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.57it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.56it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.57it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.54it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.54it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.55it/s]Train:  82%|████████▏ | 41/50 [00:09<00:01,  4.53it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.53it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.54it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.55it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.54it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.56it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.54it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.54it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.52it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.53it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.50it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.13it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.84it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.13it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.25it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.36it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.43it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.48it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.50it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.50it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.51it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.52it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.54it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.54it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.54it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.57it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.57it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.58it/s]Train:  36%|███▌      | 18/50 [00:04<00:06,  4.58it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.58it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.46it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.48it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.50it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.52it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.53it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.54it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.53it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.53it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.53it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.54it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.54it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.54it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.54it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.56it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.57it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.55it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.53it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.53it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.54it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.53it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.55it/s]Train:  82%|████████▏ | 41/50 [00:09<00:01,  4.56it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.54it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.56it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.55it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.54it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.55it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.56it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.57it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.55it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.57it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.49it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.20it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.89it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.18it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.31it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.39it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.45it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.49it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.51it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.52it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.52it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.54it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.56it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.56it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.57it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.57it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.56it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.56it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.56it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.56it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.57it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.57it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.57it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.57it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.55it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.54it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.55it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.54it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.54it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.55it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.54it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.56it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.56it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.55it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.57it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.55it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.56it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.56it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.55it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.55it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.55it/s]Train:  82%|████████▏ | 41/50 [00:09<00:01,  4.55it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.53it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.54it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.54it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.56it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.56it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.56it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.55it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.53it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.50it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.49it/s]
0: Process 0 - Local timer:  load_data  :  86.47
0: Process 0 - Local timer:  train_validate_test  :  82.84
0: Process 0 - Local timer:  create_model  :  1.2
0: Minimum timers: 
0: load_data  :  86.47
0: train_validate_test  :  82.78
0: create_model  :  1.18
0: Maximum timers: 
0: load_data  :  86.86
0: train_validate_test  :  82.84
0: create_model  :  1.2
0: Average timers: 
0: load_data  :  86.65
0: train_validate_test  :  82.8
0: create_model  :  1.2
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 17:46:28.554682613 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 17:46:29.148834496 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 17:46:29.149023254 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 17:46:29.149334043 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 17:46:29.150770616 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 17:46:29.150829749 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 17:46:29.039598041 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 17:46:29.039590216 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 17:46:29.039593683 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 17:46:29.039610625 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 17:46:29.751927958 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 17:46:29.039606216 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 17:46:29.751920524 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 17:46:29.039579967 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 17:46:29.751941073 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 17:46:29.039617077 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 17:46:29.751930753 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 17:46:29.751918150 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 17:46:29.151285262 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 17:46:29.751953817 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 17:46:29.751990456 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 17:46:29.708448821 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 17:46:29.708445886 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 17:46:29.708435356 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 17:46:29.708441107 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 17:46:29.708447258 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 17:46:29.708432991 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 17:46:29.708448100 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 17:46:29.708458420 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 17:46:29.752739625 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 17:46:29.152824049 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 17:46:29.413664543 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 17:46:29.413691194 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 17:46:29.413687637 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 17:46:29.413673861 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 17:46:29.413683870 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 17:46:29.413694070 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 17:46:29.413827523 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 17:46:29.413844696 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 17:46:29.043177514 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 05:46:32 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 05:46:32 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_3 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 17:46:44.136147226 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136308233 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136397362 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136654892 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136674259 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136683817 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.136676934 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:44.137761848 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288277545 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288344923 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288427299 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288458608 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288570720 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.288573135 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.290000577 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.290031195 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.708708377 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.709312313 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.709370944 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.709377136 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.709396432 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.709394900 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.710919973 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:45.710920024 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 17:46:46.648907756 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.648918637 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.648971958 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.648989812 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.648983580 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.648998318 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.651032274 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.665972956 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.188654693 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.188694568 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.188752709 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.188965532 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.190229388 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.190251410 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.190479161 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:46:46.191284949 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_3 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Read attr time (sec):  0.0019359588623046875
0: read and bcast: trainset/x/variable_count 0.17757749557495117
0: read and bcast: trainset/x/variable_offset 0.3528299331665039
0: read and bcast: trainset/x/variable_dim 0.35320281982421875
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5344324111938477
0: read and bcast: trainset/edge_index/variable_offset 0.7471945285797119
0: read and bcast: trainset/edge_index/variable_dim 0.7567605972290039
0: read and bcast: trainset/edge_attr/variable_count 0.9315457344055176
0: read and bcast: trainset/edge_attr/variable_offset 1.1073012351989746
0: read and bcast: trainset/edge_attr/variable_dim 1.1172964572906494
0: read and bcast: trainset/pos/variable_count 1.284947395324707
0: read and bcast: trainset/pos/variable_offset 1.4709141254425049
0: read and bcast: trainset/pos/variable_dim 1.4804270267486572
0: read and bcast: trainset/energy/variable_count 1.6541078090667725
0: read and bcast: trainset/energy/variable_offset 1.8336009979248047
0: read and bcast: trainset/energy/variable_dim 1.8435649871826172
0: read and bcast: trainset/forces/variable_count 2.022686004638672
0: read and bcast: trainset/forces/variable_offset 2.200941562652588
0: read and bcast: trainset/forces/variable_dim 2.210585832595825
0: read and bcast: trainset/y/variable_count 2.3900678157806396
0: read and bcast: trainset/y/variable_offset 2.5717742443084717
0: read and bcast: trainset/y/variable_dim 2.5827295780181885
0: Overall time (sec):  2.584885358810425
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.589627265930176
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007364749908447266
0: read and bcast: valset/x/variable_count 0.007938146591186523
0: read and bcast: valset/x/variable_offset 0.015050411224365234
0: read and bcast: valset/x/variable_dim 0.015203714370727539
0: read and bcast: valset/edge_index/variable_count 0.022372961044311523
0: read and bcast: valset/edge_index/variable_offset 0.029321908950805664
0: read and bcast: valset/edge_index/variable_dim 0.029471158981323242
0: read and bcast: valset/edge_attr/variable_count 0.03623795509338379
0: read and bcast: valset/edge_attr/variable_offset 0.04321551322937012
0: read and bcast: valset/edge_attr/variable_dim 0.043378591537475586
0: read and bcast: valset/pos/variable_count 0.05043745040893555
0: read and bcast: valset/pos/variable_offset 0.057195186614990234
0: read and bcast: valset/pos/variable_dim 0.05733513832092285
0: read and bcast: valset/energy/variable_count 0.06407690048217773
0: read and bcast: valset/energy/variable_offset 0.07111215591430664
0: read and bcast: valset/energy/variable_dim 0.0712745189666748
0: read and bcast: valset/forces/variable_count 0.0781259536743164
0: read and bcast: valset/forces/variable_offset 0.08516097068786621
0: read and bcast: valset/forces/variable_dim 0.08531951904296875
0: read and bcast: valset/y/variable_count 0.09205293655395508
0: read and bcast: valset/y/variable_offset 0.09901189804077148
0: read and bcast: valset/y/variable_dim 0.09917688369750977
0: Overall time (sec):  0.1001133918762207
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10285782814025879
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0006470680236816406
0: read and bcast: testset/x/variable_count 0.00715327262878418
0: read and bcast: testset/x/variable_offset 0.013817071914672852
0: read and bcast: testset/x/variable_dim 0.01397705078125
0: read and bcast: testset/edge_index/variable_count 0.02066349983215332
0: read and bcast: testset/edge_index/variable_offset 0.027523279190063477
0: read and bcast: testset/edge_index/variable_dim 0.027712106704711914
0: read and bcast: testset/edge_attr/variable_count 0.034726619720458984
0: read and bcast: testset/edge_attr/variable_offset 0.04160666465759277
0: read and bcast: testset/edge_attr/variable_dim 0.041771888732910156
0: read and bcast: testset/pos/variable_count 0.04862260818481445
0: read and bcast: testset/pos/variable_offset 0.0555882453918457
0: read and bcast: testset/pos/variable_dim 0.05575156211853027
0: read and bcast: testset/energy/variable_count 0.06271028518676758
0: read and bcast: testset/energy/variable_offset 0.06959295272827148
0: read and bcast: testset/energy/variable_dim 0.06975007057189941
0: read and bcast: testset/forces/variable_count 0.0766000747680664
0: read and bcast: testset/forces/variable_offset 0.08351278305053711
0: read and bcast: testset/forces/variable_dim 0.08369016647338867
0: read and bcast: testset/y/variable_count 0.0905914306640625
0: read and bcast: testset/y/variable_offset 0.09750223159790039
0: read and bcast: testset/y/variable_dim 0.09766674041748047
0: Overall time (sec):  0.09848976135253906
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10137367248535156
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
10 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
29 transition1x nsplit: 8680250 11 789114
34 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
25 alexandria nsplit: 9705384 11 882307
26 alexandria nsplit: 9705384 11 882307
27 alexandria nsplit: 9705384 11 882307
20 alexandria nsplit: 9705384 11 882308
21 alexandria nsplit: 9705384 11 882308
23 alexandria nsplit: 9705384 11 882308
19 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
0: Adios reading time (sec):  0.45911622047424316
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
0: Adios reading time (sec):  0.1324467658996582
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.12436318397521973
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 213/64000 [00:00<00:29, 2129.61it/s]Loading:   1%|          | 463/64000 [00:00<00:27, 2342.45it/s]Loading:   1%|          | 728/64000 [00:00<00:25, 2478.77it/s]Loading:   2%|▏         | 993/64000 [00:00<00:24, 2542.40it/s]Loading:   2%|▏         | 1256/64000 [00:00<00:24, 2571.37it/s]Loading:   2%|▏         | 1514/64000 [00:00<00:24, 2574.17it/s]Loading:   3%|▎         | 1772/64000 [00:00<00:24, 2569.13it/s]Loading:   3%|▎         | 2030/64000 [00:00<00:24, 2571.19it/s]Loading:   4%|▎         | 2288/64000 [00:00<00:24, 2567.51it/s]Loading:   4%|▍         | 2545/64000 [00:01<00:24, 2546.85it/s]Loading:   4%|▍         | 2802/64000 [00:01<00:23, 2553.34it/s]Loading:   5%|▍         | 3059/64000 [00:01<00:23, 2557.52it/s]Loading:   5%|▌         | 3315/64000 [00:01<00:23, 2555.65it/s]Loading:   6%|▌         | 3572/64000 [00:01<00:23, 2559.17it/s]Loading:   6%|▌         | 3828/64000 [00:01<00:23, 2554.41it/s]Loading:   6%|▋         | 4084/64000 [00:01<00:34, 1741.02it/s]Loading:   7%|▋         | 4342/64000 [00:01<00:30, 1929.41it/s]Loading:   7%|▋         | 4600/64000 [00:01<00:28, 2087.13it/s]Loading:   8%|▊         | 4860/64000 [00:02<00:26, 2218.61it/s]Loading:   8%|▊         | 5118/64000 [00:02<00:25, 2314.78it/s]Loading:   8%|▊         | 5376/64000 [00:02<00:24, 2388.04it/s]Loading:   9%|▉         | 5632/64000 [00:02<00:23, 2436.53it/s]Loading:   9%|▉         | 5891/64000 [00:02<00:23, 2480.27it/s]Loading:  10%|▉         | 6149/64000 [00:02<00:23, 2507.93it/s]Loading:  10%|█         | 6408/64000 [00:02<00:22, 2531.05it/s]Loading:  10%|█         | 6666/64000 [00:02<00:22, 2544.09it/s]Loading:  11%|█         | 6924/64000 [00:02<00:22, 2554.59it/s]Loading:  11%|█         | 7183/64000 [00:02<00:22, 2562.63it/s]Loading:  12%|█▏        | 7443/64000 [00:03<00:21, 2572.06it/s]Loading:  12%|█▏        | 7702/64000 [00:03<00:21, 2577.25it/s]Loading:  12%|█▏        | 7961/64000 [00:03<00:21, 2573.72it/s]Loading:  13%|█▎        | 8220/64000 [00:03<00:21, 2577.11it/s]Loading:  13%|█▎        | 8479/64000 [00:03<00:21, 2579.32it/s]Loading:  14%|█▎        | 8739/64000 [00:03<00:21, 2583.32it/s]Loading:  14%|█▍        | 8998/64000 [00:03<00:21, 2580.74it/s]Loading:  14%|█▍        | 9257/64000 [00:03<00:21, 2582.39it/s]Loading:  15%|█▍        | 9516/64000 [00:03<00:21, 2581.10it/s]Loading:  15%|█▌        | 9775/64000 [00:03<00:20, 2583.37it/s]Loading:  16%|█▌        | 10034/64000 [00:04<00:20, 2584.20it/s]Loading:  16%|█▌        | 10294/64000 [00:04<00:20, 2588.83it/s]Loading:  16%|█▋        | 10553/64000 [00:04<00:20, 2577.23it/s]Loading:  17%|█▋        | 10811/64000 [00:04<00:20, 2575.10it/s]Loading:  17%|█▋        | 11069/64000 [00:04<00:20, 2572.07it/s]Loading:  18%|█▊        | 11328/64000 [00:04<00:20, 2577.41it/s]Loading:  18%|█▊        | 11586/64000 [00:04<00:20, 2575.20it/s]Loading:  19%|█▊        | 11844/64000 [00:04<00:20, 2566.77it/s]Loading:  19%|█▉        | 12102/64000 [00:04<00:20, 2568.98it/s]Loading:  19%|█▉        | 12359/64000 [00:05<00:28, 1790.49it/s]Loading:  20%|█▉        | 12616/64000 [00:05<00:26, 1967.52it/s]Loading:  20%|██        | 12872/64000 [00:05<00:24, 2113.27it/s]Loading:  21%|██        | 13131/64000 [00:05<00:22, 2235.65it/s]Loading:  21%|██        | 13389/64000 [00:05<00:21, 2326.77it/s]Loading:  21%|██▏       | 13647/64000 [00:05<00:21, 2395.88it/s]Loading:  22%|██▏       | 13904/64000 [00:05<00:20, 2443.76it/s]Loading:  22%|██▏       | 14161/64000 [00:05<00:20, 2478.42it/s]Loading:  23%|██▎       | 14418/64000 [00:05<00:19, 2504.49it/s]Loading:  23%|██▎       | 14677/64000 [00:06<00:19, 2527.30it/s]Loading:  23%|██▎       | 14936/64000 [00:06<00:19, 2543.74it/s]Loading:  24%|██▎       | 15194/64000 [00:06<00:19, 2552.11it/s]Loading:  24%|██▍       | 15452/64000 [00:06<00:18, 2559.87it/s]Loading:  25%|██▍       | 15710/64000 [00:06<00:18, 2563.14it/s]Loading:  25%|██▍       | 15969/64000 [00:06<00:18, 2569.40it/s]Loading:  25%|██▌       | 16227/64000 [00:06<00:18, 2569.69it/s]Loading:  26%|██▌       | 16485/64000 [00:06<00:18, 2571.46it/s]Loading:  26%|██▌       | 16743/64000 [00:06<00:18, 2556.06it/s]Loading:  27%|██▋       | 17000/64000 [00:06<00:18, 2557.30it/s]Loading:  27%|██▋       | 17260/64000 [00:07<00:18, 2567.32it/s]Loading:  27%|██▋       | 17520/64000 [00:07<00:18, 2576.34it/s]Loading:  28%|██▊       | 17779/64000 [00:07<00:17, 2579.22it/s]Loading:  28%|██▊       | 18038/64000 [00:07<00:17, 2579.92it/s]Loading:  29%|██▊       | 18297/64000 [00:07<00:17, 2576.73it/s]Loading:  29%|██▉       | 18557/64000 [00:07<00:17, 2581.25it/s]Loading:  29%|██▉       | 18816/64000 [00:07<00:17, 2580.26it/s]Loading:  30%|██▉       | 19075/64000 [00:07<00:17, 2575.73it/s]Loading:  30%|███       | 19333/64000 [00:07<00:17, 2570.37it/s]Loading:  31%|███       | 19591/64000 [00:07<00:17, 2568.56it/s]Loading:  31%|███       | 19849/64000 [00:08<00:17, 2570.78it/s]Loading:  31%|███▏      | 20107/64000 [00:08<00:17, 2568.66it/s]Loading:  32%|███▏      | 20366/64000 [00:08<00:16, 2574.93it/s]Loading:  32%|███▏      | 20624/64000 [00:08<00:16, 2568.57it/s]Loading:  33%|███▎      | 20881/64000 [00:08<00:16, 2567.07it/s]Loading:  33%|███▎      | 21139/64000 [00:08<00:16, 2569.18it/s]Loading:  33%|███▎      | 21398/64000 [00:08<00:16, 2574.79it/s]Loading:  34%|███▍      | 21656/64000 [00:08<00:16, 2575.11it/s]Loading:  34%|███▍      | 21914/64000 [00:08<00:16, 2573.01it/s]Loading:  35%|███▍      | 22172/64000 [00:09<00:24, 1684.81it/s]Loading:  35%|███▌      | 22430/64000 [00:09<00:22, 1879.57it/s]Loading:  35%|███▌      | 22687/64000 [00:09<00:20, 2042.42it/s]Loading:  36%|███▌      | 22943/64000 [00:09<00:18, 2172.85it/s]Loading:  36%|███▋      | 23201/64000 [00:09<00:17, 2280.31it/s]Loading:  37%|███▋      | 23458/64000 [00:09<00:17, 2359.51it/s]Loading:  37%|███▋      | 23716/64000 [00:09<00:16, 2420.76it/s]Loading:  37%|███▋      | 23972/64000 [00:09<00:16, 2460.68it/s]Loading:  38%|███▊      | 24228/64000 [00:09<00:15, 2486.78it/s]Loading:  38%|███▊      | 24485/64000 [00:10<00:15, 2509.82it/s]Loading:  39%|███▊      | 24744/64000 [00:10<00:15, 2531.79it/s]Loading:  39%|███▉      | 25000/64000 [00:10<00:15, 2539.37it/s]Loading:  39%|███▉      | 25258/64000 [00:10<00:15, 2549.07it/s]Loading:  40%|███▉      | 25515/64000 [00:10<00:15, 2550.64it/s]Loading:  40%|████      | 25771/64000 [00:10<00:15, 2544.81it/s]Loading:  41%|████      | 26028/64000 [00:10<00:14, 2550.93it/s]Loading:  41%|████      | 26285/64000 [00:10<00:14, 2555.82it/s]Loading:  41%|████▏     | 26541/64000 [00:10<00:14, 2554.82it/s]Loading:  42%|████▏     | 26797/64000 [00:10<00:14, 2545.98it/s]Loading:  42%|████▏     | 27055/64000 [00:11<00:14, 2553.79it/s]Loading:  43%|████▎     | 27312/64000 [00:11<00:14, 2556.77it/s]Loading:  43%|████▎     | 27569/64000 [00:11<00:14, 2559.62it/s]Loading:  43%|████▎     | 27826/64000 [00:11<00:14, 2557.50it/s]Loading:  44%|████▍     | 28082/64000 [00:11<00:14, 2555.13it/s]Loading:  44%|████▍     | 28339/64000 [00:11<00:13, 2556.77it/s]Loading:  45%|████▍     | 28598/64000 [00:11<00:13, 2564.38it/s]Loading:  45%|████▌     | 28855/64000 [00:11<00:13, 2543.76it/s]Loading:  45%|████▌     | 29113/64000 [00:11<00:13, 2552.49it/s]Loading:  46%|████▌     | 29369/64000 [00:11<00:13, 2544.90it/s]Loading:  46%|████▋     | 29627/64000 [00:12<00:13, 2553.52it/s]Loading:  47%|████▋     | 29885/64000 [00:12<00:13, 2559.42it/s]Loading:  47%|████▋     | 30144/64000 [00:12<00:13, 2565.70it/s]Loading:  48%|████▊     | 30401/64000 [00:12<00:13, 2566.57it/s]Loading:  48%|████▊     | 30659/64000 [00:12<00:12, 2568.92it/s]Loading:  48%|████▊     | 30918/64000 [00:12<00:12, 2572.36it/s]Loading:  49%|████▊     | 31176/64000 [00:12<00:12, 2573.01it/s]Loading:  49%|████▉     | 31434/64000 [00:12<00:12, 2571.88it/s]Loading:  50%|████▉     | 31692/64000 [00:12<00:12, 2566.76it/s]Loading:  50%|████▉     | 31949/64000 [00:12<00:12, 2560.94it/s]Loading:  50%|█████     | 32206/64000 [00:13<00:12, 2560.58it/s]Loading:  51%|█████     | 32464/64000 [00:13<00:12, 2563.99it/s]Loading:  51%|█████     | 32721/64000 [00:13<00:12, 2563.78it/s]Loading:  52%|█████▏    | 32978/64000 [00:13<00:12, 2564.94it/s]Loading:  52%|█████▏    | 33235/64000 [00:13<00:11, 2564.39it/s]Loading:  52%|█████▏    | 33492/64000 [00:13<00:11, 2565.09it/s]Loading:  53%|█████▎    | 33749/64000 [00:13<00:11, 2557.83it/s]Loading:  53%|█████▎    | 34008/64000 [00:13<00:11, 2564.80it/s]Loading:  54%|█████▎    | 34266/64000 [00:13<00:11, 2568.66it/s]Loading:  54%|█████▍    | 34523/64000 [00:13<00:11, 2567.75it/s]Loading:  54%|█████▍    | 34780/64000 [00:14<00:11, 2562.26it/s]Loading:  55%|█████▍    | 35037/64000 [00:14<00:18, 1554.61it/s]Loading:  55%|█████▌    | 35293/64000 [00:14<00:16, 1761.42it/s]Loading:  56%|█████▌    | 35549/64000 [00:14<00:14, 1942.65it/s]Loading:  56%|█████▌    | 35806/64000 [00:14<00:13, 2094.73it/s]Loading:  56%|█████▋    | 36063/64000 [00:14<00:12, 2217.70it/s]Loading:  57%|█████▋    | 36320/64000 [00:14<00:11, 2312.82it/s]Loading:  57%|█████▋    | 36578/64000 [00:14<00:11, 2385.41it/s]Loading:  58%|█████▊    | 36835/64000 [00:15<00:11, 2436.16it/s]Loading:  58%|█████▊    | 37091/64000 [00:15<00:10, 2470.83it/s]Loading:  58%|█████▊    | 37347/64000 [00:15<00:10, 2496.59it/s]Loading:  59%|█████▉    | 37605/64000 [00:15<00:10, 2520.04it/s]Loading:  59%|█████▉    | 37862/64000 [00:15<00:10, 2532.19it/s]Loading:  60%|█████▉    | 38120/64000 [00:15<00:10, 2545.92it/s]Loading:  60%|█████▉    | 38377/64000 [00:15<00:10, 2549.89it/s]Loading:  60%|██████    | 38634/64000 [00:15<00:09, 2555.33it/s]Loading:  61%|██████    | 38891/64000 [00:15<00:09, 2556.12it/s]Loading:  61%|██████    | 39149/64000 [00:15<00:09, 2560.42it/s]Loading:  62%|██████▏   | 39406/64000 [00:16<00:09, 2555.15it/s]Loading:  62%|██████▏   | 39663/64000 [00:16<00:09, 2557.41it/s]Loading:  62%|██████▏   | 39919/64000 [00:16<00:09, 2557.73it/s]Loading:  63%|██████▎   | 40177/64000 [00:16<00:09, 2564.09it/s]Loading:  63%|██████▎   | 40434/64000 [00:16<00:09, 2565.49it/s]Loading:  64%|██████▎   | 40691/64000 [00:16<00:09, 2552.62it/s]Loading:  64%|██████▍   | 40948/64000 [00:16<00:09, 2555.94it/s]Loading:  64%|██████▍   | 41205/64000 [00:16<00:08, 2559.08it/s]Loading:  65%|██████▍   | 41461/64000 [00:16<00:08, 2534.42it/s]Loading:  65%|██████▌   | 41715/64000 [00:16<00:08, 2534.86it/s]Loading:  66%|██████▌   | 41969/64000 [00:17<00:08, 2526.09it/s]Loading:  66%|██████▌   | 42224/64000 [00:17<00:08, 2530.83it/s]Loading:  66%|██████▋   | 42480/64000 [00:17<00:08, 2538.44it/s]Loading:  67%|██████▋   | 42735/64000 [00:17<00:08, 2541.45it/s]Loading:  67%|██████▋   | 42991/64000 [00:17<00:08, 2544.66it/s]Loading:  68%|██████▊   | 43246/64000 [00:17<00:08, 2544.37it/s]Loading:  68%|██████▊   | 43502/64000 [00:17<00:08, 2547.71it/s]Loading:  68%|██████▊   | 43758/64000 [00:17<00:07, 2550.72it/s]Loading:  69%|██████▉   | 44015/64000 [00:17<00:07, 2554.57it/s]Loading:  69%|██████▉   | 44272/64000 [00:17<00:07, 2558.40it/s]Loading:  70%|██████▉   | 44528/64000 [00:18<00:07, 2549.69it/s]Loading:  70%|██████▉   | 44783/64000 [00:18<00:07, 2544.09it/s]Loading:  70%|███████   | 45039/64000 [00:18<00:07, 2548.38it/s]Loading:  71%|███████   | 45296/64000 [00:18<00:07, 2552.54it/s]Loading:  71%|███████   | 45553/64000 [00:18<00:07, 2557.29it/s]Loading:  72%|███████▏  | 45811/64000 [00:18<00:07, 2562.54it/s]Loading:  72%|███████▏  | 46068/64000 [00:18<00:07, 2559.99it/s]Loading:  72%|███████▏  | 46325/64000 [00:18<00:06, 2561.02it/s]Loading:  73%|███████▎  | 46582/64000 [00:18<00:06, 2556.66it/s]Loading:  73%|███████▎  | 46839/64000 [00:18<00:06, 2558.04it/s]Loading:  74%|███████▎  | 47095/64000 [00:19<00:06, 2551.92it/s]Loading:  74%|███████▍  | 47351/64000 [00:19<00:06, 2554.15it/s]Loading:  74%|███████▍  | 47607/64000 [00:19<00:06, 2547.73it/s]Loading:  75%|███████▍  | 47864/64000 [00:19<00:06, 2552.40it/s]Loading:  75%|███████▌  | 48123/64000 [00:19<00:06, 2561.78it/s]Loading:  76%|███████▌  | 48380/64000 [00:19<00:06, 2561.58it/s]Loading:  76%|███████▌  | 48637/64000 [00:19<00:06, 2557.38it/s]Loading:  76%|███████▋  | 48893/64000 [00:19<00:05, 2554.23it/s]Loading:  77%|███████▋  | 49149/64000 [00:19<00:05, 2555.85it/s]Loading:  77%|███████▋  | 49406/64000 [00:19<00:05, 2558.06it/s]Loading:  78%|███████▊  | 49662/64000 [00:20<00:05, 2553.41it/s]Loading:  78%|███████▊  | 49918/64000 [00:20<00:05, 2547.86it/s]Loading:  78%|███████▊  | 50173/64000 [00:20<00:05, 2547.58it/s]Loading:  79%|███████▉  | 50428/64000 [00:20<00:05, 2546.54it/s]Loading:  79%|███████▉  | 50685/64000 [00:20<00:05, 2553.12it/s]Loading:  80%|███████▉  | 50941/64000 [00:20<00:05, 2552.91it/s]Loading:  80%|███████▉  | 51197/64000 [00:20<00:08, 1442.02it/s]Loading:  80%|████████  | 51453/64000 [00:21<00:07, 1658.66it/s]Loading:  81%|████████  | 51708/64000 [00:21<00:06, 1851.68it/s]Loading:  81%|████████  | 51962/64000 [00:21<00:05, 2014.24it/s]Loading:  82%|████████▏ | 52218/64000 [00:21<00:05, 2151.01it/s]Loading:  82%|████████▏ | 52476/64000 [00:21<00:05, 2262.75it/s]Loading:  82%|████████▏ | 52734/64000 [00:21<00:04, 2347.60it/s]Loading:  83%|████████▎ | 52992/64000 [00:21<00:04, 2410.74it/s]Loading:  83%|████████▎ | 53248/64000 [00:21<00:04, 2451.72it/s]Loading:  84%|████████▎ | 53504/64000 [00:21<00:04, 2482.26it/s]Loading:  84%|████████▍ | 53758/64000 [00:21<00:04, 2484.87it/s]Loading:  84%|████████▍ | 54015/64000 [00:22<00:03, 2509.29it/s]Loading:  85%|████████▍ | 54270/64000 [00:22<00:03, 2521.28it/s]Loading:  85%|████████▌ | 54527/64000 [00:22<00:03, 2532.97it/s]Loading:  86%|████████▌ | 54783/64000 [00:22<00:03, 2538.57it/s]Loading:  86%|████████▌ | 55039/64000 [00:22<00:03, 2544.31it/s]Loading:  86%|████████▋ | 55296/64000 [00:22<00:03, 2549.93it/s]Loading:  87%|████████▋ | 55552/64000 [00:22<00:03, 2548.06it/s]Loading:  87%|████████▋ | 55811/64000 [00:22<00:03, 2559.94it/s]Loading:  88%|████████▊ | 56068/64000 [00:22<00:03, 2553.74it/s]Loading:  88%|████████▊ | 56324/64000 [00:22<00:03, 2555.08it/s]Loading:  88%|████████▊ | 56580/64000 [00:23<00:02, 2554.83it/s]Loading:  89%|████████▉ | 56836/64000 [00:23<00:02, 2553.98it/s]Loading:  89%|████████▉ | 57092/64000 [00:23<00:02, 2550.26it/s]Loading:  90%|████████▉ | 57348/64000 [00:23<00:02, 2549.80it/s]Loading:  90%|█████████ | 57604/64000 [00:23<00:02, 2551.57it/s]Loading:  90%|█████████ | 57861/64000 [00:23<00:02, 2556.38it/s]Loading:  91%|█████████ | 58117/64000 [00:23<00:02, 2555.76it/s]Loading:  91%|█████████ | 58374/64000 [00:23<00:02, 2557.29it/s]Loading:  92%|█████████▏| 58631/64000 [00:23<00:02, 2558.55it/s]Loading:  92%|█████████▏| 58888/64000 [00:23<00:01, 2559.69it/s]Loading:  92%|█████████▏| 59144/64000 [00:24<00:01, 2556.63it/s]Loading:  93%|█████████▎| 59400/64000 [00:24<00:01, 2552.65it/s]Loading:  93%|█████████▎| 59656/64000 [00:24<00:01, 2551.48it/s]Loading:  94%|█████████▎| 59912/64000 [00:24<00:01, 2549.55it/s]Loading:  94%|█████████▍| 60168/64000 [00:24<00:01, 2550.38it/s]Loading:  94%|█████████▍| 60424/64000 [00:24<00:01, 2551.09it/s]Loading:  95%|█████████▍| 60681/64000 [00:24<00:01, 2555.94it/s]Loading:  95%|█████████▌| 60937/64000 [00:24<00:01, 2552.64it/s]Loading:  96%|█████████▌| 61193/64000 [00:24<00:01, 2554.61it/s]Loading:  96%|█████████▌| 61449/64000 [00:24<00:00, 2555.47it/s]Loading:  96%|█████████▋| 61706/64000 [00:25<00:00, 2558.77it/s]Loading:  97%|█████████▋| 61962/64000 [00:25<00:00, 2557.73it/s]Loading:  97%|█████████▋| 62218/64000 [00:25<00:00, 2545.75it/s]Loading:  98%|█████████▊| 62475/64000 [00:25<00:00, 2550.17it/s]Loading:  98%|█████████▊| 62732/64000 [00:25<00:00, 2554.97it/s]Loading:  98%|█████████▊| 62988/64000 [00:25<00:00, 2555.27it/s]Loading:  99%|█████████▉| 63244/64000 [00:25<00:00, 2554.16it/s]Loading:  99%|█████████▉| 63501/64000 [00:25<00:00, 2556.53it/s]Loading: 100%|█████████▉| 63757/64000 [00:25<00:00, 2552.16it/s]Loading: 100%|██████████| 64000/64000 [00:25<00:00, 2466.87it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 253/49018 [00:00<00:19, 2525.88it/s]Loading:   1%|          | 521/49018 [00:00<00:18, 2611.12it/s]Loading:   2%|▏         | 790/49018 [00:00<00:18, 2643.74it/s]Loading:   2%|▏         | 1057/49018 [00:00<00:18, 2651.37it/s]Loading:   3%|▎         | 1324/49018 [00:00<00:17, 2657.76it/s]Loading:   3%|▎         | 1590/49018 [00:00<00:17, 2648.51it/s]Loading:   4%|▍         | 1860/49018 [00:00<00:17, 2662.59it/s]Loading:   4%|▍         | 2127/49018 [00:00<00:17, 2664.26it/s]Loading:   5%|▍         | 2396/49018 [00:00<00:17, 2670.62it/s]Loading:   5%|▌         | 2664/49018 [00:01<00:17, 2670.19it/s]Loading:   6%|▌         | 2932/49018 [00:01<00:17, 2664.32it/s]Loading:   7%|▋         | 3199/49018 [00:01<00:17, 2665.53it/s]Loading:   7%|▋         | 3467/49018 [00:01<00:17, 2669.58it/s]Loading:   8%|▊         | 3735/49018 [00:01<00:16, 2670.43it/s]Loading:   8%|▊         | 4004/49018 [00:01<00:16, 2674.37it/s]Loading:   9%|▊         | 4272/49018 [00:01<00:16, 2667.07it/s]Loading:   9%|▉         | 4539/49018 [00:01<00:16, 2666.93it/s]Loading:  10%|▉         | 4806/49018 [00:01<00:16, 2665.32it/s]Loading:  10%|█         | 5074/49018 [00:01<00:16, 2669.31it/s]Loading:  11%|█         | 5341/49018 [00:02<00:16, 2663.27it/s]Loading:  11%|█▏        | 5608/49018 [00:02<00:16, 2659.47it/s]Loading:  12%|█▏        | 5874/49018 [00:02<00:16, 2649.08it/s]Loading:  13%|█▎        | 6142/49018 [00:02<00:16, 2656.65it/s]Loading:  13%|█▎        | 6408/49018 [00:02<00:16, 2654.20it/s]Loading:  14%|█▎        | 6676/49018 [00:02<00:15, 2659.20it/s]Loading:  14%|█▍        | 6942/49018 [00:02<00:15, 2653.35it/s]Loading:  15%|█▍        | 7208/49018 [00:02<00:15, 2644.91it/s]Loading:  15%|█▌        | 7473/49018 [00:02<00:15, 2645.65it/s]Loading:  16%|█▌        | 7740/49018 [00:02<00:15, 2652.79it/s]Loading:  16%|█▋        | 8006/49018 [00:03<00:15, 2652.27it/s]Loading:  17%|█▋        | 8272/49018 [00:03<00:15, 2650.46it/s]Loading:  17%|█▋        | 8538/49018 [00:03<00:15, 2650.63it/s]Loading:  18%|█▊        | 8805/49018 [00:03<00:15, 2654.09it/s]Loading:  19%|█▊        | 9071/49018 [00:03<00:15, 2654.39it/s]Loading:  19%|█▉        | 9338/49018 [00:03<00:14, 2657.24it/s]Loading:  20%|█▉        | 9605/49018 [00:03<00:14, 2658.38it/s]Loading:  20%|██        | 9871/49018 [00:03<00:14, 2619.09it/s]Loading:  21%|██        | 10138/49018 [00:03<00:14, 2633.98it/s]Loading:  21%|██        | 10403/49018 [00:03<00:14, 2638.28it/s]Loading:  22%|██▏       | 10669/49018 [00:04<00:14, 2642.29it/s]Loading:  22%|██▏       | 10934/49018 [00:04<00:14, 2644.13it/s]Loading:  23%|██▎       | 11199/49018 [00:04<00:14, 2645.33it/s]Loading:  23%|██▎       | 11465/49018 [00:04<00:14, 2647.06it/s]Loading:  24%|██▍       | 11730/49018 [00:04<00:14, 2644.44it/s]Loading:  24%|██▍       | 11996/49018 [00:04<00:13, 2646.85it/s]Loading:  25%|██▌       | 12261/49018 [00:04<00:13, 2644.30it/s]Loading:  26%|██▌       | 12526/49018 [00:04<00:13, 2644.00it/s]Loading:  26%|██▌       | 12792/49018 [00:04<00:13, 2646.97it/s]Loading:  27%|██▋       | 13057/49018 [00:04<00:13, 2646.11it/s]Loading:  27%|██▋       | 13322/49018 [00:05<00:13, 2619.20it/s]Loading:  28%|██▊       | 13584/49018 [00:05<00:13, 2603.92it/s]Loading:  28%|██▊       | 13845/49018 [00:05<00:13, 2595.12it/s]Loading:  29%|██▉       | 14105/49018 [00:05<00:13, 2590.35it/s]Loading:  29%|██▉       | 14365/49018 [00:05<00:13, 2589.73it/s]Loading:  30%|██▉       | 14625/49018 [00:05<00:13, 2592.37it/s]Loading:  30%|███       | 14885/49018 [00:05<00:13, 2586.95it/s]Loading:  31%|███       | 15144/49018 [00:05<00:13, 2586.41it/s]Loading:  31%|███▏      | 15403/49018 [00:05<00:13, 2582.68it/s]Loading:  32%|███▏      | 15662/49018 [00:05<00:12, 2582.13it/s]Loading:  32%|███▏      | 15921/49018 [00:06<00:12, 2580.88it/s]Loading:  33%|███▎      | 16180/49018 [00:06<00:12, 2581.56it/s]Loading:  34%|███▎      | 16439/49018 [00:06<00:12, 2581.38it/s]Loading:  34%|███▍      | 16698/49018 [00:06<00:12, 2580.54it/s]Loading:  35%|███▍      | 16957/49018 [00:06<00:12, 2579.58it/s]Loading:  35%|███▌      | 17217/49018 [00:06<00:12, 2582.83it/s]Loading:  36%|███▌      | 17476/49018 [00:06<00:12, 2580.78it/s]Loading:  36%|███▌      | 17735/49018 [00:06<00:12, 2576.60it/s]Loading:  37%|███▋      | 17993/49018 [00:07<00:24, 1261.01it/s]Loading:  37%|███▋      | 18250/49018 [00:07<00:20, 1486.17it/s]Loading:  38%|███▊      | 18509/49018 [00:07<00:17, 1703.37it/s]Loading:  38%|███▊      | 18765/49018 [00:07<00:15, 1891.09it/s]Loading:  39%|███▉      | 19022/49018 [00:07<00:14, 2053.30it/s]Loading:  39%|███▉      | 19281/49018 [00:07<00:13, 2188.24it/s]Loading:  40%|███▉      | 19538/49018 [00:07<00:12, 2289.07it/s]Loading:  40%|████      | 19794/49018 [00:07<00:12, 2363.25it/s]Loading:  41%|████      | 20053/49018 [00:07<00:11, 2426.35it/s]Loading:  41%|████▏     | 20309/49018 [00:08<00:11, 2463.19it/s]Loading:  42%|████▏     | 20569/49018 [00:08<00:11, 2501.39it/s]Loading:  42%|████▏     | 20826/49018 [00:08<00:11, 2521.51it/s]Loading:  43%|████▎     | 21086/49018 [00:08<00:10, 2543.09it/s]Loading:  44%|████▎     | 21346/49018 [00:08<00:10, 2558.38it/s]Loading:  44%|████▍     | 21604/49018 [00:08<00:10, 2564.74it/s]Loading:  45%|████▍     | 21863/49018 [00:08<00:10, 2572.04it/s]Loading:  45%|████▌     | 22122/49018 [00:08<00:10, 2575.04it/s]Loading:  46%|████▌     | 22382/49018 [00:08<00:10, 2582.42it/s]Loading:  46%|████▌     | 22641/49018 [00:08<00:10, 2582.61it/s]Loading:  47%|████▋     | 22900/49018 [00:09<00:10, 2581.38it/s]Loading:  47%|████▋     | 23159/49018 [00:09<00:10, 2581.59it/s]Loading:  48%|████▊     | 23419/49018 [00:09<00:09, 2586.61it/s]Loading:  48%|████▊     | 23678/49018 [00:09<00:09, 2577.27it/s]Loading:  49%|████▉     | 23938/49018 [00:09<00:09, 2582.29it/s]Loading:  49%|████▉     | 24197/49018 [00:09<00:09, 2581.68it/s]Loading:  50%|████▉     | 24457/49018 [00:09<00:09, 2586.95it/s]Loading:  50%|█████     | 24716/49018 [00:09<00:09, 2587.27it/s]Loading:  51%|█████     | 24976/49018 [00:09<00:09, 2590.35it/s]Loading:  51%|█████▏    | 25236/49018 [00:09<00:09, 2588.67it/s]Loading:  52%|█████▏    | 25495/49018 [00:10<00:09, 2587.02it/s]Loading:  53%|█████▎    | 25754/49018 [00:10<00:08, 2586.74it/s]Loading:  53%|█████▎    | 26014/49018 [00:10<00:08, 2589.62it/s]Loading:  54%|█████▎    | 26273/49018 [00:10<00:08, 2587.08it/s]Loading:  54%|█████▍    | 26533/49018 [00:10<00:08, 2589.26it/s]Loading:  55%|█████▍    | 26792/49018 [00:10<00:08, 2586.29it/s]Loading:  55%|█████▌    | 27052/49018 [00:10<00:08, 2587.51it/s]Loading:  56%|█████▌    | 27312/49018 [00:10<00:08, 2590.77it/s]Loading:  56%|█████▌    | 27572/49018 [00:10<00:08, 2589.15it/s]Loading:  57%|█████▋    | 27832/49018 [00:10<00:08, 2592.05it/s]Loading:  57%|█████▋    | 28092/49018 [00:11<00:08, 2586.23it/s]Loading:  58%|█████▊    | 28352/49018 [00:11<00:07, 2588.14it/s]Loading:  58%|█████▊    | 28611/49018 [00:11<00:07, 2587.87it/s]Loading:  59%|█████▉    | 28872/49018 [00:11<00:07, 2593.40it/s]Loading:  59%|█████▉    | 29133/49018 [00:11<00:07, 2596.65it/s]Loading:  60%|█████▉    | 29394/49018 [00:11<00:07, 2598.35it/s]Loading:  60%|██████    | 29654/49018 [00:11<00:07, 2597.59it/s]Loading:  61%|██████    | 29915/49018 [00:11<00:07, 2599.61it/s]Loading:  62%|██████▏   | 30175/49018 [00:11<00:07, 2591.27it/s]Loading:  62%|██████▏   | 30436/49018 [00:11<00:07, 2594.33it/s]Loading:  63%|██████▎   | 30696/49018 [00:12<00:07, 2586.73it/s]Loading:  63%|██████▎   | 30956/49018 [00:12<00:06, 2589.39it/s]Loading:  64%|██████▎   | 31216/49018 [00:12<00:06, 2591.72it/s]Loading:  64%|██████▍   | 31476/49018 [00:12<00:06, 2589.76it/s]Loading:  65%|██████▍   | 31737/49018 [00:12<00:06, 2594.22it/s]Loading:  65%|██████▌   | 31997/49018 [00:12<00:06, 2587.96it/s]Loading:  66%|██████▌   | 32257/49018 [00:12<00:06, 2590.35it/s]Loading:  66%|██████▋   | 32517/49018 [00:12<00:06, 2586.57it/s]Loading:  67%|██████▋   | 32776/49018 [00:12<00:06, 2560.83it/s]Loading:  67%|██████▋   | 33033/49018 [00:13<00:06, 2561.92it/s]Loading:  68%|██████▊   | 33292/49018 [00:13<00:06, 2568.94it/s]Loading:  68%|██████▊   | 33550/49018 [00:13<00:06, 2571.16it/s]Loading:  69%|██████▉   | 33811/49018 [00:13<00:05, 2581.39it/s]Loading:  70%|██████▉   | 34070/49018 [00:13<00:05, 2582.83it/s]Loading:  70%|███████   | 34331/49018 [00:13<00:05, 2588.08it/s]Loading:  71%|███████   | 34590/49018 [00:13<00:05, 2587.46it/s]Loading:  71%|███████   | 34851/49018 [00:13<00:05, 2593.11it/s]Loading:  72%|███████▏  | 35111/49018 [00:13<00:05, 2593.57it/s]Loading:  72%|███████▏  | 35371/49018 [00:13<00:05, 2594.10it/s]Loading:  73%|███████▎  | 35632/49018 [00:14<00:05, 2598.23it/s]Loading:  73%|███████▎  | 35892/49018 [00:14<00:05, 2594.71it/s]Loading:  74%|███████▍  | 36152/49018 [00:14<00:04, 2596.20it/s]Loading:  74%|███████▍  | 36412/49018 [00:14<00:04, 2595.55it/s]Loading:  75%|███████▍  | 36673/49018 [00:14<00:04, 2597.46it/s]Loading:  75%|███████▌  | 36933/49018 [00:14<00:04, 2592.29it/s]Loading:  76%|███████▌  | 37193/49018 [00:14<00:04, 2593.23it/s]Loading:  76%|███████▋  | 37453/49018 [00:14<00:04, 2588.54it/s]Loading:  77%|███████▋  | 37714/49018 [00:14<00:04, 2593.76it/s]Loading:  77%|███████▋  | 37974/49018 [00:14<00:04, 2591.25it/s]Loading:  78%|███████▊  | 38235/49018 [00:15<00:04, 2595.70it/s]Loading:  79%|███████▊  | 38495/49018 [00:15<00:04, 2590.31it/s]Loading:  79%|███████▉  | 38755/49018 [00:15<00:03, 2592.59it/s]Loading:  80%|███████▉  | 39015/49018 [00:15<00:03, 2592.39it/s]Loading:  80%|████████  | 39276/49018 [00:15<00:03, 2596.78it/s]Loading:  81%|████████  | 39536/49018 [00:15<00:03, 2593.76it/s]Loading:  81%|████████  | 39796/49018 [00:15<00:03, 2593.49it/s]Loading:  82%|████████▏ | 40057/49018 [00:15<00:03, 2596.97it/s]Loading:  82%|████████▏ | 40317/49018 [00:15<00:03, 2595.73it/s]Loading:  83%|████████▎ | 40577/49018 [00:15<00:03, 2595.73it/s]Loading:  83%|████████▎ | 40837/49018 [00:16<00:03, 2592.77it/s]Loading:  84%|████████▍ | 41097/49018 [00:16<00:03, 2593.55it/s]Loading:  84%|████████▍ | 41357/49018 [00:16<00:02, 2594.24it/s]Loading:  85%|████████▍ | 41617/49018 [00:16<00:02, 2595.69it/s]Loading:  85%|████████▌ | 41877/49018 [00:16<00:02, 2594.56it/s]Loading:  86%|████████▌ | 42137/49018 [00:16<00:02, 2559.91it/s]Loading:  86%|████████▋ | 42394/49018 [00:16<00:02, 2555.91it/s]Loading:  87%|████████▋ | 42654/49018 [00:16<00:02, 2568.99it/s]Loading:  88%|████████▊ | 42914/49018 [00:16<00:02, 2577.69it/s]Loading:  88%|████████▊ | 43174/49018 [00:16<00:02, 2584.23it/s]Loading:  89%|████████▊ | 43433/49018 [00:17<00:02, 2577.62it/s]Loading:  89%|████████▉ | 43693/49018 [00:17<00:02, 2583.34it/s]Loading:  90%|████████▉ | 43953/49018 [00:17<00:01, 2587.43it/s]Loading:  90%|█████████ | 44214/49018 [00:17<00:01, 2592.52it/s]Loading:  91%|█████████ | 44475/49018 [00:17<00:01, 2593.07it/s]Loading:  91%|█████████▏| 44735/49018 [00:17<00:01, 2594.35it/s]Loading:  92%|█████████▏| 44995/49018 [00:17<00:01, 2595.78it/s]Loading:  92%|█████████▏| 45255/49018 [00:17<00:01, 2595.83it/s]Loading:  93%|█████████▎| 45516/49018 [00:17<00:01, 2599.86it/s]Loading:  93%|█████████▎| 45776/49018 [00:18<00:02, 1140.54it/s]Loading:  94%|█████████▍| 46036/49018 [00:18<00:02, 1370.57it/s]Loading:  94%|█████████▍| 46295/49018 [00:18<00:01, 1594.79it/s]Loading:  95%|█████████▍| 46554/49018 [00:18<00:01, 1801.23it/s]Loading:  96%|█████████▌| 46813/49018 [00:18<00:01, 1981.32it/s]Loading:  96%|█████████▌| 47074/49018 [00:18<00:00, 2134.53it/s]Loading:  97%|█████████▋| 47334/49018 [00:18<00:00, 2255.06it/s]Loading:  97%|█████████▋| 47592/49018 [00:19<00:00, 2341.53it/s]Loading:  98%|█████████▊| 47851/49018 [00:19<00:00, 2410.37it/s]Loading:  98%|█████████▊| 48112/49018 [00:19<00:00, 2465.27it/s]Loading:  99%|█████████▊| 48372/49018 [00:19<00:00, 2502.13it/s]Loading:  99%|█████████▉| 48633/49018 [00:19<00:00, 2531.03it/s]Loading: 100%|█████████▉| 48894/49018 [00:19<00:00, 2552.87it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2500.46it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 268/49018 [00:00<00:18, 2675.09it/s]Loading:   1%|          | 540/49018 [00:00<00:17, 2699.74it/s]Loading:   2%|▏         | 812/49018 [00:00<00:17, 2706.10it/s]Loading:   2%|▏         | 1083/49018 [00:00<00:17, 2706.53it/s]Loading:   3%|▎         | 1355/49018 [00:00<00:17, 2708.34it/s]Loading:   3%|▎         | 1626/49018 [00:00<00:17, 2706.17it/s]Loading:   4%|▍         | 1897/49018 [00:01<00:42, 1101.41it/s]Loading:   4%|▍         | 2166/49018 [00:01<00:34, 1354.49it/s]Loading:   5%|▍         | 2436/49018 [00:01<00:29, 1604.05it/s]Loading:   6%|▌         | 2707/49018 [00:01<00:25, 1835.32it/s]Loading:   6%|▌         | 2977/49018 [00:01<00:22, 2033.40it/s]Loading:   7%|▋         | 3245/49018 [00:01<00:20, 2191.66it/s]Loading:   7%|▋         | 3515/49018 [00:01<00:19, 2323.09it/s]Loading:   8%|▊         | 3784/49018 [00:01<00:18, 2421.98it/s]Loading:   8%|▊         | 4053/49018 [00:01<00:18, 2496.19it/s]Loading:   9%|▉         | 4323/49018 [00:02<00:17, 2552.16it/s]Loading:   9%|▉         | 4591/49018 [00:02<00:17, 2588.21it/s]Loading:  10%|▉         | 4858/49018 [00:02<00:16, 2607.75it/s]Loading:  10%|█         | 5127/49018 [00:02<00:16, 2630.73it/s]Loading:  11%|█         | 5395/49018 [00:02<00:16, 2642.05it/s]Loading:  12%|█▏        | 5664/49018 [00:02<00:16, 2654.75it/s]Loading:  12%|█▏        | 5932/49018 [00:02<00:16, 2659.78it/s]Loading:  13%|█▎        | 6200/49018 [00:02<00:16, 2653.02it/s]Loading:  13%|█▎        | 6468/49018 [00:02<00:15, 2659.59it/s]Loading:  14%|█▎        | 6738/49018 [00:02<00:15, 2669.50it/s]Loading:  14%|█▍        | 7006/49018 [00:03<00:15, 2669.63it/s]Loading:  15%|█▍        | 7274/49018 [00:03<00:15, 2670.84it/s]Loading:  15%|█▌        | 7542/49018 [00:03<00:15, 2666.64it/s]Loading:  16%|█▌        | 7812/49018 [00:03<00:15, 2673.72it/s]Loading:  16%|█▋        | 8080/49018 [00:03<00:15, 2673.40it/s]Loading:  17%|█▋        | 8348/49018 [00:03<00:15, 2674.40it/s]Loading:  18%|█▊        | 8616/49018 [00:03<00:15, 2667.28it/s]Loading:  18%|█▊        | 8884/49018 [00:03<00:15, 2670.46it/s]Loading:  19%|█▊        | 9152/49018 [00:03<00:14, 2672.12it/s]Loading:  19%|█▉        | 9421/49018 [00:03<00:14, 2676.10it/s]Loading:  20%|█▉        | 9689/49018 [00:04<00:14, 2674.85it/s]Loading:  20%|██        | 9957/49018 [00:04<00:14, 2675.65it/s]Loading:  21%|██        | 10225/49018 [00:04<00:14, 2669.19it/s]Loading:  21%|██▏       | 10494/49018 [00:04<00:14, 2673.52it/s]Loading:  22%|██▏       | 10762/49018 [00:04<00:14, 2675.22it/s]Loading:  23%|██▎       | 11031/49018 [00:04<00:14, 2678.06it/s]Loading:  23%|██▎       | 11299/49018 [00:04<00:14, 2677.27it/s]Loading:  24%|██▎       | 11567/49018 [00:04<00:14, 2673.28it/s]Loading:  24%|██▍       | 11835/49018 [00:04<00:13, 2674.09it/s]Loading:  25%|██▍       | 12104/49018 [00:04<00:13, 2676.12it/s]Loading:  25%|██▌       | 12372/49018 [00:05<00:13, 2669.18it/s]Loading:  26%|██▌       | 12640/49018 [00:05<00:13, 2671.77it/s]Loading:  26%|██▋       | 12908/49018 [00:05<00:13, 2668.41it/s]Loading:  27%|██▋       | 13177/49018 [00:05<00:13, 2674.17it/s]Loading:  27%|██▋       | 13446/49018 [00:05<00:13, 2678.33it/s]Loading:  28%|██▊       | 13714/49018 [00:05<00:13, 2674.60it/s]Loading:  29%|██▊       | 13983/49018 [00:05<00:13, 2677.69it/s]Loading:  29%|██▉       | 14251/49018 [00:05<00:13, 2671.18it/s]Loading:  30%|██▉       | 14519/49018 [00:05<00:13, 2643.43it/s]Loading:  30%|███       | 14787/49018 [00:05<00:12, 2652.71it/s]Loading:  31%|███       | 15055/49018 [00:06<00:12, 2659.29it/s]Loading:  31%|███▏      | 15323/49018 [00:06<00:12, 2665.31it/s]Loading:  32%|███▏      | 15591/49018 [00:06<00:12, 2669.50it/s]Loading:  32%|███▏      | 15858/49018 [00:06<00:12, 2667.94it/s]Loading:  33%|███▎      | 16127/49018 [00:06<00:12, 2671.85it/s]Loading:  33%|███▎      | 16395/49018 [00:06<00:12, 2659.05it/s]Loading:  34%|███▍      | 16663/49018 [00:06<00:12, 2664.05it/s]Loading:  35%|███▍      | 16930/49018 [00:06<00:12, 2657.77it/s]Loading:  35%|███▌      | 17198/49018 [00:06<00:11, 2662.50it/s]Loading:  36%|███▌      | 17465/49018 [00:06<00:11, 2658.02it/s]Loading:  36%|███▌      | 17732/49018 [00:07<00:11, 2658.92it/s]Loading:  37%|███▋      | 17998/49018 [00:07<00:11, 2657.81it/s]Loading:  37%|███▋      | 18264/49018 [00:07<00:11, 2639.53it/s]Loading:  38%|███▊      | 18528/49018 [00:07<00:11, 2623.37it/s]Loading:  38%|███▊      | 18791/49018 [00:07<00:11, 2616.86it/s]Loading:  39%|███▉      | 19053/49018 [00:07<00:11, 2605.33it/s]Loading:  39%|███▉      | 19314/49018 [00:07<00:11, 2603.98it/s]Loading:  40%|███▉      | 19575/49018 [00:07<00:11, 2597.59it/s]Loading:  40%|████      | 19836/49018 [00:07<00:11, 2599.51it/s]Loading:  41%|████      | 20096/49018 [00:07<00:11, 2593.84it/s]Loading:  42%|████▏     | 20356/49018 [00:08<00:11, 2593.05it/s]Loading:  42%|████▏     | 20616/49018 [00:08<00:10, 2590.41it/s]Loading:  43%|████▎     | 20876/49018 [00:08<00:10, 2593.11it/s]Loading:  43%|████▎     | 21136/49018 [00:08<00:10, 2595.04it/s]Loading:  44%|████▎     | 21396/49018 [00:08<00:10, 2596.48it/s]Loading:  44%|████▍     | 21656/49018 [00:08<00:10, 2594.39it/s]Loading:  45%|████▍     | 21916/49018 [00:08<00:10, 2591.58it/s]Loading:  45%|████▌     | 22176/49018 [00:08<00:10, 2593.57it/s]Loading:  46%|████▌     | 22436/49018 [00:08<00:10, 2591.02it/s]Loading:  46%|████▋     | 22697/49018 [00:08<00:10, 2594.50it/s]Loading:  47%|████▋     | 22957/49018 [00:09<00:10, 2588.07it/s]Loading:  47%|████▋     | 23217/49018 [00:09<00:09, 2590.99it/s]Loading:  48%|████▊     | 23477/49018 [00:09<00:09, 2587.33it/s]Loading:  48%|████▊     | 23736/49018 [00:09<00:09, 2586.35it/s]Loading:  49%|████▉     | 23996/49018 [00:09<00:09, 2587.35it/s]Loading:  49%|████▉     | 24255/49018 [00:09<00:09, 2582.12it/s]Loading:  50%|█████     | 24514/49018 [00:09<00:09, 2583.68it/s]Loading:  51%|█████     | 24774/49018 [00:09<00:09, 2586.75it/s]Loading:  51%|█████     | 25033/49018 [00:09<00:09, 2583.21it/s]Loading:  52%|█████▏    | 25295/49018 [00:09<00:09, 2591.41it/s]Loading:  52%|█████▏    | 25555/49018 [00:10<00:09, 2590.64it/s]Loading:  53%|█████▎    | 25815/49018 [00:10<00:08, 2590.45it/s]Loading:  53%|█████▎    | 26076/49018 [00:10<00:08, 2595.93it/s]Loading:  54%|█████▎    | 26336/49018 [00:10<00:08, 2591.69it/s]Loading:  54%|█████▍    | 26596/49018 [00:10<00:08, 2591.76it/s]Loading:  55%|█████▍    | 26856/49018 [00:10<00:08, 2582.79it/s]Loading:  55%|█████▌    | 27116/49018 [00:10<00:08, 2586.38it/s]Loading:  56%|█████▌    | 27377/49018 [00:10<00:08, 2590.45it/s]Loading:  56%|█████▋    | 27637/49018 [00:10<00:08, 2589.82it/s]Loading:  57%|█████▋    | 27897/49018 [00:10<00:08, 2590.67it/s]Loading:  57%|█████▋    | 28157/49018 [00:11<00:08, 2589.05it/s]Loading:  58%|█████▊    | 28417/49018 [00:11<00:07, 2590.10it/s]Loading:  59%|█████▊    | 28677/49018 [00:11<00:07, 2592.56it/s]Loading:  59%|█████▉    | 28937/49018 [00:11<00:07, 2591.55it/s]Loading:  60%|█████▉    | 29197/49018 [00:11<00:07, 2592.42it/s]Loading:  60%|██████    | 29457/49018 [00:11<00:07, 2591.02it/s]Loading:  61%|██████    | 29717/49018 [00:11<00:07, 2590.40it/s]Loading:  61%|██████    | 29979/49018 [00:11<00:07, 2596.44it/s]Loading:  62%|██████▏   | 30239/49018 [00:11<00:07, 2591.59it/s]Loading:  62%|██████▏   | 30499/49018 [00:11<00:07, 2590.33it/s]Loading:  63%|██████▎   | 30759/49018 [00:12<00:07, 2589.17it/s]Loading:  63%|██████▎   | 31020/49018 [00:12<00:06, 2594.95it/s]Loading:  64%|██████▍   | 31280/49018 [00:12<00:06, 2593.61it/s]Loading:  64%|██████▍   | 31540/49018 [00:12<00:06, 2595.30it/s]Loading:  65%|██████▍   | 31800/49018 [00:12<00:06, 2591.81it/s]Loading:  65%|██████▌   | 32060/49018 [00:12<00:06, 2593.35it/s]Loading:  66%|██████▌   | 32320/49018 [00:12<00:06, 2588.92it/s]Loading:  66%|██████▋   | 32581/49018 [00:12<00:06, 2593.58it/s]Loading:  67%|██████▋   | 32841/49018 [00:12<00:06, 2590.41it/s]Loading:  68%|██████▊   | 33101/49018 [00:13<00:06, 2592.34it/s]Loading:  68%|██████▊   | 33361/49018 [00:13<00:06, 2588.26it/s]Loading:  69%|██████▊   | 33620/49018 [00:13<00:05, 2584.03it/s]Loading:  69%|██████▉   | 33879/49018 [00:13<00:05, 2582.83it/s]Loading:  70%|██████▉   | 34140/49018 [00:13<00:05, 2588.40it/s]Loading:  70%|███████   | 34399/49018 [00:13<00:05, 2588.10it/s]Loading:  71%|███████   | 34658/49018 [00:13<00:05, 2584.72it/s]Loading:  71%|███████   | 34918/49018 [00:13<00:05, 2587.97it/s]Loading:  72%|███████▏  | 35177/49018 [00:13<00:05, 2587.60it/s]Loading:  72%|███████▏  | 35437/49018 [00:13<00:05, 2590.85it/s]Loading:  73%|███████▎  | 35697/49018 [00:14<00:05, 2588.03it/s]Loading:  73%|███████▎  | 35957/49018 [00:14<00:05, 2591.51it/s]Loading:  74%|███████▍  | 36217/49018 [00:14<00:04, 2590.00it/s]Loading:  74%|███████▍  | 36477/49018 [00:14<00:04, 2589.84it/s]Loading:  75%|███████▍  | 36736/49018 [00:14<00:04, 2589.74it/s]Loading:  75%|███████▌  | 36995/49018 [00:14<00:04, 2585.36it/s]Loading:  76%|███████▌  | 37254/49018 [00:14<00:04, 2582.21it/s]Loading:  77%|███████▋  | 37514/49018 [00:14<00:04, 2585.76it/s]Loading:  77%|███████▋  | 37773/49018 [00:15<00:11, 981.75it/s] Loading:  78%|███████▊  | 38031/49018 [00:15<00:09, 1204.43it/s]Loading:  78%|███████▊  | 38290/49018 [00:15<00:07, 1433.91it/s]Loading:  79%|███████▊  | 38547/49018 [00:15<00:06, 1651.12it/s]Loading:  79%|███████▉  | 38807/49018 [00:15<00:05, 1853.86it/s]Loading:  80%|███████▉  | 39065/49018 [00:15<00:04, 2022.79it/s]Loading:  80%|████████  | 39324/49018 [00:15<00:04, 2164.08it/s]Loading:  81%|████████  | 39579/49018 [00:16<00:04, 2263.89it/s]Loading:  81%|████████▏ | 39836/49018 [00:16<00:03, 2347.50it/s]Loading:  82%|████████▏ | 40094/49018 [00:16<00:03, 2412.03it/s]Loading:  82%|████████▏ | 40352/49018 [00:16<00:03, 2458.93it/s]Loading:  83%|████████▎ | 40610/49018 [00:16<00:03, 2493.75it/s]Loading:  83%|████████▎ | 40867/49018 [00:16<00:03, 2515.85it/s]Loading:  84%|████████▍ | 41124/49018 [00:16<00:03, 2531.32it/s]Loading:  84%|████████▍ | 41384/49018 [00:16<00:02, 2549.27it/s]Loading:  85%|████████▍ | 41643/49018 [00:16<00:02, 2559.61it/s]Loading:  85%|████████▌ | 41901/49018 [00:16<00:02, 2564.97it/s]Loading:  86%|████████▌ | 42160/49018 [00:17<00:02, 2569.61it/s]Loading:  87%|████████▋ | 42418/49018 [00:17<00:02, 2567.91it/s]Loading:  87%|████████▋ | 42677/49018 [00:17<00:02, 2574.41it/s]Loading:  88%|████████▊ | 42935/49018 [00:17<00:02, 2573.30it/s]Loading:  88%|████████▊ | 43194/49018 [00:17<00:02, 2576.05it/s]Loading:  89%|████████▊ | 43452/49018 [00:17<00:02, 2572.50it/s]Loading:  89%|████████▉ | 43710/49018 [00:17<00:02, 2574.07it/s]Loading:  90%|████████▉ | 43970/49018 [00:17<00:01, 2579.84it/s]Loading:  90%|█████████ | 44230/49018 [00:17<00:01, 2582.67it/s]Loading:  91%|█████████ | 44489/49018 [00:17<00:01, 2577.97it/s]Loading:  91%|█████████▏| 44747/49018 [00:18<00:01, 2574.70it/s]Loading:  92%|█████████▏| 45005/49018 [00:18<00:01, 2574.34it/s]Loading:  92%|█████████▏| 45264/49018 [00:18<00:01, 2577.74it/s]Loading:  93%|█████████▎| 45522/49018 [00:18<00:01, 2578.17it/s]Loading:  93%|█████████▎| 45781/49018 [00:18<00:01, 2581.04it/s]Loading:  94%|█████████▍| 46040/49018 [00:18<00:01, 2575.67it/s]Loading:  94%|█████████▍| 46298/49018 [00:18<00:01, 2576.18it/s]Loading:  95%|█████████▍| 46558/49018 [00:18<00:00, 2582.56it/s]Loading:  96%|█████████▌| 46817/49018 [00:18<00:00, 2581.08it/s]Loading:  96%|█████████▌| 47077/49018 [00:18<00:00, 2585.20it/s]Loading:  97%|█████████▋| 47336/49018 [00:19<00:00, 2574.70it/s]Loading:  97%|█████████▋| 47595/49018 [00:19<00:00, 2578.39it/s]Loading:  98%|█████████▊| 47854/49018 [00:19<00:00, 2579.38it/s]Loading:  98%|█████████▊| 48113/49018 [00:19<00:00, 2579.71it/s]Loading:  99%|█████████▊| 48371/49018 [00:19<00:00, 2579.30it/s]Loading:  99%|█████████▉| 48630/49018 [00:19<00:00, 2580.11it/s]Loading: 100%|█████████▉| 48889/49018 [00:19<00:00, 2576.13it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2485.84it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank33]:[W424 17:48:41.229299295 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 17:48:41.229298072 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 17:48:41.229291600 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 17:48:41.229288344 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 17:48:41.229286360 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 17:48:41.229295027 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 17:48:41.229283044 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 17:48:41.229448307 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 17:48:41.357044316 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 17:48:41.357058763 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 17:48:41.357051018 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 17:48:41.357041881 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 17:48:41.357072469 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 17:48:41.357080514 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 17:48:41.357085273 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 17:48:41.537156459 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 17:48:41.540081840 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 17:48:41.541625565 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 17:48:41.542161183 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 17:48:42.403841232 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 17:48:42.403831223 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 17:48:42.403836934 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 17:48:42.403833487 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 17:48:42.403835180 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 17:48:42.403828307 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 17:48:42.900903838 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 17:48:42.900899260 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 17:48:42.900895232 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 17:48:42.900905511 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 17:48:42.900901864 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 17:48:42.900911703 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 17:48:42.462600676 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 17:48:42.354561517 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 17:48:42.354540147 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 17:48:42.733371140 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 17:48:42.733417859 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 17:48:42.733509824 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 17:48:43.678390752 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 17:48:43.939563491 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 17:48:43.845525655 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:25<21:08, 25.88s/it]Train:   4%|▍         | 2/50 [00:26<08:38, 10.81s/it]Train:   6%|▌         | 3/50 [00:26<04:40,  5.97s/it]Train:   8%|▊         | 4/50 [00:26<02:49,  3.69s/it]Train:  10%|█         | 5/50 [00:26<01:49,  2.43s/it]Train:  12%|█▏        | 6/50 [00:26<01:13,  1.68s/it]Train:  14%|█▍        | 7/50 [00:27<00:51,  1.19s/it]Train:  16%|█▌        | 8/50 [00:27<00:36,  1.14it/s]Train:  18%|█▊        | 9/50 [00:27<00:27,  1.50it/s]Train:  20%|██        | 10/50 [00:27<00:20,  1.91it/s]Train:  22%|██▏       | 11/50 [00:27<00:16,  2.35it/s]Train:  24%|██▍       | 12/50 [00:28<00:13,  2.79it/s]Train:  26%|██▌       | 13/50 [00:28<00:11,  3.21it/s]Train:  28%|██▊       | 14/50 [00:28<00:10,  3.59it/s]Train:  30%|███       | 15/50 [00:28<00:08,  3.92it/s]Train:  32%|███▏      | 16/50 [00:28<00:08,  4.18it/s]Train:  34%|███▍      | 17/50 [00:29<00:07,  4.37it/s]Train:  36%|███▌      | 18/50 [00:29<00:07,  4.55it/s]Train:  38%|███▊      | 19/50 [00:29<00:06,  4.66it/s]Train:  40%|████      | 20/50 [00:29<00:06,  4.74it/s]Train:  42%|████▏     | 21/50 [00:29<00:06,  4.80it/s]Train:  44%|████▍     | 22/50 [00:30<00:05,  4.85it/s]Train:  46%|████▌     | 23/50 [00:30<00:05,  4.87it/s]Train:  48%|████▊     | 24/50 [00:30<00:05,  4.86it/s]Train:  50%|█████     | 25/50 [00:30<00:05,  4.89it/s]Train:  52%|█████▏    | 26/50 [00:31<00:04,  4.89it/s]Train:  54%|█████▍    | 27/50 [00:31<00:04,  4.91it/s]Train:  56%|█████▌    | 28/50 [00:31<00:04,  4.93it/s]Train:  58%|█████▊    | 29/50 [00:31<00:04,  4.93it/s]Train:  60%|██████    | 30/50 [00:31<00:04,  4.95it/s]Train:  62%|██████▏   | 31/50 [00:32<00:03,  4.95it/s]Train:  64%|██████▍   | 32/50 [00:32<00:03,  4.96it/s]Train:  66%|██████▌   | 33/50 [00:32<00:03,  4.96it/s]Train:  68%|██████▊   | 34/50 [00:32<00:03,  4.95it/s]Train:  70%|███████   | 35/50 [00:32<00:03,  4.95it/s]Train:  72%|███████▏  | 36/50 [00:33<00:02,  4.95it/s]Train:  74%|███████▍  | 37/50 [00:33<00:02,  4.96it/s]Train:  76%|███████▌  | 38/50 [00:33<00:02,  4.96it/s]Train:  78%|███████▊  | 39/50 [00:33<00:02,  4.97it/s]Train:  80%|████████  | 40/50 [00:33<00:02,  4.98it/s]Train:  82%|████████▏ | 41/50 [00:34<00:01,  4.97it/s]Train:  84%|████████▍ | 42/50 [00:34<00:01,  4.96it/s]Train:  86%|████████▌ | 43/50 [00:34<00:01,  4.92it/s]Train:  88%|████████▊ | 44/50 [00:34<00:01,  4.93it/s]Train:  90%|█████████ | 45/50 [00:34<00:01,  4.93it/s]Train:  92%|█████████▏| 46/50 [00:35<00:00,  4.95it/s]Train:  94%|█████████▍| 47/50 [00:35<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:35<00:00,  4.98it/s]Train:  98%|█████████▊| 49/50 [00:35<00:00,  4.94it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  4.95it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  1.39it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:22,  2.20it/s]Train:   4%|▍         | 2/50 [00:00<00:14,  3.26it/s]Train:   6%|▌         | 3/50 [00:00<00:12,  3.85it/s]Train:   8%|▊         | 4/50 [00:01<00:10,  4.24it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.49it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.64it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.73it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.81it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.87it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.89it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.91it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.92it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.95it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.95it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.95it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.95it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.96it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.94it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.95it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.96it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.97it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.95it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.94it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.94it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.94it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.95it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.94it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.92it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.93it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.94it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.96it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.95it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.97it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.97it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.96it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.97it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.97it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.98it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.99it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.97it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.91it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.94it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.94it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.96it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.98it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.97it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.98it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.99it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.99it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.82it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.43it/s]Train:   4%|▍         | 2/50 [00:00<00:11,  4.19it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.52it/s]Train:   8%|▊         | 4/50 [00:00<00:09,  4.66it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.77it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.80it/s]Train:  14%|█▍        | 7/50 [00:01<00:08,  4.86it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.89it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.92it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.92it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.91it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.92it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.94it/s]Train:  28%|██▊       | 14/50 [00:02<00:07,  4.95it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.94it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.95it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.96it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.96it/s]Train:  38%|███▊      | 19/50 [00:03<00:06,  4.98it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.95it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.94it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.94it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.96it/s]Train:  48%|████▊     | 24/50 [00:04<00:05,  4.98it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.97it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.93it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.94it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.95it/s]Train:  58%|█████▊    | 29/50 [00:05<00:04,  4.97it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.94it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.94it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.95it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.95it/s]Train:  68%|██████▊   | 34/50 [00:06<00:03,  4.96it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.96it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.94it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.96it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.97it/s]Train:  78%|███████▊  | 39/50 [00:07<00:02,  4.96it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.97it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.98it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.97it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.98it/s]Train:  88%|████████▊ | 44/50 [00:08<00:01,  4.97it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.98it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.96it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.96it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.97it/s]Train:  98%|█████████▊| 49/50 [00:09<00:00,  4.97it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.97it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.90it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.44it/s]Train:   4%|▍         | 2/50 [00:00<00:11,  4.19it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.50it/s]Train:   8%|▊         | 4/50 [00:00<00:09,  4.67it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.78it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.84it/s]Train:  14%|█▍        | 7/50 [00:01<00:08,  4.88it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.90it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.93it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.95it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.95it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.95it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.96it/s]Train:  28%|██▊       | 14/50 [00:02<00:07,  4.93it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.96it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.96it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.96it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.97it/s]Train:  38%|███▊      | 19/50 [00:03<00:06,  4.95it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.94it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.95it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.95it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.95it/s]Train:  48%|████▊     | 24/50 [00:04<00:05,  4.95it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.94it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.95it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.95it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.95it/s]Train:  58%|█████▊    | 29/50 [00:05<00:04,  4.96it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.95it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.96it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.97it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.99it/s]Train:  68%|██████▊   | 34/50 [00:06<00:03,  5.00it/s]Train:  70%|███████   | 35/50 [00:07<00:02,  5.01it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  5.01it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.99it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.99it/s]Train:  78%|███████▊  | 39/50 [00:07<00:02,  4.95it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.94it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.95it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.95it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.97it/s]Train:  88%|████████▊ | 44/50 [00:08<00:01,  4.96it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.93it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.93it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.94it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.94it/s]Train:  98%|█████████▊| 49/50 [00:09<00:00,  4.94it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.96it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.90it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.45it/s]Train:   4%|▍         | 2/50 [00:00<00:11,  4.23it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.55it/s]Train:   8%|▊         | 4/50 [00:00<00:09,  4.69it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.77it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.82it/s]Train:  14%|█▍        | 7/50 [00:01<00:08,  4.86it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.89it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.91it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.92it/s]Train:  22%|██▏       | 11/50 [00:02<00:07,  4.93it/s]Train:  24%|██▍       | 12/50 [00:02<00:07,  4.93it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.93it/s]Train:  28%|██▊       | 14/50 [00:02<00:07,  4.93it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.94it/s]Train:  32%|███▏      | 16/50 [00:03<00:06,  4.94it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.94it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.96it/s]Train:  38%|███▊      | 19/50 [00:03<00:06,  4.97it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.98it/s]Train:  42%|████▏     | 21/50 [00:04<00:05,  4.97it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.96it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.95it/s]Train:  48%|████▊     | 24/50 [00:04<00:05,  4.97it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.97it/s]Train:  52%|█████▏    | 26/50 [00:05<00:04,  4.97it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.97it/s]Train:  56%|█████▌    | 28/50 [00:05<00:04,  4.97it/s]Train:  58%|█████▊    | 29/50 [00:05<00:04,  4.97it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.97it/s]Train:  62%|██████▏   | 31/50 [00:06<00:03,  4.98it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.97it/s]Train:  66%|██████▌   | 33/50 [00:06<00:03,  4.98it/s]Train:  68%|██████▊   | 34/50 [00:06<00:03,  4.99it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.99it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.97it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.96it/s]Train:  76%|███████▌  | 38/50 [00:07<00:02,  4.92it/s]Train:  78%|███████▊  | 39/50 [00:07<00:02,  4.94it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.93it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.94it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.96it/s]Train:  86%|████████▌ | 43/50 [00:08<00:01,  4.98it/s]Train:  88%|████████▊ | 44/50 [00:08<00:01,  4.97it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.99it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.97it/s]Train:  94%|█████████▍| 47/50 [00:09<00:00,  4.98it/s]Train:  96%|█████████▌| 48/50 [00:09<00:00,  4.96it/s]Train:  98%|█████████▊| 49/50 [00:09<00:00,  4.95it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.96it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.90it/s]
0: Process 0 - Local timer:  load_data  :  86.16
0: Process 0 - Local timer:  train_validate_test  :  77.04
0: Process 0 - Local timer:  create_model  :  1.17
0: Minimum timers: 
0: load_data  :  86.16
0: train_validate_test  :  76.98
0: create_model  :  1.16
0: Maximum timers: 
0: load_data  :  87.86
0: train_validate_test  :  77.04
0: create_model  :  1.24
0: Average timers: 
0: load_data  :  87.45
0: train_validate_test  :  77.0
0: create_model  :  1.19
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 17:49:36.534323914 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 17:49:37.122909847 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 17:49:37.123119144 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 17:49:37.123042058 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 17:49:37.123605155 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 17:49:37.123825984 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 17:49:37.681026425 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 17:49:37.681041073 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 17:49:37.681029611 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 17:49:37.681087110 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 17:49:37.681077041 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 17:49:37.681084695 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 17:49:37.681255379 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 17:49:37.681268985 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 17:49:37.124322175 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 17:49:37.124471588 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 17:49:37.013692793 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 17:49:37.013669639 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 17:49:37.013698363 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 17:49:37.725969407 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 17:49:37.013685078 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 17:49:37.725946664 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 17:49:37.013690969 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 17:49:37.725941534 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 17:49:37.013673907 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 17:49:37.725958045 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 17:49:37.013698834 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 17:49:37.725948317 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 17:49:37.725950781 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 17:49:37.385289777 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 17:49:37.726096688 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 17:49:37.385266814 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 17:49:37.726086498 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 17:49:37.385269569 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 17:49:37.385311238 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 17:49:37.385292022 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 17:49:37.386001260 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 17:49:37.386750955 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 17:49:37.387156376 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 17:49:37.016851159 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 05:49:40 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 05:49:40 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_4 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
srun: Step created for StepId=3393901.5
[W424 17:49:52.714298750 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714353634 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714356420 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714419880 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714494932 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714510332 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.714537964 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:52.715736118 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585477680 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585821621 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.873598682 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585874922 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.873689033 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585917583 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585931729 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585940145 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.873831273 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.585942841 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.873928036 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.245270759 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.245315274 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.245380768 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.245407970 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.245444449 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.874984949 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.587344915 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.875233681 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.875425695 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.246795087 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.876676706 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.248023804 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:53.249244695 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 17:49:54.518909149 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.519227192 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.519283700 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.519297686 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.519303728 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.520700977 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.520954728 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:49:54.535739011 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_4 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
0: Read attr time (sec):  0.0011539459228515625
0: read and bcast: trainset/x/variable_count 0.1716771125793457
0: read and bcast: trainset/x/variable_offset 0.3458068370819092
0: read and bcast: trainset/x/variable_dim 0.34618330001831055
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5516583919525146
0: read and bcast: trainset/edge_index/variable_offset 0.7304980754852295
0: read and bcast: trainset/edge_index/variable_dim 0.7403168678283691
0: read and bcast: trainset/edge_attr/variable_count 0.9160797595977783
0: read and bcast: trainset/edge_attr/variable_offset 1.093834400177002
0: read and bcast: trainset/edge_attr/variable_dim 1.1032309532165527
0: read and bcast: trainset/pos/variable_count 1.2832000255584717
0: read and bcast: trainset/pos/variable_offset 1.4630987644195557
0: read and bcast: trainset/pos/variable_dim 1.4731106758117676
0: read and bcast: trainset/energy/variable_count 1.6489875316619873
0: read and bcast: trainset/energy/variable_offset 1.8274643421173096
0: read and bcast: trainset/energy/variable_dim 1.838622808456421
0: read and bcast: trainset/forces/variable_count 2.0184125900268555
0: read and bcast: trainset/forces/variable_offset 2.201343059539795
0: read and bcast: trainset/forces/variable_dim 2.210604667663574
0: read and bcast: trainset/y/variable_count 2.382751226425171
0: read and bcast: trainset/y/variable_offset 2.5596542358398438
0: read and bcast: trainset/y/variable_dim 2.5700411796569824
0: Overall time (sec):  2.571403980255127
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.5759549140930176
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007035732269287109
0: read and bcast: valset/x/variable_count 0.007708311080932617
0: read and bcast: valset/x/variable_offset 0.014766693115234375
0: read and bcast: valset/x/variable_dim 0.014922380447387695
0: read and bcast: valset/edge_index/variable_count 0.0219423770904541
0: read and bcast: valset/edge_index/variable_offset 0.0291290283203125
0: read and bcast: valset/edge_index/variable_dim 0.02930593490600586
0: read and bcast: valset/edge_attr/variable_count 0.0365750789642334
0: read and bcast: valset/edge_attr/variable_offset 0.04373431205749512
0: read and bcast: valset/edge_attr/variable_dim 0.043892621994018555
0: read and bcast: valset/pos/variable_count 0.05096149444580078
0: read and bcast: valset/pos/variable_offset 0.058099985122680664
0: read and bcast: valset/pos/variable_dim 0.05827522277832031
0: read and bcast: valset/energy/variable_count 0.06520938873291016
0: read and bcast: valset/energy/variable_offset 0.07233381271362305
0: read and bcast: valset/energy/variable_dim 0.07249760627746582
0: read and bcast: valset/forces/variable_count 0.07937240600585938
0: read and bcast: valset/forces/variable_offset 0.08628439903259277
0: read and bcast: valset/forces/variable_dim 0.08643484115600586
0: read and bcast: valset/y/variable_count 0.09328651428222656
0: read and bcast: valset/y/variable_offset 0.10028505325317383
0: read and bcast: valset/y/variable_dim 0.10046577453613281
0: Overall time (sec):  0.10134625434875488
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10416841506958008
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007348060607910156
0: read and bcast: testset/x/variable_count 0.007253885269165039
0: read and bcast: testset/x/variable_offset 0.014093399047851562
0: read and bcast: testset/x/variable_dim 0.014249563217163086
0: read and bcast: testset/edge_index/variable_count 0.021366119384765625
0: read and bcast: testset/edge_index/variable_offset 0.02817249298095703
0: read and bcast: testset/edge_index/variable_dim 0.028342247009277344
0: read and bcast: testset/edge_attr/variable_count 0.03523111343383789
0: read and bcast: testset/edge_attr/variable_offset 0.04225468635559082
0: read and bcast: testset/edge_attr/variable_dim 0.04242086410522461
0: read and bcast: testset/pos/variable_count 0.04936814308166504
0: read and bcast: testset/pos/variable_offset 0.05630826950073242
0: read and bcast: testset/pos/variable_dim 0.056474924087524414
0: read and bcast: testset/energy/variable_count 0.06326556205749512
0: read and bcast: testset/energy/variable_offset 0.07005977630615234
0: read and bcast: testset/energy/variable_dim 0.07021045684814453
0: read and bcast: testset/forces/variable_count 0.07721257209777832
0: read and bcast: testset/forces/variable_offset 0.08409237861633301
0: read and bcast: testset/forces/variable_dim 0.08424830436706543
0: read and bcast: testset/y/variable_count 0.0910341739654541
0: read and bcast: testset/y/variable_offset 0.09795689582824707
0: read and bcast: testset/y/variable_dim 0.09812140464782715
0: Overall time (sec):  0.0990447998046875
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10187983512878418
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
10 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
34 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
25 alexandria nsplit: 9705384 11 882307
26 alexandria nsplit: 9705384 11 882307
27 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
24 alexandria nsplit: 9705384 11 882308
19 alexandria nsplit: 9705384 11 882308
20 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
0: Adios reading time (sec):  0.46166443824768066
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.12945079803466797
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.12475109100341797
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 233/64000 [00:00<00:27, 2321.40it/s]Loading:   1%|          | 484/64000 [00:00<00:26, 2426.76it/s]Loading:   1%|          | 750/64000 [00:00<00:24, 2532.17it/s]Loading:   2%|▏         | 1018/64000 [00:00<00:24, 2586.14it/s]Loading:   2%|▏         | 1283/64000 [00:00<00:24, 2608.29it/s]Loading:   2%|▏         | 1544/64000 [00:00<00:24, 2594.33it/s]Loading:   3%|▎         | 1804/64000 [00:00<00:24, 2573.98it/s]Loading:   3%|▎         | 2062/64000 [00:00<00:24, 2570.05it/s]Loading:   4%|▎         | 2320/64000 [00:00<00:24, 2568.29it/s]Loading:   4%|▍         | 2578/64000 [00:01<00:23, 2570.72it/s]Loading:   4%|▍         | 2836/64000 [00:01<00:23, 2567.88it/s]Loading:   5%|▍         | 3093/64000 [00:01<00:23, 2566.61it/s]Loading:   5%|▌         | 3350/64000 [00:01<00:23, 2563.10it/s]Loading:   6%|▌         | 3608/64000 [00:01<00:23, 2567.74it/s]Loading:   6%|▌         | 3865/64000 [00:01<00:34, 1746.18it/s]Loading:   6%|▋         | 4120/64000 [00:01<00:31, 1926.75it/s]Loading:   7%|▋         | 4375/64000 [00:01<00:28, 2078.68it/s]Loading:   7%|▋         | 4630/64000 [00:01<00:26, 2200.12it/s]Loading:   8%|▊         | 4889/64000 [00:02<00:25, 2303.54it/s]Loading:   8%|▊         | 5147/64000 [00:02<00:24, 2378.06it/s]Loading:   8%|▊         | 5403/64000 [00:02<00:24, 2429.55it/s]Loading:   9%|▉         | 5659/64000 [00:02<00:23, 2467.01it/s]Loading:   9%|▉         | 5918/64000 [00:02<00:23, 2502.36it/s]Loading:  10%|▉         | 6175/64000 [00:02<00:22, 2520.98it/s]Loading:  10%|█         | 6431/64000 [00:02<00:22, 2531.59it/s]Loading:  10%|█         | 6687/64000 [00:02<00:22, 2526.10it/s]Loading:  11%|█         | 6942/64000 [00:02<00:22, 2524.16it/s]Loading:  11%|█         | 7198/64000 [00:02<00:22, 2532.86it/s]Loading:  12%|█▏        | 7455/64000 [00:03<00:22, 2541.66it/s]Loading:  12%|█▏        | 7713/64000 [00:03<00:22, 2550.74it/s]Loading:  12%|█▏        | 7969/64000 [00:03<00:21, 2548.21it/s]Loading:  13%|█▎        | 8225/64000 [00:03<00:21, 2548.83it/s]Loading:  13%|█▎        | 8481/64000 [00:03<00:21, 2551.21it/s]Loading:  14%|█▎        | 8740/64000 [00:03<00:21, 2561.12it/s]Loading:  14%|█▍        | 8997/64000 [00:03<00:21, 2556.46it/s]Loading:  14%|█▍        | 9254/64000 [00:03<00:21, 2559.94it/s]Loading:  15%|█▍        | 9511/64000 [00:03<00:21, 2557.28it/s]Loading:  15%|█▌        | 9768/64000 [00:03<00:21, 2560.28it/s]Loading:  16%|█▌        | 10025/64000 [00:04<00:21, 2561.38it/s]Loading:  16%|█▌        | 10282/64000 [00:04<00:20, 2562.91it/s]Loading:  16%|█▋        | 10539/64000 [00:04<00:20, 2561.41it/s]Loading:  17%|█▋        | 10796/64000 [00:04<00:20, 2563.74it/s]Loading:  17%|█▋        | 11053/64000 [00:04<00:20, 2562.93it/s]Loading:  18%|█▊        | 11310/64000 [00:04<00:20, 2540.14it/s]Loading:  18%|█▊        | 11567/64000 [00:04<00:20, 2548.71it/s]Loading:  18%|█▊        | 11824/64000 [00:04<00:20, 2553.46it/s]Loading:  19%|█▉        | 12080/64000 [00:04<00:20, 2553.36it/s]Loading:  19%|█▉        | 12336/64000 [00:05<00:29, 1771.42it/s]Loading:  20%|█▉        | 12590/64000 [00:05<00:26, 1946.47it/s]Loading:  20%|██        | 12845/64000 [00:05<00:24, 2093.17it/s]Loading:  20%|██        | 13099/64000 [00:05<00:23, 2207.33it/s]Loading:  21%|██        | 13355/64000 [00:05<00:21, 2302.57it/s]Loading:  21%|██▏       | 13611/64000 [00:05<00:21, 2374.24it/s]Loading:  22%|██▏       | 13867/64000 [00:05<00:20, 2425.04it/s]Loading:  22%|██▏       | 14122/64000 [00:05<00:20, 2459.46it/s]Loading:  22%|██▏       | 14379/64000 [00:05<00:19, 2490.68it/s]Loading:  23%|██▎       | 14636/64000 [00:06<00:19, 2511.85it/s]Loading:  23%|██▎       | 14894/64000 [00:06<00:19, 2529.80it/s]Loading:  24%|██▎       | 15150/64000 [00:06<00:19, 2537.16it/s]Loading:  24%|██▍       | 15409/64000 [00:06<00:19, 2552.72it/s]Loading:  24%|██▍       | 15666/64000 [00:06<00:18, 2551.35it/s]Loading:  25%|██▍       | 15923/64000 [00:06<00:18, 2556.09it/s]Loading:  25%|██▌       | 16180/64000 [00:06<00:18, 2559.26it/s]Loading:  26%|██▌       | 16437/64000 [00:06<00:18, 2555.82it/s]Loading:  26%|██▌       | 16693/64000 [00:06<00:18, 2550.42it/s]Loading:  26%|██▋       | 16949/64000 [00:06<00:18, 2551.37it/s]Loading:  27%|██▋       | 17205/64000 [00:07<00:18, 2544.72it/s]Loading:  27%|██▋       | 17462/64000 [00:07<00:18, 2550.38it/s]Loading:  28%|██▊       | 17718/64000 [00:07<00:18, 2551.00it/s]Loading:  28%|██▊       | 17974/64000 [00:07<00:18, 2551.44it/s]Loading:  28%|██▊       | 18230/64000 [00:07<00:17, 2550.25it/s]Loading:  29%|██▉       | 18488/64000 [00:07<00:17, 2556.46it/s]Loading:  29%|██▉       | 18744/64000 [00:07<00:17, 2556.37it/s]Loading:  30%|██▉       | 19000/64000 [00:07<00:17, 2545.99it/s]Loading:  30%|███       | 19255/64000 [00:07<00:17, 2544.69it/s]Loading:  30%|███       | 19510/64000 [00:07<00:17, 2543.16it/s]Loading:  31%|███       | 19766/64000 [00:08<00:17, 2547.26it/s]Loading:  31%|███▏      | 20021/64000 [00:08<00:17, 2547.28it/s]Loading:  32%|███▏      | 20277/64000 [00:08<00:17, 2551.06it/s]Loading:  32%|███▏      | 20533/64000 [00:08<00:17, 2546.02it/s]Loading:  32%|███▏      | 20788/64000 [00:08<00:16, 2546.18it/s]Loading:  33%|███▎      | 21044/64000 [00:08<00:16, 2547.72it/s]Loading:  33%|███▎      | 21301/64000 [00:08<00:16, 2554.19it/s]Loading:  34%|███▎      | 21558/64000 [00:08<00:16, 2556.40it/s]Loading:  34%|███▍      | 21814/64000 [00:08<00:16, 2553.94it/s]Loading:  34%|███▍      | 22070/64000 [00:08<00:16, 2552.45it/s]Loading:  35%|███▍      | 22326/64000 [00:09<00:25, 1660.35it/s]Loading:  35%|███▌      | 22583/64000 [00:09<00:22, 1857.36it/s]Loading:  36%|███▌      | 22837/64000 [00:09<00:20, 2017.50it/s]Loading:  36%|███▌      | 23090/64000 [00:09<00:19, 2146.57it/s]Loading:  36%|███▋      | 23342/64000 [00:09<00:18, 2243.37it/s]Loading:  37%|███▋      | 23598/64000 [00:09<00:17, 2329.85it/s]Loading:  37%|███▋      | 23853/64000 [00:09<00:16, 2390.64it/s]Loading:  38%|███▊      | 24109/64000 [00:09<00:16, 2437.78it/s]Loading:  38%|███▊      | 24362/64000 [00:10<00:16, 2464.46it/s]Loading:  38%|███▊      | 24618/64000 [00:10<00:15, 2491.57it/s]Loading:  39%|███▉      | 24874/64000 [00:10<00:15, 2511.48it/s]Loading:  39%|███▉      | 25130/64000 [00:10<00:15, 2525.27it/s]Loading:  40%|███▉      | 25389/64000 [00:10<00:15, 2543.71it/s]Loading:  40%|████      | 25645/64000 [00:10<00:15, 2547.34it/s]Loading:  40%|████      | 25903/64000 [00:10<00:14, 2555.92it/s]Loading:  41%|████      | 26162/64000 [00:10<00:14, 2563.46it/s]Loading:  41%|████▏     | 26419/64000 [00:10<00:14, 2562.45it/s]Loading:  42%|████▏     | 26676/64000 [00:10<00:14, 2558.60it/s]Loading:  42%|████▏     | 26934/64000 [00:11<00:14, 2562.75it/s]Loading:  42%|████▏     | 27191/64000 [00:11<00:14, 2563.99it/s]Loading:  43%|████▎     | 27449/64000 [00:11<00:14, 2566.04it/s]Loading:  43%|████▎     | 27706/64000 [00:11<00:14, 2564.50it/s]Loading:  44%|████▎     | 27963/64000 [00:11<00:14, 2564.06it/s]Loading:  44%|████▍     | 28220/64000 [00:11<00:13, 2558.94it/s]Loading:  44%|████▍     | 28476/64000 [00:11<00:13, 2556.02it/s]Loading:  45%|████▍     | 28734/64000 [00:11<00:13, 2561.37it/s]Loading:  45%|████▌     | 28991/64000 [00:11<00:13, 2557.14it/s]Loading:  46%|████▌     | 29247/64000 [00:11<00:13, 2556.87it/s]Loading:  46%|████▌     | 29503/64000 [00:12<00:13, 2557.36it/s]Loading:  46%|████▋     | 29760/64000 [00:12<00:13, 2560.48it/s]Loading:  47%|████▋     | 30017/64000 [00:12<00:13, 2557.36it/s]Loading:  47%|████▋     | 30274/64000 [00:12<00:13, 2559.55it/s]Loading:  48%|████▊     | 30530/64000 [00:12<00:13, 2549.67it/s]Loading:  48%|████▊     | 30786/64000 [00:12<00:13, 2549.56it/s]Loading:  49%|████▊     | 31041/64000 [00:12<00:12, 2549.67it/s]Loading:  49%|████▉     | 31296/64000 [00:12<00:12, 2548.25it/s]Loading:  49%|████▉     | 31551/64000 [00:12<00:12, 2538.28it/s]Loading:  50%|████▉     | 31805/64000 [00:12<00:12, 2537.16it/s]Loading:  50%|█████     | 32059/64000 [00:13<00:12, 2536.62it/s]Loading:  50%|█████     | 32314/64000 [00:13<00:12, 2540.13it/s]Loading:  51%|█████     | 32570/64000 [00:13<00:12, 2543.20it/s]Loading:  51%|█████▏    | 32825/64000 [00:13<00:12, 2544.62it/s]Loading:  52%|█████▏    | 33080/64000 [00:13<00:12, 2546.13it/s]Loading:  52%|█████▏    | 33335/64000 [00:13<00:12, 2544.47it/s]Loading:  52%|█████▏    | 33590/64000 [00:13<00:11, 2544.53it/s]Loading:  53%|█████▎    | 33845/64000 [00:13<00:11, 2546.03it/s]Loading:  53%|█████▎    | 34101/64000 [00:13<00:11, 2549.91it/s]Loading:  54%|█████▎    | 34358/64000 [00:13<00:11, 2554.27it/s]Loading:  54%|█████▍    | 34614/64000 [00:14<00:11, 2555.60it/s]Loading:  54%|█████▍    | 34870/64000 [00:14<00:11, 2553.18it/s]Loading:  55%|█████▍    | 35126/64000 [00:14<00:18, 1541.32it/s]Loading:  55%|█████▌    | 35381/64000 [00:14<00:16, 1747.12it/s]Loading:  56%|█████▌    | 35637/64000 [00:14<00:14, 1930.20it/s]Loading:  56%|█████▌    | 35890/64000 [00:14<00:13, 2075.75it/s]Loading:  56%|█████▋    | 36145/64000 [00:14<00:12, 2197.62it/s]Loading:  57%|█████▋    | 36401/64000 [00:14<00:12, 2294.06it/s]Loading:  57%|█████▋    | 36657/64000 [00:15<00:11, 2365.19it/s]Loading:  58%|█████▊    | 36914/64000 [00:15<00:11, 2421.27it/s]Loading:  58%|█████▊    | 37170/64000 [00:15<00:10, 2461.00it/s]Loading:  58%|█████▊    | 37429/64000 [00:15<00:10, 2496.26it/s]Loading:  59%|█████▉    | 37684/64000 [00:15<00:10, 2512.03it/s]Loading:  59%|█████▉    | 37942/64000 [00:15<00:10, 2530.58it/s]Loading:  60%|█████▉    | 38199/64000 [00:15<00:10, 2542.19it/s]Loading:  60%|██████    | 38457/64000 [00:15<00:10, 2552.15it/s]Loading:  60%|██████    | 38715/64000 [00:15<00:09, 2558.88it/s]Loading:  61%|██████    | 38972/64000 [00:15<00:09, 2554.52it/s]Loading:  61%|██████▏   | 39229/64000 [00:16<00:09, 2558.22it/s]Loading:  62%|██████▏   | 39486/64000 [00:16<00:09, 2549.91it/s]Loading:  62%|██████▏   | 39742/64000 [00:16<00:09, 2552.00it/s]Loading:  62%|██████▏   | 39998/64000 [00:16<00:09, 2544.66it/s]Loading:  63%|██████▎   | 40254/64000 [00:16<00:09, 2547.79it/s]Loading:  63%|██████▎   | 40509/64000 [00:16<00:09, 2547.31it/s]Loading:  64%|██████▎   | 40765/64000 [00:16<00:09, 2550.12it/s]Loading:  64%|██████▍   | 41021/64000 [00:16<00:09, 2550.35it/s]Loading:  64%|██████▍   | 41277/64000 [00:16<00:08, 2546.15it/s]Loading:  65%|██████▍   | 41532/64000 [00:16<00:08, 2539.17it/s]Loading:  65%|██████▌   | 41787/64000 [00:17<00:08, 2542.28it/s]Loading:  66%|██████▌   | 42042/64000 [00:17<00:08, 2542.58it/s]Loading:  66%|██████▌   | 42297/64000 [00:17<00:08, 2536.16it/s]Loading:  66%|██████▋   | 42551/64000 [00:17<00:08, 2534.50it/s]Loading:  67%|██████▋   | 42805/64000 [00:17<00:08, 2529.58it/s]Loading:  67%|██████▋   | 43059/64000 [00:17<00:08, 2530.48it/s]Loading:  68%|██████▊   | 43313/64000 [00:17<00:08, 2526.70it/s]Loading:  68%|██████▊   | 43567/64000 [00:17<00:08, 2530.62it/s]Loading:  68%|██████▊   | 43821/64000 [00:17<00:07, 2527.02it/s]Loading:  69%|██████▉   | 44075/64000 [00:17<00:07, 2529.69it/s]Loading:  69%|██████▉   | 44328/64000 [00:18<00:07, 2527.52it/s]Loading:  70%|██████▉   | 44581/64000 [00:18<00:07, 2527.45it/s]Loading:  70%|███████   | 44834/64000 [00:18<00:07, 2525.93it/s]Loading:  70%|███████   | 45089/64000 [00:18<00:07, 2530.62it/s]Loading:  71%|███████   | 45344/64000 [00:18<00:07, 2533.58it/s]Loading:  71%|███████   | 45598/64000 [00:18<00:07, 2533.51it/s]Loading:  72%|███████▏  | 45852/64000 [00:18<00:07, 2530.77it/s]Loading:  72%|███████▏  | 46106/64000 [00:18<00:07, 2531.65it/s]Loading:  72%|███████▏  | 46360/64000 [00:18<00:06, 2534.04it/s]Loading:  73%|███████▎  | 46614/64000 [00:18<00:06, 2534.76it/s]Loading:  73%|███████▎  | 46868/64000 [00:19<00:06, 2534.73it/s]Loading:  74%|███████▎  | 47122/64000 [00:19<00:06, 2531.40it/s]Loading:  74%|███████▍  | 47376/64000 [00:19<00:06, 2532.43it/s]Loading:  74%|███████▍  | 47631/64000 [00:19<00:06, 2536.81it/s]Loading:  75%|███████▍  | 47886/64000 [00:19<00:06, 2538.16it/s]Loading:  75%|███████▌  | 48141/64000 [00:19<00:06, 2540.78it/s]Loading:  76%|███████▌  | 48398/64000 [00:19<00:06, 2547.02it/s]Loading:  76%|███████▌  | 48654/64000 [00:19<00:06, 2548.21it/s]Loading:  76%|███████▋  | 48909/64000 [00:19<00:05, 2545.37it/s]Loading:  77%|███████▋  | 49164/64000 [00:19<00:05, 2544.13it/s]Loading:  77%|███████▋  | 49419/64000 [00:20<00:05, 2539.70it/s]Loading:  78%|███████▊  | 49673/64000 [00:20<00:05, 2536.66it/s]Loading:  78%|███████▊  | 49927/64000 [00:20<00:05, 2531.64it/s]Loading:  78%|███████▊  | 50182/64000 [00:20<00:05, 2535.99it/s]Loading:  79%|███████▉  | 50437/64000 [00:20<00:05, 2537.77it/s]Loading:  79%|███████▉  | 50691/64000 [00:20<00:05, 2538.30it/s]Loading:  80%|███████▉  | 50945/64000 [00:20<00:09, 1385.96it/s]Loading:  80%|███████▉  | 51197/64000 [00:21<00:08, 1599.69it/s]Loading:  80%|████████  | 51451/64000 [00:21<00:06, 1799.07it/s]Loading:  81%|████████  | 51703/64000 [00:21<00:06, 1966.87it/s]Loading:  81%|████████  | 51958/64000 [00:21<00:05, 2110.52it/s]Loading:  82%|████████▏ | 52211/64000 [00:21<00:05, 2220.22it/s]Loading:  82%|████████▏ | 52467/64000 [00:21<00:04, 2311.94it/s]Loading:  82%|████████▏ | 52721/64000 [00:21<00:04, 2373.82it/s]Loading:  83%|████████▎ | 52975/64000 [00:21<00:04, 2420.37it/s]Loading:  83%|████████▎ | 53226/64000 [00:21<00:04, 2446.15it/s]Loading:  84%|████████▎ | 53480/64000 [00:21<00:04, 2472.55it/s]Loading:  84%|████████▍ | 53733/64000 [00:22<00:04, 2487.10it/s]Loading:  84%|████████▍ | 53989/64000 [00:22<00:03, 2505.90it/s]Loading:  85%|████████▍ | 54244/64000 [00:22<00:03, 2516.66it/s]Loading:  85%|████████▌ | 54499/64000 [00:22<00:03, 2523.84it/s]Loading:  86%|████████▌ | 54756/64000 [00:22<00:03, 2534.76it/s]Loading:  86%|████████▌ | 55012/64000 [00:22<00:03, 2540.54it/s]Loading:  86%|████████▋ | 55268/64000 [00:22<00:03, 2545.80it/s]Loading:  87%|████████▋ | 55523/64000 [00:22<00:03, 2539.20it/s]Loading:  87%|████████▋ | 55780/64000 [00:22<00:03, 2546.35it/s]Loading:  88%|████████▊ | 56035/64000 [00:22<00:03, 2544.32it/s]Loading:  88%|████████▊ | 56291/64000 [00:23<00:03, 2547.13it/s]Loading:  88%|████████▊ | 56546/64000 [00:23<00:02, 2545.91it/s]Loading:  89%|████████▉ | 56803/64000 [00:23<00:02, 2551.08it/s]Loading:  89%|████████▉ | 57059/64000 [00:23<00:02, 2548.42it/s]Loading:  90%|████████▉ | 57316/64000 [00:23<00:02, 2552.53it/s]Loading:  90%|████████▉ | 57572/64000 [00:23<00:02, 2550.23it/s]Loading:  90%|█████████ | 57829/64000 [00:23<00:02, 2555.11it/s]Loading:  91%|█████████ | 58085/64000 [00:23<00:02, 2554.70it/s]Loading:  91%|█████████ | 58341/64000 [00:23<00:02, 2548.48it/s]Loading:  92%|█████████▏| 58597/64000 [00:23<00:02, 2550.51it/s]Loading:  92%|█████████▏| 58853/64000 [00:24<00:02, 2546.14it/s]Loading:  92%|█████████▏| 59108/64000 [00:24<00:01, 2546.24it/s]Loading:  93%|█████████▎| 59363/64000 [00:24<00:01, 2541.57it/s]Loading:  93%|█████████▎| 59618/64000 [00:24<00:01, 2538.70it/s]Loading:  94%|█████████▎| 59873/64000 [00:24<00:01, 2541.08it/s]Loading:  94%|█████████▍| 60128/64000 [00:24<00:01, 2543.33it/s]Loading:  94%|█████████▍| 60384/64000 [00:24<00:01, 2545.61it/s]Loading:  95%|█████████▍| 60639/64000 [00:24<00:01, 2542.06it/s]Loading:  95%|█████████▌| 60894/64000 [00:24<00:01, 2542.19it/s]Loading:  96%|█████████▌| 61149/64000 [00:24<00:01, 2543.06it/s]Loading:  96%|█████████▌| 61404/64000 [00:25<00:01, 2536.32it/s]Loading:  96%|█████████▋| 61658/64000 [00:25<00:00, 2536.41it/s]Loading:  97%|█████████▋| 61913/64000 [00:25<00:00, 2539.58it/s]Loading:  97%|█████████▋| 62167/64000 [00:25<00:00, 2536.98it/s]Loading:  98%|█████████▊| 62421/64000 [00:25<00:00, 2537.32it/s]Loading:  98%|█████████▊| 62675/64000 [00:25<00:00, 2537.37it/s]Loading:  98%|█████████▊| 62930/64000 [00:25<00:00, 2538.21it/s]Loading:  99%|█████████▊| 63184/64000 [00:25<00:00, 2536.76it/s]Loading:  99%|█████████▉| 63439/64000 [00:25<00:00, 2538.23it/s]Loading: 100%|█████████▉| 63693/64000 [00:25<00:00, 2533.95it/s]Loading: 100%|█████████▉| 63948/64000 [00:26<00:00, 2537.00it/s]Loading: 100%|██████████| 64000/64000 [00:26<00:00, 2452.71it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 254/49018 [00:00<00:19, 2535.10it/s]Loading:   1%|          | 515/49018 [00:00<00:18, 2576.02it/s]Loading:   2%|▏         | 779/49018 [00:00<00:18, 2604.52it/s]Loading:   2%|▏         | 1040/49018 [00:00<00:18, 2595.69it/s]Loading:   3%|▎         | 1306/49018 [00:00<00:18, 2616.96it/s]Loading:   3%|▎         | 1571/49018 [00:00<00:18, 2626.90it/s]Loading:   4%|▎         | 1836/49018 [00:00<00:17, 2632.48it/s]Loading:   4%|▍         | 2100/49018 [00:00<00:17, 2626.00it/s]Loading:   5%|▍         | 2365/49018 [00:00<00:17, 2633.10it/s]Loading:   5%|▌         | 2629/49018 [00:01<00:17, 2629.70it/s]Loading:   6%|▌         | 2893/49018 [00:01<00:17, 2631.12it/s]Loading:   6%|▋         | 3157/49018 [00:01<00:17, 2633.69it/s]Loading:   7%|▋         | 3421/49018 [00:01<00:17, 2623.90it/s]Loading:   8%|▊         | 3684/49018 [00:01<00:17, 2612.16it/s]Loading:   8%|▊         | 3950/49018 [00:01<00:17, 2626.37it/s]Loading:   9%|▊         | 4213/49018 [00:01<00:17, 2626.38it/s]Loading:   9%|▉         | 4478/49018 [00:01<00:16, 2630.53it/s]Loading:  10%|▉         | 4742/49018 [00:01<00:16, 2620.78it/s]Loading:  10%|█         | 5005/49018 [00:01<00:16, 2618.45it/s]Loading:  11%|█         | 5269/49018 [00:02<00:16, 2621.95it/s]Loading:  11%|█▏        | 5535/49018 [00:02<00:16, 2632.55it/s]Loading:  12%|█▏        | 5799/49018 [00:02<00:16, 2627.59it/s]Loading:  12%|█▏        | 6062/49018 [00:02<00:16, 2628.00it/s]Loading:  13%|█▎        | 6325/49018 [00:02<00:16, 2624.38it/s]Loading:  13%|█▎        | 6590/49018 [00:02<00:16, 2630.80it/s]Loading:  14%|█▍        | 6855/49018 [00:02<00:15, 2636.02it/s]Loading:  15%|█▍        | 7119/49018 [00:02<00:15, 2622.29it/s]Loading:  15%|█▌        | 7382/49018 [00:02<00:15, 2613.35it/s]Loading:  16%|█▌        | 7644/49018 [00:02<00:16, 2572.56it/s]Loading:  16%|█▌        | 7906/49018 [00:03<00:15, 2583.89it/s]Loading:  17%|█▋        | 8167/49018 [00:03<00:15, 2590.25it/s]Loading:  17%|█▋        | 8429/49018 [00:03<00:15, 2597.19it/s]Loading:  18%|█▊        | 8690/49018 [00:03<00:15, 2599.16it/s]Loading:  18%|█▊        | 8952/49018 [00:03<00:15, 2604.73it/s]Loading:  19%|█▉        | 9215/49018 [00:03<00:15, 2610.90it/s]Loading:  19%|█▉        | 9479/49018 [00:03<00:15, 2618.70it/s]Loading:  20%|█▉        | 9741/49018 [00:03<00:15, 2618.27it/s]Loading:  20%|██        | 10003/49018 [00:03<00:14, 2618.15it/s]Loading:  21%|██        | 10265/49018 [00:03<00:14, 2611.88it/s]Loading:  21%|██▏       | 10528/49018 [00:04<00:14, 2616.83it/s]Loading:  22%|██▏       | 10790/49018 [00:04<00:14, 2617.33it/s]Loading:  23%|██▎       | 11053/49018 [00:04<00:14, 2618.50it/s]Loading:  23%|██▎       | 11315/49018 [00:04<00:14, 2613.71it/s]Loading:  24%|██▎       | 11577/49018 [00:04<00:14, 2607.84it/s]Loading:  24%|██▍       | 11838/49018 [00:04<00:14, 2606.12it/s]Loading:  25%|██▍       | 12099/49018 [00:04<00:14, 2600.26it/s]Loading:  25%|██▌       | 12362/49018 [00:04<00:14, 2607.02it/s]Loading:  26%|██▌       | 12623/49018 [00:04<00:13, 2601.84it/s]Loading:  26%|██▋       | 12884/49018 [00:04<00:13, 2600.11it/s]Loading:  27%|██▋       | 13145/49018 [00:05<00:13, 2584.28it/s]Loading:  27%|██▋       | 13404/49018 [00:05<00:13, 2568.00it/s]Loading:  28%|██▊       | 13661/49018 [00:05<00:13, 2558.25it/s]Loading:  28%|██▊       | 13917/49018 [00:05<00:13, 2556.92it/s]Loading:  29%|██▉       | 14173/49018 [00:05<00:13, 2544.39it/s]Loading:  29%|██▉       | 14430/49018 [00:05<00:13, 2549.74it/s]Loading:  30%|██▉       | 14686/49018 [00:05<00:13, 2552.35it/s]Loading:  30%|███       | 14943/49018 [00:05<00:13, 2555.90it/s]Loading:  31%|███       | 15199/49018 [00:05<00:13, 2551.78it/s]Loading:  32%|███▏      | 15455/49018 [00:05<00:13, 2540.70it/s]Loading:  32%|███▏      | 15710/49018 [00:06<00:13, 2543.44it/s]Loading:  33%|███▎      | 15965/49018 [00:06<00:13, 2540.89it/s]Loading:  33%|███▎      | 16221/49018 [00:06<00:12, 2544.98it/s]Loading:  34%|███▎      | 16476/49018 [00:06<00:12, 2543.37it/s]Loading:  34%|███▍      | 16731/49018 [00:06<00:12, 2537.82it/s]Loading:  35%|███▍      | 16986/49018 [00:06<00:12, 2538.77it/s]Loading:  35%|███▌      | 17243/49018 [00:06<00:12, 2546.26it/s]Loading:  36%|███▌      | 17498/49018 [00:06<00:12, 2545.01it/s]Loading:  36%|███▌      | 17754/49018 [00:06<00:12, 2548.55it/s]Loading:  37%|███▋      | 18009/49018 [00:07<00:24, 1251.81it/s]Loading:  37%|███▋      | 18262/49018 [00:07<00:20, 1473.21it/s]Loading:  38%|███▊      | 18514/49018 [00:07<00:18, 1680.80it/s]Loading:  38%|███▊      | 18768/49018 [00:07<00:16, 1869.30it/s]Loading:  39%|███▉      | 19021/49018 [00:07<00:14, 2027.74it/s]Loading:  39%|███▉      | 19274/49018 [00:07<00:13, 2154.56it/s]Loading:  40%|███▉      | 19528/49018 [00:07<00:13, 2255.50it/s]Loading:  40%|████      | 19775/49018 [00:07<00:12, 2299.14it/s]Loading:  41%|████      | 20027/49018 [00:08<00:12, 2360.83it/s]Loading:  41%|████▏     | 20279/49018 [00:08<00:11, 2405.40it/s]Loading:  42%|████▏     | 20528/49018 [00:08<00:11, 2426.17it/s]Loading:  42%|████▏     | 20780/49018 [00:08<00:11, 2451.37it/s]Loading:  43%|████▎     | 21033/49018 [00:08<00:11, 2471.86it/s]Loading:  43%|████▎     | 21288/49018 [00:08<00:11, 2492.71it/s]Loading:  44%|████▍     | 21541/49018 [00:08<00:10, 2501.81it/s]Loading:  44%|████▍     | 21796/49018 [00:08<00:10, 2514.76it/s]Loading:  45%|████▍     | 22049/49018 [00:08<00:10, 2519.21it/s]Loading:  45%|████▌     | 22302/49018 [00:08<00:10, 2518.69it/s]Loading:  46%|████▌     | 22556/49018 [00:09<00:10, 2524.01it/s]Loading:  47%|████▋     | 22809/49018 [00:09<00:10, 2522.06it/s]Loading:  47%|████▋     | 23062/49018 [00:09<00:10, 2523.23it/s]Loading:  48%|████▊     | 23315/49018 [00:09<00:10, 2525.06it/s]Loading:  48%|████▊     | 23568/49018 [00:09<00:10, 2524.64it/s]Loading:  49%|████▊     | 23821/49018 [00:09<00:09, 2524.56it/s]Loading:  49%|████▉     | 24075/49018 [00:09<00:09, 2528.14it/s]Loading:  50%|████▉     | 24329/49018 [00:09<00:09, 2528.99it/s]Loading:  50%|█████     | 24582/49018 [00:09<00:09, 2528.13it/s]Loading:  51%|█████     | 24835/49018 [00:09<00:09, 2521.07it/s]Loading:  51%|█████     | 25089/49018 [00:10<00:09, 2525.11it/s]Loading:  52%|█████▏    | 25342/49018 [00:10<00:09, 2524.68it/s]Loading:  52%|█████▏    | 25596/49018 [00:10<00:09, 2526.77it/s]Loading:  53%|█████▎    | 25849/49018 [00:10<00:09, 2523.81it/s]Loading:  53%|█████▎    | 26103/49018 [00:10<00:09, 2525.86it/s]Loading:  54%|█████▍    | 26356/49018 [00:10<00:08, 2518.04it/s]Loading:  54%|█████▍    | 26610/49018 [00:10<00:08, 2524.29it/s]Loading:  55%|█████▍    | 26863/49018 [00:10<00:08, 2523.41it/s]Loading:  55%|█████▌    | 27118/49018 [00:10<00:08, 2528.80it/s]Loading:  56%|█████▌    | 27371/49018 [00:11<00:08, 2523.13it/s]Loading:  56%|█████▋    | 27625/49018 [00:11<00:08, 2527.53it/s]Loading:  57%|█████▋    | 27879/49018 [00:11<00:08, 2530.14it/s]Loading:  57%|█████▋    | 28133/49018 [00:11<00:08, 2530.55it/s]Loading:  58%|█████▊    | 28387/49018 [00:11<00:08, 2530.12it/s]Loading:  58%|█████▊    | 28642/49018 [00:11<00:08, 2534.14it/s]Loading:  59%|█████▉    | 28898/49018 [00:11<00:07, 2540.30it/s]Loading:  59%|█████▉    | 29153/49018 [00:11<00:07, 2540.12it/s]Loading:  60%|█████▉    | 29408/49018 [00:11<00:07, 2541.67it/s]Loading:  61%|██████    | 29663/49018 [00:11<00:07, 2534.20it/s]Loading:  61%|██████    | 29917/49018 [00:12<00:07, 2534.96it/s]Loading:  62%|██████▏   | 30171/49018 [00:12<00:07, 2530.43it/s]Loading:  62%|██████▏   | 30426/49018 [00:12<00:07, 2533.60it/s]Loading:  63%|██████▎   | 30680/49018 [00:12<00:07, 2534.21it/s]Loading:  63%|██████▎   | 30934/49018 [00:12<00:07, 2535.74it/s]Loading:  64%|██████▎   | 31189/49018 [00:12<00:07, 2537.14it/s]Loading:  64%|██████▍   | 31443/49018 [00:12<00:06, 2534.70it/s]Loading:  65%|██████▍   | 31699/49018 [00:12<00:06, 2539.55it/s]Loading:  65%|██████▌   | 31953/49018 [00:12<00:06, 2536.95it/s]Loading:  66%|██████▌   | 32208/49018 [00:12<00:06, 2537.99it/s]Loading:  66%|██████▌   | 32462/49018 [00:13<00:06, 2533.25it/s]Loading:  67%|██████▋   | 32716/49018 [00:13<00:06, 2534.79it/s]Loading:  67%|██████▋   | 32970/49018 [00:13<00:06, 2530.94it/s]Loading:  68%|██████▊   | 33226/49018 [00:13<00:06, 2536.83it/s]Loading:  68%|██████▊   | 33480/49018 [00:13<00:06, 2535.24it/s]Loading:  69%|██████▉   | 33736/49018 [00:13<00:06, 2540.18it/s]Loading:  69%|██████▉   | 33991/49018 [00:13<00:05, 2540.58it/s]Loading:  70%|██████▉   | 34246/49018 [00:13<00:05, 2539.16it/s]Loading:  70%|███████   | 34500/49018 [00:13<00:05, 2537.98it/s]Loading:  71%|███████   | 34754/49018 [00:13<00:05, 2537.94it/s]Loading:  71%|███████▏  | 35008/49018 [00:14<00:05, 2537.22it/s]Loading:  72%|███████▏  | 35262/49018 [00:14<00:05, 2533.50it/s]Loading:  72%|███████▏  | 35517/49018 [00:14<00:05, 2536.01it/s]Loading:  73%|███████▎  | 35771/49018 [00:14<00:05, 2533.22it/s]Loading:  73%|███████▎  | 36025/49018 [00:14<00:05, 2532.37it/s]Loading:  74%|███████▍  | 36279/49018 [00:14<00:05, 2527.27it/s]Loading:  75%|███████▍  | 36533/49018 [00:14<00:04, 2528.33it/s]Loading:  75%|███████▌  | 36786/49018 [00:14<00:04, 2527.25it/s]Loading:  76%|███████▌  | 37041/49018 [00:14<00:04, 2533.16it/s]Loading:  76%|███████▌  | 37295/49018 [00:14<00:04, 2531.25it/s]Loading:  77%|███████▋  | 37549/49018 [00:15<00:04, 2532.89it/s]Loading:  77%|███████▋  | 37804/49018 [00:15<00:04, 2536.30it/s]Loading:  78%|███████▊  | 38058/49018 [00:15<00:04, 2534.15it/s]Loading:  78%|███████▊  | 38313/49018 [00:15<00:04, 2536.16it/s]Loading:  79%|███████▊  | 38567/49018 [00:15<00:04, 2534.34it/s]Loading:  79%|███████▉  | 38821/49018 [00:15<00:04, 2535.70it/s]Loading:  80%|███████▉  | 39076/49018 [00:15<00:03, 2539.39it/s]Loading:  80%|████████  | 39330/49018 [00:15<00:03, 2539.51it/s]Loading:  81%|████████  | 39584/49018 [00:15<00:03, 2536.69it/s]Loading:  81%|████████▏ | 39838/49018 [00:15<00:03, 2535.61it/s]Loading:  82%|████████▏ | 40092/49018 [00:16<00:03, 2534.16it/s]Loading:  82%|████████▏ | 40346/49018 [00:16<00:03, 2535.10it/s]Loading:  83%|████████▎ | 40601/49018 [00:16<00:03, 2537.66it/s]Loading:  83%|████████▎ | 40855/49018 [00:16<00:03, 2536.74it/s]Loading:  84%|████████▍ | 41110/49018 [00:16<00:03, 2539.84it/s]Loading:  84%|████████▍ | 41364/49018 [00:16<00:03, 2537.44it/s]Loading:  85%|████████▍ | 41619/49018 [00:16<00:02, 2539.39it/s]Loading:  85%|████████▌ | 41873/49018 [00:16<00:02, 2536.97it/s]Loading:  86%|████████▌ | 42128/49018 [00:16<00:02, 2538.56it/s]Loading:  86%|████████▋ | 42382/49018 [00:16<00:02, 2533.36it/s]Loading:  87%|████████▋ | 42636/49018 [00:17<00:02, 2532.27it/s]Loading:  87%|████████▋ | 42890/49018 [00:17<00:02, 2531.90it/s]Loading:  88%|████████▊ | 43144/49018 [00:17<00:02, 2531.99it/s]Loading:  89%|████████▊ | 43398/49018 [00:17<00:02, 2533.65it/s]Loading:  89%|████████▉ | 43652/49018 [00:17<00:02, 2534.11it/s]Loading:  90%|████████▉ | 43907/49018 [00:17<00:02, 2536.37it/s]Loading:  90%|█████████ | 44161/49018 [00:17<00:01, 2535.25it/s]Loading:  91%|█████████ | 44415/49018 [00:17<00:01, 2535.22it/s]Loading:  91%|█████████ | 44669/49018 [00:17<00:01, 2535.49it/s]Loading:  92%|█████████▏| 44923/49018 [00:17<00:01, 2534.53it/s]Loading:  92%|█████████▏| 45177/49018 [00:18<00:01, 2535.17it/s]Loading:  93%|█████████▎| 45433/49018 [00:18<00:01, 2540.23it/s]Loading:  93%|█████████▎| 45688/49018 [00:18<00:02, 1121.92it/s]Loading:  94%|█████████▎| 45942/49018 [00:18<00:02, 1346.75it/s]Loading:  94%|█████████▍| 46196/49018 [00:18<00:01, 1566.59it/s]Loading:  95%|█████████▍| 46450/49018 [00:18<00:01, 1769.10it/s]Loading:  95%|█████████▌| 46702/49018 [00:19<00:01, 1941.53it/s]Loading:  96%|█████████▌| 46954/49018 [00:19<00:00, 2083.62it/s]Loading:  96%|█████████▋| 47205/49018 [00:19<00:00, 2194.72it/s]Loading:  97%|█████████▋| 47460/49018 [00:19<00:00, 2288.85it/s]Loading:  97%|█████████▋| 47715/49018 [00:19<00:00, 2359.89it/s]Loading:  98%|█████████▊| 47967/49018 [00:19<00:00, 2404.14it/s]Loading:  98%|█████████▊| 48223/49018 [00:19<00:00, 2447.51it/s]Loading:  99%|█████████▉| 48475/49018 [00:19<00:00, 2467.36it/s]Loading:  99%|█████████▉| 48729/49018 [00:19<00:00, 2488.41it/s]Loading: 100%|█████████▉| 48982/49018 [00:19<00:00, 2495.79it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2455.01it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 263/49018 [00:00<00:18, 2624.22it/s]Loading:   1%|          | 529/49018 [00:00<00:18, 2642.98it/s]Loading:   2%|▏         | 794/49018 [00:00<00:18, 2631.36it/s]Loading:   2%|▏         | 1060/49018 [00:00<00:18, 2640.30it/s]Loading:   3%|▎         | 1325/49018 [00:00<00:18, 2643.70it/s]Loading:   3%|▎         | 1592/49018 [00:00<00:17, 2650.27it/s]Loading:   4%|▍         | 1858/49018 [00:01<00:43, 1076.28it/s]Loading:   4%|▍         | 2122/49018 [00:01<00:35, 1324.13it/s]Loading:   5%|▍         | 2387/49018 [00:01<00:29, 1568.43it/s]Loading:   5%|▌         | 2650/49018 [00:01<00:25, 1789.92it/s]Loading:   6%|▌         | 2904/49018 [00:01<00:23, 1961.82it/s]Loading:   6%|▋         | 3169/49018 [00:01<00:21, 2130.00it/s]Loading:   7%|▋         | 3432/49018 [00:01<00:20, 2260.33it/s]Loading:   8%|▊         | 3696/49018 [00:01<00:19, 2361.46it/s]Loading:   8%|▊         | 3961/49018 [00:01<00:18, 2441.41it/s]Loading:   9%|▊         | 4225/49018 [00:02<00:17, 2495.48it/s]Loading:   9%|▉         | 4490/49018 [00:02<00:17, 2538.10it/s]Loading:  10%|▉         | 4752/49018 [00:02<00:17, 2557.47it/s]Loading:  10%|█         | 5014/49018 [00:02<00:17, 2573.56it/s]Loading:  11%|█         | 5276/49018 [00:02<00:16, 2586.24it/s]Loading:  11%|█▏        | 5538/49018 [00:02<00:16, 2595.45it/s]Loading:  12%|█▏        | 5800/49018 [00:02<00:16, 2600.28it/s]Loading:  12%|█▏        | 6062/49018 [00:02<00:16, 2602.98it/s]Loading:  13%|█▎        | 6324/49018 [00:02<00:16, 2606.32it/s]Loading:  13%|█▎        | 6587/49018 [00:02<00:16, 2612.17it/s]Loading:  14%|█▍        | 6849/49018 [00:03<00:16, 2614.45it/s]Loading:  15%|█▍        | 7112/49018 [00:03<00:16, 2616.46it/s]Loading:  15%|█▌        | 7374/49018 [00:03<00:15, 2610.45it/s]Loading:  16%|█▌        | 7636/49018 [00:03<00:15, 2609.88it/s]Loading:  16%|█▌        | 7899/49018 [00:03<00:15, 2613.93it/s]Loading:  17%|█▋        | 8161/49018 [00:03<00:15, 2607.19it/s]Loading:  17%|█▋        | 8423/49018 [00:03<00:15, 2610.45it/s]Loading:  18%|█▊        | 8685/49018 [00:03<00:15, 2608.28it/s]Loading:  18%|█▊        | 8947/49018 [00:03<00:15, 2610.21it/s]Loading:  19%|█▉        | 9210/49018 [00:03<00:15, 2614.50it/s]Loading:  19%|█▉        | 9473/49018 [00:04<00:15, 2617.05it/s]Loading:  20%|█▉        | 9735/49018 [00:04<00:15, 2617.04it/s]Loading:  20%|██        | 9998/49018 [00:04<00:14, 2619.78it/s]Loading:  21%|██        | 10260/49018 [00:04<00:14, 2611.80it/s]Loading:  21%|██▏       | 10523/49018 [00:04<00:14, 2615.87it/s]Loading:  22%|██▏       | 10785/49018 [00:04<00:14, 2614.18it/s]Loading:  23%|██▎       | 11048/49018 [00:04<00:14, 2618.29it/s]Loading:  23%|██▎       | 11310/49018 [00:04<00:14, 2615.00it/s]Loading:  24%|██▎       | 11572/49018 [00:04<00:14, 2614.94it/s]Loading:  24%|██▍       | 11834/49018 [00:04<00:14, 2614.60it/s]Loading:  25%|██▍       | 12096/49018 [00:05<00:14, 2615.27it/s]Loading:  25%|██▌       | 12358/49018 [00:05<00:14, 2611.71it/s]Loading:  26%|██▌       | 12620/49018 [00:05<00:13, 2613.24it/s]Loading:  26%|██▋       | 12882/49018 [00:05<00:13, 2609.47it/s]Loading:  27%|██▋       | 13146/49018 [00:05<00:13, 2616.03it/s]Loading:  27%|██▋       | 13410/49018 [00:05<00:13, 2620.55it/s]Loading:  28%|██▊       | 13673/49018 [00:05<00:13, 2622.63it/s]Loading:  28%|██▊       | 13936/49018 [00:05<00:13, 2622.17it/s]Loading:  29%|██▉       | 14199/49018 [00:05<00:13, 2617.96it/s]Loading:  30%|██▉       | 14461/49018 [00:05<00:13, 2618.50it/s]Loading:  30%|███       | 14723/49018 [00:06<00:13, 2610.46it/s]Loading:  31%|███       | 14985/49018 [00:06<00:13, 2607.59it/s]Loading:  31%|███       | 15247/49018 [00:06<00:12, 2608.81it/s]Loading:  32%|███▏      | 15508/49018 [00:06<00:12, 2608.96it/s]Loading:  32%|███▏      | 15770/49018 [00:06<00:12, 2611.01it/s]Loading:  33%|███▎      | 16032/49018 [00:06<00:12, 2610.92it/s]Loading:  33%|███▎      | 16294/49018 [00:06<00:12, 2608.84it/s]Loading:  34%|███▍      | 16555/49018 [00:06<00:12, 2607.30it/s]Loading:  34%|███▍      | 16816/49018 [00:06<00:12, 2600.87it/s]Loading:  35%|███▍      | 17078/49018 [00:06<00:12, 2605.39it/s]Loading:  35%|███▌      | 17339/49018 [00:07<00:12, 2601.80it/s]Loading:  36%|███▌      | 17600/49018 [00:07<00:12, 2602.93it/s]Loading:  36%|███▋      | 17861/49018 [00:07<00:11, 2602.19it/s]Loading:  37%|███▋      | 18122/49018 [00:07<00:11, 2585.46it/s]Loading:  37%|███▋      | 18381/49018 [00:07<00:11, 2567.79it/s]Loading:  38%|███▊      | 18638/49018 [00:07<00:11, 2555.14it/s]Loading:  39%|███▊      | 18894/49018 [00:07<00:11, 2548.54it/s]Loading:  39%|███▉      | 19149/49018 [00:07<00:11, 2541.55it/s]Loading:  40%|███▉      | 19404/49018 [00:07<00:11, 2542.41it/s]Loading:  40%|████      | 19659/49018 [00:07<00:11, 2538.56it/s]Loading:  41%|████      | 19913/49018 [00:08<00:11, 2537.46it/s]Loading:  41%|████      | 20167/49018 [00:08<00:11, 2533.86it/s]Loading:  42%|████▏     | 20421/49018 [00:08<00:11, 2533.48it/s]Loading:  42%|████▏     | 20675/49018 [00:08<00:11, 2531.73it/s]Loading:  43%|████▎     | 20929/49018 [00:08<00:11, 2530.73it/s]Loading:  43%|████▎     | 21183/49018 [00:08<00:10, 2530.87it/s]Loading:  44%|████▎     | 21437/49018 [00:08<00:10, 2533.56it/s]Loading:  44%|████▍     | 21692/49018 [00:08<00:10, 2536.77it/s]Loading:  45%|████▍     | 21946/49018 [00:08<00:10, 2535.66it/s]Loading:  45%|████▌     | 22200/49018 [00:08<00:10, 2536.59it/s]Loading:  46%|████▌     | 22454/49018 [00:09<00:10, 2532.32it/s]Loading:  46%|████▋     | 22709/49018 [00:09<00:10, 2535.42it/s]Loading:  47%|████▋     | 22963/49018 [00:09<00:10, 2536.09it/s]Loading:  47%|████▋     | 23217/49018 [00:09<00:10, 2533.08it/s]Loading:  48%|████▊     | 23471/49018 [00:09<00:10, 2532.21it/s]Loading:  48%|████▊     | 23726/49018 [00:09<00:09, 2535.26it/s]Loading:  49%|████▉     | 23980/49018 [00:09<00:09, 2528.00it/s]Loading:  49%|████▉     | 24234/49018 [00:09<00:09, 2529.93it/s]Loading:  50%|████▉     | 24488/49018 [00:09<00:09, 2531.37it/s]Loading:  50%|█████     | 24742/49018 [00:09<00:09, 2533.13it/s]Loading:  51%|█████     | 24997/49018 [00:10<00:09, 2537.52it/s]Loading:  52%|█████▏    | 25251/49018 [00:10<00:09, 2532.59it/s]Loading:  52%|█████▏    | 25505/49018 [00:10<00:09, 2532.20it/s]Loading:  53%|█████▎    | 25759/49018 [00:10<00:09, 2530.39it/s]Loading:  53%|█████▎    | 26014/49018 [00:10<00:09, 2534.94it/s]Loading:  54%|█████▎    | 26268/49018 [00:10<00:08, 2534.55it/s]Loading:  54%|█████▍    | 26522/49018 [00:10<00:08, 2532.69it/s]Loading:  55%|█████▍    | 26776/49018 [00:10<00:08, 2531.61it/s]Loading:  55%|█████▌    | 27030/49018 [00:10<00:08, 2530.90it/s]Loading:  56%|█████▌    | 27284/49018 [00:10<00:08, 2529.08it/s]Loading:  56%|█████▌    | 27540/49018 [00:11<00:08, 2535.46it/s]Loading:  57%|█████▋    | 27794/49018 [00:11<00:08, 2532.83it/s]Loading:  57%|█████▋    | 28049/49018 [00:11<00:08, 2535.23it/s]Loading:  58%|█████▊    | 28303/49018 [00:11<00:08, 2532.37it/s]Loading:  58%|█████▊    | 28557/49018 [00:11<00:08, 2531.38it/s]Loading:  59%|█████▉    | 28811/49018 [00:11<00:07, 2533.74it/s]Loading:  59%|█████▉    | 29065/49018 [00:11<00:07, 2534.34it/s]Loading:  60%|█████▉    | 29320/49018 [00:11<00:07, 2537.64it/s]Loading:  60%|██████    | 29575/49018 [00:11<00:07, 2537.95it/s]Loading:  61%|██████    | 29829/49018 [00:11<00:07, 2532.33it/s]Loading:  61%|██████▏   | 30083/49018 [00:12<00:07, 2532.06it/s]Loading:  62%|██████▏   | 30337/49018 [00:12<00:07, 2530.19it/s]Loading:  62%|██████▏   | 30591/49018 [00:12<00:07, 2527.46it/s]Loading:  63%|██████▎   | 30845/49018 [00:12<00:07, 2530.46it/s]Loading:  63%|██████▎   | 31099/49018 [00:12<00:07, 2528.14it/s]Loading:  64%|██████▍   | 31354/49018 [00:12<00:06, 2533.81it/s]Loading:  64%|██████▍   | 31609/49018 [00:12<00:06, 2535.81it/s]Loading:  65%|██████▌   | 31863/49018 [00:12<00:06, 2529.15it/s]Loading:  66%|██████▌   | 32118/49018 [00:12<00:06, 2533.65it/s]Loading:  66%|██████▌   | 32372/49018 [00:13<00:06, 2519.81it/s]Loading:  67%|██████▋   | 32627/49018 [00:13<00:06, 2526.89it/s]Loading:  67%|██████▋   | 32880/49018 [00:13<00:06, 2525.31it/s]Loading:  68%|██████▊   | 33133/49018 [00:13<00:06, 2524.90it/s]Loading:  68%|██████▊   | 33386/49018 [00:13<00:06, 2522.80it/s]Loading:  69%|██████▊   | 33640/49018 [00:13<00:06, 2526.71it/s]Loading:  69%|██████▉   | 33893/49018 [00:13<00:05, 2524.40it/s]Loading:  70%|██████▉   | 34148/49018 [00:13<00:05, 2530.44it/s]Loading:  70%|███████   | 34402/49018 [00:13<00:05, 2524.63it/s]Loading:  71%|███████   | 34656/49018 [00:13<00:05, 2527.01it/s]Loading:  71%|███████   | 34911/49018 [00:14<00:05, 2531.50it/s]Loading:  72%|███████▏  | 35165/49018 [00:14<00:05, 2528.70it/s]Loading:  72%|███████▏  | 35418/49018 [00:14<00:05, 2528.81it/s]Loading:  73%|███████▎  | 35671/49018 [00:14<00:05, 2525.90it/s]Loading:  73%|███████▎  | 35925/49018 [00:14<00:05, 2529.27it/s]Loading:  74%|███████▍  | 36178/49018 [00:14<00:05, 2527.19it/s]Loading:  74%|███████▍  | 36431/49018 [00:14<00:04, 2524.48it/s]Loading:  75%|███████▍  | 36686/49018 [00:14<00:04, 2529.70it/s]Loading:  75%|███████▌  | 36940/49018 [00:14<00:04, 2530.53it/s]Loading:  76%|███████▌  | 37194/49018 [00:14<00:04, 2532.34it/s]Loading:  76%|███████▋  | 37448/49018 [00:15<00:04, 2534.38it/s]Loading:  77%|███████▋  | 37702/49018 [00:15<00:04, 2534.37it/s]Loading:  77%|███████▋  | 37956/49018 [00:15<00:11, 967.81it/s] Loading:  78%|███████▊  | 38210/49018 [00:15<00:09, 1188.31it/s]Loading:  78%|███████▊  | 38459/49018 [00:15<00:07, 1405.61it/s]Loading:  79%|███████▉  | 38713/49018 [00:16<00:06, 1623.02it/s]Loading:  79%|███████▉  | 38965/49018 [00:16<00:05, 1816.29it/s]Loading:  80%|████████  | 39220/49018 [00:16<00:04, 1987.12it/s]Loading:  81%|████████  | 39472/49018 [00:16<00:04, 2121.06it/s]Loading:  81%|████████  | 39724/49018 [00:16<00:04, 2226.17it/s]Loading:  82%|████████▏ | 39977/49018 [00:16<00:03, 2308.95it/s]Loading:  82%|████████▏ | 40232/49018 [00:16<00:03, 2375.10it/s]Loading:  83%|████████▎ | 40486/49018 [00:16<00:03, 2419.80it/s]Loading:  83%|████████▎ | 40739/49018 [00:16<00:03, 2449.68it/s]Loading:  84%|████████▎ | 40993/49018 [00:16<00:03, 2475.67it/s]Loading:  84%|████████▍ | 41246/49018 [00:17<00:03, 2487.33it/s]Loading:  85%|████████▍ | 41499/49018 [00:17<00:03, 2499.27it/s]Loading:  85%|████████▌ | 41753/49018 [00:17<00:02, 2509.96it/s]Loading:  86%|████████▌ | 42007/49018 [00:17<00:02, 2517.58it/s]Loading:  86%|████████▌ | 42262/49018 [00:17<00:02, 2525.06it/s]Loading:  87%|████████▋ | 42517/49018 [00:17<00:02, 2529.81it/s]Loading:  87%|████████▋ | 42771/49018 [00:17<00:02, 2523.82it/s]Loading:  88%|████████▊ | 43024/49018 [00:17<00:02, 2525.34it/s]Loading:  88%|████████▊ | 43277/49018 [00:17<00:02, 2525.03it/s]Loading:  89%|████████▉ | 43532/49018 [00:17<00:02, 2529.72it/s]Loading:  89%|████████▉ | 43786/49018 [00:18<00:02, 2532.17it/s]Loading:  90%|████████▉ | 44040/49018 [00:18<00:01, 2528.70it/s]Loading:  90%|█████████ | 44294/49018 [00:18<00:01, 2531.37it/s]Loading:  91%|█████████ | 44548/49018 [00:18<00:01, 2530.64it/s]Loading:  91%|█████████▏| 44802/49018 [00:18<00:01, 2531.96it/s]Loading:  92%|█████████▏| 45057/49018 [00:18<00:01, 2534.66it/s]Loading:  92%|█████████▏| 45312/49018 [00:18<00:01, 2538.73it/s]Loading:  93%|█████████▎| 45566/49018 [00:18<00:01, 2536.28it/s]Loading:  93%|█████████▎| 45820/49018 [00:18<00:01, 2535.42it/s]Loading:  94%|█████████▍| 46074/49018 [00:18<00:01, 2535.88it/s]Loading:  95%|█████████▍| 46329/49018 [00:19<00:01, 2537.44it/s]Loading:  95%|█████████▌| 46583/49018 [00:19<00:00, 2530.77it/s]Loading:  96%|█████████▌| 46837/49018 [00:19<00:00, 2531.82it/s]Loading:  96%|█████████▌| 47091/49018 [00:19<00:00, 2531.89it/s]Loading:  97%|█████████▋| 47345/49018 [00:19<00:00, 2534.05it/s]Loading:  97%|█████████▋| 47599/49018 [00:19<00:00, 2534.53it/s]Loading:  98%|█████████▊| 47853/49018 [00:19<00:00, 2527.93it/s]Loading:  98%|█████████▊| 48106/49018 [00:19<00:00, 2527.54it/s]Loading:  99%|█████████▊| 48360/49018 [00:19<00:00, 2529.26it/s]Loading:  99%|█████████▉| 48614/49018 [00:19<00:00, 2531.46it/s]Loading: 100%|█████████▉| 48868/49018 [00:20<00:00, 2531.37it/s]Loading: 100%|██████████| 49018/49018 [00:20<00:00, 2435.88it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank33]:[W424 17:51:43.544727467 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 17:51:43.544725945 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 17:51:43.544724983 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 17:51:43.544731575 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 17:51:43.544723640 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 17:51:43.546616544 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 17:51:43.547255925 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 17:51:43.548227916 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 17:51:46.924381427 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 17:51:46.933044467 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 17:51:46.933150548 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 17:51:46.934848430 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 17:51:46.937765325 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 17:51:46.938043493 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 17:51:46.941288742 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 17:51:48.832702087 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 17:51:48.832883852 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 17:51:48.833224961 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 17:51:48.834575979 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 17:51:48.909380269 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 17:51:48.915685179 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 17:51:48.917515158 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 17:51:48.918434891 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 17:51:48.920753525 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 17:51:48.923027565 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 17:51:49.514407754 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 17:51:49.514416240 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 17:51:49.514414297 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 17:51:49.514431459 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 17:51:49.514434515 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 17:51:49.514435377 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 17:51:49.196018473 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 17:51:49.196737961 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 17:51:49.197085351 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 17:51:49.949693759 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 17:51:49.855529475 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 17:51:49.855891030 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 17:51:49.195595993 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 17:51:49.456471062 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 17:51:50.433230031 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:25<20:32, 25.16s/it]Train:   4%|▍         | 2/50 [00:25<08:25, 10.53s/it]Train:   6%|▌         | 3/50 [00:25<04:33,  5.82s/it]Train:   8%|▊         | 4/50 [00:25<02:46,  3.61s/it]Train:  10%|█         | 5/50 [00:26<01:47,  2.39s/it]Train:  12%|█▏        | 6/50 [00:26<01:12,  1.66s/it]Train:  14%|█▍        | 7/50 [00:26<00:51,  1.19s/it]Train:  16%|█▌        | 8/50 [00:26<00:37,  1.13it/s]Train:  18%|█▊        | 9/50 [00:27<00:27,  1.48it/s]Train:  20%|██        | 10/50 [00:27<00:21,  1.86it/s]Train:  22%|██▏       | 11/50 [00:27<00:17,  2.26it/s]Train:  24%|██▍       | 12/50 [00:27<00:14,  2.65it/s]Train:  26%|██▌       | 13/50 [00:27<00:12,  3.01it/s]Train:  28%|██▊       | 14/50 [00:28<00:10,  3.33it/s]Train:  30%|███       | 15/50 [00:28<00:09,  3.60it/s]Train:  32%|███▏      | 16/50 [00:28<00:08,  3.81it/s]Train:  34%|███▍      | 17/50 [00:28<00:08,  3.94it/s]Train:  36%|███▌      | 18/50 [00:29<00:07,  4.07it/s]Train:  38%|███▊      | 19/50 [00:29<00:07,  4.18it/s]Train:  40%|████      | 20/50 [00:29<00:07,  4.25it/s]Train:  42%|████▏     | 21/50 [00:29<00:06,  4.30it/s]Train:  44%|████▍     | 22/50 [00:29<00:06,  4.33it/s]Train:  46%|████▌     | 23/50 [00:30<00:06,  4.36it/s]Train:  48%|████▊     | 24/50 [00:30<00:05,  4.37it/s]Train:  50%|█████     | 25/50 [00:30<00:05,  4.39it/s]Train:  52%|█████▏    | 26/50 [00:30<00:05,  4.39it/s]Train:  54%|█████▍    | 27/50 [00:31<00:05,  4.40it/s]Train:  56%|█████▌    | 28/50 [00:31<00:04,  4.40it/s]Train:  58%|█████▊    | 29/50 [00:31<00:04,  4.42it/s]Train:  60%|██████    | 30/50 [00:31<00:04,  4.43it/s]Train:  62%|██████▏   | 31/50 [00:32<00:04,  4.43it/s]Train:  64%|██████▍   | 32/50 [00:32<00:04,  4.42it/s]Train:  66%|██████▌   | 33/50 [00:32<00:03,  4.41it/s]Train:  68%|██████▊   | 34/50 [00:32<00:03,  4.41it/s]Train:  70%|███████   | 35/50 [00:32<00:03,  4.41it/s]Train:  72%|███████▏  | 36/50 [00:33<00:03,  4.40it/s]Train:  74%|███████▍  | 37/50 [00:33<00:02,  4.40it/s]Train:  76%|███████▌  | 38/50 [00:33<00:02,  4.42it/s]Train:  78%|███████▊  | 39/50 [00:33<00:02,  4.40it/s]Train:  80%|████████  | 40/50 [00:34<00:02,  4.41it/s]Train:  82%|████████▏ | 41/50 [00:34<00:02,  4.41it/s]Train:  84%|████████▍ | 42/50 [00:34<00:01,  4.39it/s]Train:  86%|████████▌ | 43/50 [00:34<00:01,  4.40it/s]Train:  88%|████████▊ | 44/50 [00:34<00:01,  4.41it/s]Train:  90%|█████████ | 45/50 [00:35<00:01,  4.41it/s]Train:  92%|█████████▏| 46/50 [00:35<00:00,  4.42it/s]Train:  94%|█████████▍| 47/50 [00:35<00:00,  4.42it/s]Train:  96%|█████████▌| 48/50 [00:35<00:00,  4.42it/s]Train:  98%|█████████▊| 49/50 [00:36<00:00,  4.41it/s]Train: 100%|██████████| 50/50 [00:36<00:00,  4.42it/s]Train: 100%|██████████| 50/50 [00:36<00:00,  1.37it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:19,  2.48it/s]Train:   4%|▍         | 2/50 [00:00<00:14,  3.34it/s]Train:   6%|▌         | 3/50 [00:00<00:12,  3.74it/s]Train:   8%|▊         | 4/50 [00:01<00:11,  4.00it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.15it/s]Train:  12%|█▏        | 6/50 [00:01<00:10,  4.24it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.31it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.34it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.40it/s]Train:  20%|██        | 10/50 [00:02<00:09,  4.39it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.41it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.40it/s]Train:  26%|██▌       | 13/50 [00:03<00:08,  4.40it/s]Train:  28%|██▊       | 14/50 [00:03<00:08,  4.40it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.42it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.42it/s]Train:  34%|███▍      | 17/50 [00:04<00:07,  4.41it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.41it/s]Train:  38%|███▊      | 19/50 [00:04<00:07,  4.41it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.41it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.42it/s]Train:  44%|████▍     | 22/50 [00:05<00:06,  4.42it/s]Train:  46%|████▌     | 23/50 [00:05<00:06,  4.43it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.42it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.41it/s]Train:  52%|█████▏    | 26/50 [00:06<00:05,  4.41it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.43it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.43it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.46it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.45it/s]Train:  62%|██████▏   | 31/50 [00:07<00:04,  4.46it/s]Train:  64%|██████▍   | 32/50 [00:07<00:04,  4.45it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.43it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.43it/s]Train:  70%|███████   | 35/50 [00:08<00:03,  4.43it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.43it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.43it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.44it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.43it/s]Train:  80%|████████  | 40/50 [00:09<00:02,  4.42it/s]Train:  82%|████████▏ | 41/50 [00:09<00:02,  4.42it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.42it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.42it/s]Train:  88%|████████▊ | 44/50 [00:10<00:01,  4.42it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.43it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.43it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.43it/s]Train:  96%|█████████▌| 48/50 [00:11<00:00,  4.43it/s]Train:  98%|█████████▊| 49/50 [00:11<00:00,  4.42it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.42it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.34it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.20it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.82it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.08it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.21it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.28it/s]Train:  12%|█▏        | 6/50 [00:01<00:10,  4.32it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.37it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.38it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.41it/s]Train:  20%|██        | 10/50 [00:02<00:09,  4.41it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.41it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.41it/s]Train:  26%|██▌       | 13/50 [00:03<00:08,  4.41it/s]Train:  28%|██▊       | 14/50 [00:03<00:08,  4.42it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.43it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.45it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.44it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.43it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.44it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.41it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.39it/s]Train:  44%|████▍     | 22/50 [00:05<00:06,  4.40it/s]Train:  46%|████▌     | 23/50 [00:05<00:06,  4.41it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.40it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.41it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.40it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.41it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.42it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.43it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.42it/s]Train:  62%|██████▏   | 31/50 [00:07<00:04,  4.42it/s]Train:  64%|██████▍   | 32/50 [00:07<00:04,  4.42it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.42it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.42it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.44it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.45it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.45it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.43it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.46it/s]Train:  80%|████████  | 40/50 [00:09<00:02,  4.45it/s]Train:  82%|████████▏ | 41/50 [00:09<00:02,  4.44it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.41it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.42it/s]Train:  88%|████████▊ | 44/50 [00:10<00:01,  4.43it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.41it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.43it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.43it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.41it/s]Train:  98%|█████████▊| 49/50 [00:11<00:00,  4.42it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.41it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.38it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.11it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.75it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.02it/s]Train:   8%|▊         | 4/50 [00:01<00:11,  4.17it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.28it/s]Train:  12%|█▏        | 6/50 [00:01<00:10,  4.32it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.35it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.37it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.40it/s]Train:  20%|██        | 10/50 [00:02<00:09,  4.42it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.42it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.42it/s]Train:  26%|██▌       | 13/50 [00:03<00:08,  4.44it/s]Train:  28%|██▊       | 14/50 [00:03<00:08,  4.45it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.44it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.43it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.42it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.44it/s]Train:  38%|███▊      | 19/50 [00:04<00:07,  4.43it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.42it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.40it/s]Train:  44%|████▍     | 22/50 [00:05<00:06,  4.43it/s]Train:  46%|████▌     | 23/50 [00:05<00:06,  4.44it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.43it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.41it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.44it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.44it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.43it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.42it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.42it/s]Train:  62%|██████▏   | 31/50 [00:07<00:04,  4.42it/s]Train:  64%|██████▍   | 32/50 [00:07<00:04,  4.42it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.43it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.43it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.43it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.43it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.42it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.42it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.42it/s]Train:  80%|████████  | 40/50 [00:09<00:02,  4.42it/s]Train:  82%|████████▏ | 41/50 [00:09<00:02,  4.44it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.44it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.44it/s]Train:  88%|████████▊ | 44/50 [00:10<00:01,  4.44it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.42it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.42it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.43it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.43it/s]Train:  98%|█████████▊| 49/50 [00:11<00:00,  4.43it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.44it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.38it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.18it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.84it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.07it/s]Train:   8%|▊         | 4/50 [00:00<00:11,  4.17it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.24it/s]Train:  12%|█▏        | 6/50 [00:01<00:10,  4.32it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.35it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.35it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.37it/s]Train:  20%|██        | 10/50 [00:02<00:09,  4.39it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.40it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.40it/s]Train:  26%|██▌       | 13/50 [00:03<00:08,  4.41it/s]Train:  28%|██▊       | 14/50 [00:03<00:08,  4.41it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.42it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.42it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.43it/s]Train:  36%|███▌      | 18/50 [00:04<00:07,  4.43it/s]Train:  38%|███▊      | 19/50 [00:04<00:07,  4.42it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.43it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.44it/s]Train:  44%|████▍     | 22/50 [00:05<00:06,  4.43it/s]Train:  46%|████▌     | 23/50 [00:05<00:06,  4.43it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.43it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.42it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.41it/s]Train:  54%|█████▍    | 27/50 [00:06<00:05,  4.43it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.42it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.42it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.45it/s]Train:  62%|██████▏   | 31/50 [00:07<00:04,  4.45it/s]Train:  64%|██████▍   | 32/50 [00:07<00:04,  4.44it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.34it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.35it/s]Train:  70%|███████   | 35/50 [00:08<00:03,  4.37it/s]Train:  72%|███████▏  | 36/50 [00:08<00:03,  4.40it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.41it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.41it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.42it/s]Train:  80%|████████  | 40/50 [00:09<00:02,  4.42it/s]Train:  82%|████████▏ | 41/50 [00:09<00:02,  4.42it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.43it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.44it/s]Train:  88%|████████▊ | 44/50 [00:10<00:01,  4.43it/s]Train:  90%|█████████ | 45/50 [00:10<00:01,  4.43it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.43it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.44it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.45it/s]Train:  98%|█████████▊| 49/50 [00:11<00:00,  4.43it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.43it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.37it/s]
0: Process 0 - Local timer:  load_data  :  86.12
0: Process 0 - Local timer:  train_validate_test  :  82.31
0: Process 0 - Local timer:  create_model  :  1.18
0: Minimum timers: 
0: load_data  :  86.12
0: train_validate_test  :  82.25
0: create_model  :  1.17
0: Maximum timers: 
0: load_data  :  87.49
0: train_validate_test  :  82.31
0: create_model  :  1.18
0: Average timers: 
0: load_data  :  87.07
0: train_validate_test  :  82.27
0: create_model  :  1.18
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 17:52:48.968096773 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 17:52:50.553386403 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 17:52:50.553503395 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 17:52:50.154335292 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 17:52:50.154388593 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 17:52:50.154710663 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 17:52:50.442842041 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 17:52:50.442850267 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 17:52:50.111370562 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 17:52:50.155118735 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 17:52:50.442848193 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 17:52:50.111353770 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 17:52:50.442861628 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 17:52:50.554433959 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 17:52:50.111378427 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 17:52:50.442883290 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 17:52:50.111379599 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 17:52:50.442880554 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 17:52:50.111430606 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 17:52:50.442890844 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 17:52:50.111387975 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 17:52:50.111434112 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 17:52:50.111533952 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 17:52:50.554734248 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 17:52:50.155640162 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 17:52:50.555094792 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 17:52:50.555644865 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 17:52:50.444490165 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 17:52:50.156726829 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 17:52:50.556231677 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 17:52:50.157091930 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 17:52:50.816218956 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 17:52:50.816259583 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 17:52:50.816297114 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 17:52:50.816397786 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 17:52:50.816383088 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 17:52:50.816387997 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 17:52:50.157928604 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 17:52:50.817295713 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 17:52:50.817372088 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 05:52:52 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 05:52:52 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_5 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 17:53:04.105550691 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.105977411 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.105984805 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.105963665 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.105995766 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.106055419 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.106989552 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.107345027 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.023863347 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.023913763 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.023955603 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.023995739 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.024043229 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.025457337 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.025595911 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.025601792 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696278152 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696367641 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696396586 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696438846 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696496045 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.696494582 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.697878906 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.698000446 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411156961 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411226412 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411272269 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411288389 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411352280 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411397666 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.411395372 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.412677990 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 17:53:04.226363104 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.226522757 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.229176999 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.230013114 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.230048161 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.231605523 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.231950848 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 17:53:04.245279376 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_BASELINE_5 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Read attr time (sec):  0.0010561943054199219
0: read and bcast: trainset/x/variable_count 0.1788480281829834
0: read and bcast: trainset/x/variable_offset 0.389786958694458
0: read and bcast: trainset/x/variable_dim 0.39019060134887695
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5721907615661621
0: read and bcast: trainset/edge_index/variable_offset 0.7503135204315186
0: read and bcast: trainset/edge_index/variable_dim 0.7591812610626221
0: read and bcast: trainset/edge_attr/variable_count 0.943140983581543
0: read and bcast: trainset/edge_attr/variable_offset 1.1255309581756592
0: read and bcast: trainset/edge_attr/variable_dim 1.1346089839935303
0: read and bcast: trainset/pos/variable_count 1.3113572597503662
0: read and bcast: trainset/pos/variable_offset 1.4832844734191895
0: read and bcast: trainset/pos/variable_dim 1.492271900177002
0: read and bcast: trainset/energy/variable_count 1.6686370372772217
0: read and bcast: trainset/energy/variable_offset 1.847726583480835
0: read and bcast: trainset/energy/variable_dim 1.857248067855835
0: read and bcast: trainset/forces/variable_count 2.0326170921325684
0: read and bcast: trainset/forces/variable_offset 2.2138876914978027
0: read and bcast: trainset/forces/variable_dim 2.224489688873291
0: read and bcast: trainset/y/variable_count 2.402658462524414
0: read and bcast: trainset/y/variable_offset 2.585338592529297
0: read and bcast: trainset/y/variable_dim 2.5926499366760254
0: Overall time (sec):  2.593935489654541
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.5983309745788574
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007410049438476562
0: read and bcast: valset/x/variable_count 0.007928133010864258
0: read and bcast: valset/x/variable_offset 0.015009880065917969
0: read and bcast: valset/x/variable_dim 0.015166521072387695
0: read and bcast: valset/edge_index/variable_count 0.02250385284423828
0: read and bcast: valset/edge_index/variable_offset 0.029618501663208008
0: read and bcast: valset/edge_index/variable_dim 0.029790878295898438
0: read and bcast: valset/edge_attr/variable_count 0.036895751953125
0: read and bcast: valset/edge_attr/variable_offset 0.04403567314147949
0: read and bcast: valset/edge_attr/variable_dim 0.04420804977416992
0: read and bcast: valset/pos/variable_count 0.05132865905761719
0: read and bcast: valset/pos/variable_offset 0.0582273006439209
0: read and bcast: valset/pos/variable_dim 0.05837893486022949
0: read and bcast: valset/energy/variable_count 0.06545448303222656
0: read and bcast: valset/energy/variable_offset 0.07248234748840332
0: read and bcast: valset/energy/variable_dim 0.07264256477355957
0: read and bcast: valset/forces/variable_count 0.07952404022216797
0: read and bcast: valset/forces/variable_offset 0.08649158477783203
0: read and bcast: valset/forces/variable_dim 0.08664798736572266
0: read and bcast: valset/y/variable_count 0.09345483779907227
0: read and bcast: valset/y/variable_offset 0.1002798080444336
0: read and bcast: valset/y/variable_dim 0.10043811798095703
0: Overall time (sec):  0.1013646125793457
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10419321060180664
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007388591766357422
0: read and bcast: testset/x/variable_count 0.007368803024291992
0: read and bcast: testset/x/variable_offset 0.014516353607177734
0: read and bcast: testset/x/variable_dim 0.014670133590698242
0: read and bcast: testset/edge_index/variable_count 0.0218503475189209
0: read and bcast: testset/edge_index/variable_offset 0.028738021850585938
0: read and bcast: testset/edge_index/variable_dim 0.028908967971801758
0: read and bcast: testset/edge_attr/variable_count 0.035750627517700195
0: read and bcast: testset/edge_attr/variable_offset 0.042816162109375
0: read and bcast: testset/edge_attr/variable_dim 0.042989253997802734
0: read and bcast: testset/pos/variable_count 0.04996967315673828
0: read and bcast: testset/pos/variable_offset 0.05722618103027344
0: read and bcast: testset/pos/variable_dim 0.05739235877990723
0: read and bcast: testset/energy/variable_count 0.06441116333007812
0: read and bcast: testset/energy/variable_offset 0.07131719589233398
0: read and bcast: testset/energy/variable_dim 0.0714881420135498
0: read and bcast: testset/forces/variable_count 0.07855391502380371
0: read and bcast: testset/forces/variable_offset 0.08539342880249023
0: read and bcast: testset/forces/variable_dim 0.08554482460021973
0: read and bcast: testset/y/variable_count 0.09248161315917969
0: read and bcast: testset/y/variable_offset 0.09935474395751953
0: read and bcast: testset/y/variable_dim 0.09950900077819824
0: Overall time (sec):  0.10045123100280762
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10331296920776367
2 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
1 ani1x nsplit: 4460384 6 743398
5 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
9 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
34 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
37 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
35 transition1x nsplit: 8680250 11 789114
38 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
25 alexandria nsplit: 9705384 11 882307
26 alexandria nsplit: 9705384 11 882307
28 alexandria nsplit: 9705384 11 882307
19 alexandria nsplit: 9705384 11 882308
20 alexandria nsplit: 9705384 11 882308
21 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
23 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
0: Adios reading time (sec):  0.4625375270843506
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.1274862289428711
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.1291201114654541
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 228/64000 [00:00<00:28, 2276.96it/s]Loading:   1%|          | 473/64000 [00:00<00:26, 2377.77it/s]Loading:   1%|          | 733/64000 [00:00<00:25, 2475.67it/s]Loading:   2%|▏         | 997/64000 [00:00<00:24, 2537.05it/s]Loading:   2%|▏         | 1259/64000 [00:00<00:24, 2565.56it/s]Loading:   2%|▏         | 1516/64000 [00:00<00:24, 2552.27it/s]Loading:   3%|▎         | 1772/64000 [00:00<00:24, 2541.27it/s]Loading:   3%|▎         | 2027/64000 [00:00<00:24, 2537.29it/s]Loading:   4%|▎         | 2281/64000 [00:00<00:24, 2527.78it/s]Loading:   4%|▍         | 2534/64000 [00:01<00:24, 2525.40it/s]Loading:   4%|▍         | 2787/64000 [00:01<00:24, 2518.15it/s]Loading:   5%|▍         | 3039/64000 [00:01<00:24, 2514.67it/s]Loading:   5%|▌         | 3291/64000 [00:01<00:24, 2513.69it/s]Loading:   6%|▌         | 3543/64000 [00:01<00:24, 2512.08it/s]Loading:   6%|▌         | 3795/64000 [00:01<00:24, 2503.39it/s]Loading:   6%|▋         | 4046/64000 [00:01<00:33, 1770.32it/s]Loading:   7%|▋         | 4296/64000 [00:01<00:30, 1938.41it/s]Loading:   7%|▋         | 4544/64000 [00:01<00:28, 2073.09it/s]Loading:   7%|▋         | 4795/64000 [00:02<00:27, 2185.96it/s]Loading:   8%|▊         | 5043/64000 [00:02<00:26, 2264.13it/s]Loading:   8%|▊         | 5293/64000 [00:02<00:25, 2329.54it/s]Loading:   9%|▊         | 5543/64000 [00:02<00:24, 2377.34it/s]Loading:   9%|▉         | 5795/64000 [00:02<00:24, 2418.18it/s]Loading:   9%|▉         | 6046/64000 [00:02<00:23, 2444.66it/s]Loading:  10%|▉         | 6295/64000 [00:02<00:23, 2457.56it/s]Loading:  10%|█         | 6548/64000 [00:02<00:23, 2477.97it/s]Loading:  11%|█         | 6801/64000 [00:02<00:22, 2490.40it/s]Loading:  11%|█         | 7054/64000 [00:02<00:22, 2500.80it/s]Loading:  11%|█▏        | 7305/64000 [00:03<00:22, 2502.02it/s]Loading:  12%|█▏        | 7557/64000 [00:03<00:22, 2505.44it/s]Loading:  12%|█▏        | 7809/64000 [00:03<00:22, 2507.52it/s]Loading:  13%|█▎        | 8061/64000 [00:03<00:22, 2510.72it/s]Loading:  13%|█▎        | 8313/64000 [00:03<00:22, 2510.67it/s]Loading:  13%|█▎        | 8566/64000 [00:03<00:22, 2513.97it/s]Loading:  14%|█▍        | 8819/64000 [00:03<00:21, 2517.59it/s]Loading:  14%|█▍        | 9071/64000 [00:03<00:21, 2512.33it/s]Loading:  15%|█▍        | 9323/64000 [00:03<00:21, 2514.30it/s]Loading:  15%|█▍        | 9575/64000 [00:03<00:21, 2512.39it/s]Loading:  15%|█▌        | 9827/64000 [00:04<00:21, 2511.50it/s]Loading:  16%|█▌        | 10079/64000 [00:04<00:21, 2502.19it/s]Loading:  16%|█▌        | 10332/64000 [00:04<00:21, 2507.86it/s]Loading:  17%|█▋        | 10583/64000 [00:04<00:21, 2505.77it/s]Loading:  17%|█▋        | 10835/64000 [00:04<00:21, 2507.73it/s]Loading:  17%|█▋        | 11087/64000 [00:04<00:21, 2509.50it/s]Loading:  18%|█▊        | 11338/64000 [00:04<00:21, 2482.32it/s]Loading:  18%|█▊        | 11587/64000 [00:04<00:21, 2481.93it/s]Loading:  18%|█▊        | 11839/64000 [00:04<00:20, 2490.68it/s]Loading:  19%|█▉        | 12092/64000 [00:04<00:20, 2500.22it/s]Loading:  19%|█▉        | 12343/64000 [00:05<00:29, 1737.88it/s]Loading:  20%|█▉        | 12594/64000 [00:05<00:26, 1913.17it/s]Loading:  20%|██        | 12845/64000 [00:05<00:24, 2058.21it/s]Loading:  20%|██        | 13095/64000 [00:05<00:23, 2171.92it/s]Loading:  21%|██        | 13343/64000 [00:05<00:22, 2254.70it/s]Loading:  21%|██        | 13594/64000 [00:05<00:21, 2325.49it/s]Loading:  22%|██▏       | 13844/64000 [00:05<00:21, 2373.80it/s]Loading:  22%|██▏       | 14095/64000 [00:05<00:20, 2412.10it/s]Loading:  22%|██▏       | 14346/64000 [00:06<00:20, 2439.05it/s]Loading:  23%|██▎       | 14597/64000 [00:06<00:20, 2458.69it/s]Loading:  23%|██▎       | 14847/64000 [00:06<00:19, 2470.82it/s]Loading:  24%|██▎       | 15097/64000 [00:06<00:19, 2478.20it/s]Loading:  24%|██▍       | 15347/64000 [00:06<00:19, 2479.70it/s]Loading:  24%|██▍       | 15597/64000 [00:06<00:19, 2483.19it/s]Loading:  25%|██▍       | 15847/64000 [00:06<00:19, 2487.57it/s]Loading:  25%|██▌       | 16098/64000 [00:06<00:19, 2492.05it/s]Loading:  26%|██▌       | 16349/64000 [00:06<00:19, 2496.03it/s]Loading:  26%|██▌       | 16599/64000 [00:06<00:19, 2487.71it/s]Loading:  26%|██▋       | 16850/64000 [00:07<00:18, 2491.52it/s]Loading:  27%|██▋       | 17100/64000 [00:07<00:18, 2492.89it/s]Loading:  27%|██▋       | 17350/64000 [00:07<00:18, 2492.10it/s]Loading:  28%|██▊       | 17601/64000 [00:07<00:18, 2497.33it/s]Loading:  28%|██▊       | 17852/64000 [00:07<00:18, 2499.94it/s]Loading:  28%|██▊       | 18103/64000 [00:07<00:18, 2496.12it/s]Loading:  29%|██▊       | 18353/64000 [00:07<00:18, 2494.21it/s]Loading:  29%|██▉       | 18605/64000 [00:07<00:18, 2499.25it/s]Loading:  29%|██▉       | 18855/64000 [00:07<00:18, 2497.99it/s]Loading:  30%|██▉       | 19106/64000 [00:07<00:17, 2500.49it/s]Loading:  30%|███       | 19357/64000 [00:08<00:17, 2498.04it/s]Loading:  31%|███       | 19609/64000 [00:08<00:17, 2501.74it/s]Loading:  31%|███       | 19860/64000 [00:08<00:17, 2503.59it/s]Loading:  31%|███▏      | 20111/64000 [00:08<00:17, 2501.95it/s]Loading:  32%|███▏      | 20362/64000 [00:08<00:17, 2500.95it/s]Loading:  32%|███▏      | 20613/64000 [00:08<00:17, 2494.98it/s]Loading:  33%|███▎      | 20863/64000 [00:08<00:17, 2495.02it/s]Loading:  33%|███▎      | 21114/64000 [00:08<00:17, 2498.40it/s]Loading:  33%|███▎      | 21365/64000 [00:08<00:17, 2499.37it/s]Loading:  34%|███▍      | 21615/64000 [00:08<00:16, 2495.56it/s]Loading:  34%|███▍      | 21866/64000 [00:09<00:16, 2498.08it/s]Loading:  35%|███▍      | 22116/64000 [00:09<00:16, 2491.37it/s]Loading:  35%|███▍      | 22366/64000 [00:09<00:25, 1632.71it/s]Loading:  35%|███▌      | 22615/64000 [00:09<00:22, 1818.60it/s]Loading:  36%|███▌      | 22862/64000 [00:09<00:20, 1972.76it/s]Loading:  36%|███▌      | 23114/64000 [00:09<00:19, 2109.84it/s]Loading:  36%|███▋      | 23356/64000 [00:09<00:18, 2192.12it/s]Loading:  37%|███▋      | 23606/64000 [00:09<00:17, 2275.17it/s]Loading:  37%|███▋      | 23855/64000 [00:10<00:17, 2335.16it/s]Loading:  38%|███▊      | 24105/64000 [00:10<00:16, 2381.80it/s]Loading:  38%|███▊      | 24356/64000 [00:10<00:16, 2416.64it/s]Loading:  38%|███▊      | 24606/64000 [00:10<00:16, 2439.20it/s]Loading:  39%|███▉      | 24857/64000 [00:10<00:15, 2457.40it/s]Loading:  39%|███▉      | 25106/64000 [00:10<00:15, 2463.90it/s]Loading:  40%|███▉      | 25356/64000 [00:10<00:15, 2473.47it/s]Loading:  40%|████      | 25606/64000 [00:10<00:15, 2479.00it/s]Loading:  40%|████      | 25858/64000 [00:10<00:15, 2488.69it/s]Loading:  41%|████      | 26108/64000 [00:10<00:15, 2487.46it/s]Loading:  41%|████      | 26358/64000 [00:11<00:15, 2488.16it/s]Loading:  42%|████▏     | 26608/64000 [00:11<00:15, 2477.29it/s]Loading:  42%|████▏     | 26859/64000 [00:11<00:14, 2485.97it/s]Loading:  42%|████▏     | 27110/64000 [00:11<00:14, 2490.20it/s]Loading:  43%|████▎     | 27360/64000 [00:11<00:14, 2489.72it/s]Loading:  43%|████▎     | 27610/64000 [00:11<00:14, 2489.71it/s]Loading:  44%|████▎     | 27860/64000 [00:11<00:14, 2487.49it/s]Loading:  44%|████▍     | 28111/64000 [00:11<00:14, 2493.46it/s]Loading:  44%|████▍     | 28362/64000 [00:11<00:14, 2495.51it/s]Loading:  45%|████▍     | 28613/64000 [00:11<00:14, 2499.11it/s]Loading:  45%|████▌     | 28863/64000 [00:12<00:14, 2494.31it/s]Loading:  45%|████▌     | 29113/64000 [00:12<00:13, 2494.85it/s]Loading:  46%|████▌     | 29363/64000 [00:12<00:13, 2490.62it/s]Loading:  46%|████▋     | 29613/64000 [00:12<00:13, 2491.89it/s]Loading:  47%|████▋     | 29864/64000 [00:12<00:13, 2496.14it/s]Loading:  47%|████▋     | 30114/64000 [00:12<00:13, 2495.59it/s]Loading:  47%|████▋     | 30365/64000 [00:12<00:13, 2498.71it/s]Loading:  48%|████▊     | 30615/64000 [00:12<00:13, 2496.54it/s]Loading:  48%|████▊     | 30868/64000 [00:12<00:13, 2504.79it/s]Loading:  49%|████▊     | 31119/64000 [00:12<00:13, 2504.00it/s]Loading:  49%|████▉     | 31370/64000 [00:13<00:13, 2505.12it/s]Loading:  49%|████▉     | 31621/64000 [00:13<00:12, 2496.41it/s]Loading:  50%|████▉     | 31871/64000 [00:13<00:12, 2497.40it/s]Loading:  50%|█████     | 32121/64000 [00:13<00:12, 2495.88it/s]Loading:  51%|█████     | 32371/64000 [00:13<00:12, 2495.36it/s]Loading:  51%|█████     | 32623/64000 [00:13<00:12, 2499.69it/s]Loading:  51%|█████▏    | 32873/64000 [00:13<00:12, 2489.86it/s]Loading:  52%|█████▏    | 33123/64000 [00:13<00:12, 2490.60it/s]Loading:  52%|█████▏    | 33373/64000 [00:13<00:12, 2491.64it/s]Loading:  53%|█████▎    | 33624/64000 [00:13<00:12, 2496.65it/s]Loading:  53%|█████▎    | 33874/64000 [00:14<00:12, 2493.87it/s]Loading:  53%|█████▎    | 34125/64000 [00:14<00:11, 2496.73it/s]Loading:  54%|█████▎    | 34375/64000 [00:14<00:11, 2493.80it/s]Loading:  54%|█████▍    | 34627/64000 [00:14<00:11, 2498.84it/s]Loading:  54%|█████▍    | 34878/64000 [00:14<00:11, 2499.27it/s]Loading:  55%|█████▍    | 35128/64000 [00:14<00:19, 1515.52it/s]Loading:  55%|█████▌    | 35379/64000 [00:14<00:16, 1719.18it/s]Loading:  56%|█████▌    | 35628/64000 [00:14<00:14, 1892.87it/s]Loading:  56%|█████▌    | 35878/64000 [00:15<00:13, 2040.67it/s]Loading:  56%|█████▋    | 36128/64000 [00:15<00:12, 2158.09it/s]Loading:  57%|█████▋    | 36379/64000 [00:15<00:12, 2251.57it/s]Loading:  57%|█████▋    | 36629/64000 [00:15<00:11, 2320.47it/s]Loading:  58%|█████▊    | 36881/64000 [00:15<00:11, 2375.94it/s]Loading:  58%|█████▊    | 37131/64000 [00:15<00:11, 2410.02it/s]Loading:  58%|█████▊    | 37380/64000 [00:15<00:10, 2432.46it/s]Loading:  59%|█████▉    | 37631/64000 [00:15<00:10, 2454.00it/s]Loading:  59%|█████▉    | 37881/64000 [00:15<00:10, 2467.16it/s]Loading:  60%|█████▉    | 38133/64000 [00:15<00:10, 2480.01it/s]Loading:  60%|█████▉    | 38384/64000 [00:16<00:10, 2488.75it/s]Loading:  60%|██████    | 38635/64000 [00:16<00:10, 2493.45it/s]Loading:  61%|██████    | 38886/64000 [00:16<00:10, 2493.29it/s]Loading:  61%|██████    | 39137/64000 [00:16<00:09, 2496.54it/s]Loading:  62%|██████▏   | 39388/64000 [00:16<00:09, 2496.57it/s]Loading:  62%|██████▏   | 39638/64000 [00:16<00:09, 2490.71it/s]Loading:  62%|██████▏   | 39888/64000 [00:16<00:09, 2486.28it/s]Loading:  63%|██████▎   | 40138/64000 [00:16<00:09, 2487.59it/s]Loading:  63%|██████▎   | 40388/64000 [00:16<00:09, 2491.20it/s]Loading:  63%|██████▎   | 40638/64000 [00:16<00:09, 2492.77it/s]Loading:  64%|██████▍   | 40889/64000 [00:17<00:09, 2496.96it/s]Loading:  64%|██████▍   | 41139/64000 [00:17<00:09, 2494.10it/s]Loading:  65%|██████▍   | 41389/64000 [00:17<00:09, 2493.40it/s]Loading:  65%|██████▌   | 41639/64000 [00:17<00:08, 2493.62it/s]Loading:  65%|██████▌   | 41891/64000 [00:17<00:08, 2499.61it/s]Loading:  66%|██████▌   | 42141/64000 [00:17<00:08, 2498.96it/s]Loading:  66%|██████▌   | 42392/64000 [00:17<00:08, 2499.76it/s]Loading:  67%|██████▋   | 42642/64000 [00:17<00:08, 2488.78it/s]Loading:  67%|██████▋   | 42892/64000 [00:17<00:08, 2490.28it/s]Loading:  67%|██████▋   | 43143/64000 [00:17<00:08, 2495.22it/s]Loading:  68%|██████▊   | 43394/64000 [00:18<00:08, 2498.77it/s]Loading:  68%|██████▊   | 43645/64000 [00:18<00:08, 2501.79it/s]Loading:  69%|██████▊   | 43896/64000 [00:18<00:08, 2503.26it/s]Loading:  69%|██████▉   | 44147/64000 [00:18<00:07, 2503.09it/s]Loading:  69%|██████▉   | 44398/64000 [00:18<00:07, 2499.94it/s]Loading:  70%|██████▉   | 44648/64000 [00:18<00:07, 2497.54it/s]Loading:  70%|███████   | 44898/64000 [00:18<00:07, 2495.57it/s]Loading:  71%|███████   | 45148/64000 [00:18<00:07, 2494.69it/s]Loading:  71%|███████   | 45398/64000 [00:18<00:07, 2489.49it/s]Loading:  71%|███████▏  | 45649/64000 [00:18<00:07, 2494.64it/s]Loading:  72%|███████▏  | 45900/64000 [00:19<00:07, 2496.80it/s]Loading:  72%|███████▏  | 46150/64000 [00:19<00:07, 2487.64it/s]Loading:  72%|███████▎  | 46400/64000 [00:19<00:07, 2490.02it/s]Loading:  73%|███████▎  | 46650/64000 [00:19<00:06, 2484.48it/s]Loading:  73%|███████▎  | 46899/64000 [00:19<00:06, 2484.64it/s]Loading:  74%|███████▎  | 47148/64000 [00:19<00:06, 2482.30it/s]Loading:  74%|███████▍  | 47397/64000 [00:19<00:06, 2479.91it/s]Loading:  74%|███████▍  | 47646/64000 [00:19<00:06, 2481.09it/s]Loading:  75%|███████▍  | 47896/64000 [00:19<00:06, 2484.79it/s]Loading:  75%|███████▌  | 48146/64000 [00:19<00:06, 2486.96it/s]Loading:  76%|███████▌  | 48395/64000 [00:20<00:06, 2487.63it/s]Loading:  76%|███████▌  | 48645/64000 [00:20<00:06, 2489.60it/s]Loading:  76%|███████▋  | 48895/64000 [00:20<00:06, 2491.61it/s]Loading:  77%|███████▋  | 49146/64000 [00:20<00:05, 2496.67it/s]Loading:  77%|███████▋  | 49396/64000 [00:20<00:05, 2489.20it/s]Loading:  78%|███████▊  | 49645/64000 [00:20<00:05, 2488.44it/s]Loading:  78%|███████▊  | 49894/64000 [00:20<00:05, 2485.25it/s]Loading:  78%|███████▊  | 50145/64000 [00:20<00:05, 2489.84it/s]Loading:  79%|███████▊  | 50394/64000 [00:20<00:05, 2487.05it/s]Loading:  79%|███████▉  | 50645/64000 [00:20<00:05, 2492.90it/s]Loading:  80%|███████▉  | 50895/64000 [00:21<00:05, 2492.88it/s]Loading:  80%|███████▉  | 51145/64000 [00:21<00:09, 1385.99it/s]Loading:  80%|████████  | 51394/64000 [00:21<00:07, 1596.98it/s]Loading:  81%|████████  | 51646/64000 [00:21<00:06, 1794.60it/s]Loading:  81%|████████  | 51899/64000 [00:21<00:06, 1966.27it/s]Loading:  81%|████████▏ | 52150/64000 [00:21<00:05, 2101.13it/s]Loading:  82%|████████▏ | 52401/64000 [00:21<00:05, 2207.68it/s]Loading:  82%|████████▏ | 52653/64000 [00:22<00:04, 2292.50it/s]Loading:  83%|████████▎ | 52907/64000 [00:22<00:04, 2360.36it/s]Loading:  83%|████████▎ | 53157/64000 [00:22<00:04, 2400.10it/s]Loading:  83%|████████▎ | 53408/64000 [00:22<00:04, 2429.62it/s]Loading:  84%|████████▍ | 53657/64000 [00:22<00:04, 2447.28it/s]Loading:  84%|████████▍ | 53906/64000 [00:22<00:04, 2457.73it/s]Loading:  85%|████████▍ | 54157/64000 [00:22<00:03, 2471.36it/s]Loading:  85%|████████▌ | 54410/64000 [00:22<00:03, 2485.95it/s]Loading:  85%|████████▌ | 54661/64000 [00:22<00:03, 2491.57it/s]Loading:  86%|████████▌ | 54912/64000 [00:22<00:03, 2492.86it/s]Loading:  86%|████████▌ | 55163/64000 [00:23<00:03, 2497.67it/s]Loading:  87%|████████▋ | 55414/64000 [00:23<00:03, 2498.79it/s]Loading:  87%|████████▋ | 55667/64000 [00:23<00:03, 2506.22it/s]Loading:  87%|████████▋ | 55918/64000 [00:23<00:03, 2505.39it/s]Loading:  88%|████████▊ | 56171/64000 [00:23<00:03, 2511.37it/s]Loading:  88%|████████▊ | 56423/64000 [00:23<00:03, 2510.04it/s]Loading:  89%|████████▊ | 56675/64000 [00:23<00:02, 2503.26it/s]Loading:  89%|████████▉ | 56927/64000 [00:23<00:02, 2505.58it/s]Loading:  89%|████████▉ | 57178/64000 [00:23<00:02, 2502.25it/s]Loading:  90%|████████▉ | 57431/64000 [00:23<00:02, 2508.85it/s]Loading:  90%|█████████ | 57685/64000 [00:24<00:02, 2515.35it/s]Loading:  91%|█████████ | 57937/64000 [00:24<00:02, 2511.28it/s]Loading:  91%|█████████ | 58189/64000 [00:24<00:02, 2502.30it/s]Loading:  91%|█████████▏| 58440/64000 [00:24<00:02, 2504.13it/s]Loading:  92%|█████████▏| 58691/64000 [00:24<00:02, 2499.55it/s]Loading:  92%|█████████▏| 58943/64000 [00:24<00:02, 2504.47it/s]Loading:  92%|█████████▏| 59195/64000 [00:24<00:01, 2506.38it/s]Loading:  93%|█████████▎| 59447/64000 [00:24<00:01, 2509.57it/s]Loading:  93%|█████████▎| 59700/64000 [00:24<00:01, 2514.09it/s]Loading:  94%|█████████▎| 59952/64000 [00:24<00:01, 2508.40it/s]Loading:  94%|█████████▍| 60203/64000 [00:25<00:01, 2508.79it/s]Loading:  94%|█████████▍| 60454/64000 [00:25<00:01, 2506.96it/s]Loading:  95%|█████████▍| 60705/64000 [00:25<00:01, 2505.57it/s]Loading:  95%|█████████▌| 60956/64000 [00:25<00:01, 2506.00it/s]Loading:  96%|█████████▌| 61208/64000 [00:25<00:01, 2509.33it/s]Loading:  96%|█████████▌| 61459/64000 [00:25<00:01, 2503.57it/s]Loading:  96%|█████████▋| 61710/64000 [00:25<00:00, 2500.57it/s]Loading:  97%|█████████▋| 61962/64000 [00:25<00:00, 2503.69it/s]Loading:  97%|█████████▋| 62214/64000 [00:25<00:00, 2507.29it/s]Loading:  98%|█████████▊| 62465/64000 [00:25<00:00, 2505.71it/s]Loading:  98%|█████████▊| 62716/64000 [00:26<00:00, 2504.08it/s]Loading:  98%|█████████▊| 62968/64000 [00:26<00:00, 2505.98it/s]Loading:  99%|█████████▉| 63219/64000 [00:26<00:00, 2497.70it/s]Loading:  99%|█████████▉| 63470/64000 [00:26<00:00, 2499.42it/s]Loading: 100%|█████████▉| 63720/64000 [00:26<00:00, 2495.93it/s]Loading: 100%|█████████▉| 63970/64000 [00:26<00:00, 2490.96it/s]Loading: 100%|██████████| 64000/64000 [00:26<00:00, 2409.84it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 255/49018 [00:00<00:19, 2543.80it/s]Loading:   1%|          | 518/49018 [00:00<00:18, 2594.30it/s]Loading:   2%|▏         | 780/49018 [00:00<00:18, 2604.90it/s]Loading:   2%|▏         | 1043/49018 [00:00<00:18, 2614.30it/s]Loading:   3%|▎         | 1307/49018 [00:00<00:18, 2621.97it/s]Loading:   3%|▎         | 1571/49018 [00:00<00:18, 2626.80it/s]Loading:   4%|▎         | 1835/49018 [00:00<00:17, 2627.59it/s]Loading:   4%|▍         | 2099/49018 [00:00<00:17, 2629.39it/s]Loading:   5%|▍         | 2364/49018 [00:00<00:17, 2632.83it/s]Loading:   5%|▌         | 2628/49018 [00:01<00:17, 2629.68it/s]Loading:   6%|▌         | 2891/49018 [00:01<00:17, 2628.05it/s]Loading:   6%|▋         | 3155/49018 [00:01<00:17, 2629.67it/s]Loading:   7%|▋         | 3419/49018 [00:01<00:17, 2631.12it/s]Loading:   8%|▊         | 3684/49018 [00:01<00:17, 2634.84it/s]Loading:   8%|▊         | 3948/49018 [00:01<00:17, 2635.49it/s]Loading:   9%|▊         | 4212/49018 [00:01<00:17, 2630.78it/s]Loading:   9%|▉         | 4477/49018 [00:01<00:16, 2636.17it/s]Loading:  10%|▉         | 4741/49018 [00:01<00:16, 2631.66it/s]Loading:  10%|█         | 5005/49018 [00:01<00:16, 2632.80it/s]Loading:  11%|█         | 5269/49018 [00:02<00:16, 2634.31it/s]Loading:  11%|█▏        | 5533/49018 [00:02<00:16, 2635.54it/s]Loading:  12%|█▏        | 5797/49018 [00:02<00:16, 2630.47it/s]Loading:  12%|█▏        | 6061/49018 [00:02<00:16, 2603.94it/s]Loading:  13%|█▎        | 6326/49018 [00:02<00:16, 2616.48it/s]Loading:  13%|█▎        | 6588/49018 [00:02<00:16, 2617.34it/s]Loading:  14%|█▍        | 6853/49018 [00:02<00:16, 2625.14it/s]Loading:  15%|█▍        | 7117/49018 [00:02<00:15, 2628.90it/s]Loading:  15%|█▌        | 7380/49018 [00:02<00:15, 2628.64it/s]Loading:  16%|█▌        | 7643/49018 [00:02<00:15, 2621.09it/s]Loading:  16%|█▌        | 7906/49018 [00:03<00:15, 2623.08it/s]Loading:  17%|█▋        | 8169/49018 [00:03<00:15, 2620.55it/s]Loading:  17%|█▋        | 8433/49018 [00:03<00:15, 2623.05it/s]Loading:  18%|█▊        | 8696/49018 [00:03<00:15, 2617.90it/s]Loading:  18%|█▊        | 8958/49018 [00:03<00:15, 2616.54it/s]Loading:  19%|█▉        | 9221/49018 [00:03<00:15, 2617.60it/s]Loading:  19%|█▉        | 9486/49018 [00:03<00:15, 2624.39it/s]Loading:  20%|█▉        | 9749/49018 [00:03<00:14, 2623.23it/s]Loading:  20%|██        | 10012/49018 [00:03<00:14, 2624.81it/s]Loading:  21%|██        | 10275/49018 [00:03<00:14, 2624.59it/s]Loading:  22%|██▏       | 10539/49018 [00:04<00:14, 2626.91it/s]Loading:  22%|██▏       | 10802/49018 [00:04<00:14, 2622.13it/s]Loading:  23%|██▎       | 11067/49018 [00:04<00:14, 2628.09it/s]Loading:  23%|██▎       | 11330/49018 [00:04<00:14, 2625.97it/s]Loading:  24%|██▎       | 11593/49018 [00:04<00:14, 2624.23it/s]Loading:  24%|██▍       | 11856/49018 [00:04<00:14, 2623.16it/s]Loading:  25%|██▍       | 12119/49018 [00:04<00:14, 2619.45it/s]Loading:  25%|██▌       | 12381/49018 [00:04<00:13, 2619.25it/s]Loading:  26%|██▌       | 12643/49018 [00:04<00:13, 2614.77it/s]Loading:  26%|██▋       | 12905/49018 [00:04<00:13, 2613.11it/s]Loading:  27%|██▋       | 13167/49018 [00:05<00:13, 2608.33it/s]Loading:  27%|██▋       | 13428/49018 [00:05<00:13, 2593.90it/s]Loading:  28%|██▊       | 13688/49018 [00:05<00:13, 2581.84it/s]Loading:  28%|██▊       | 13947/49018 [00:05<00:13, 2572.16it/s]Loading:  29%|██▉       | 14205/49018 [00:05<00:13, 2559.43it/s]Loading:  30%|██▉       | 14461/49018 [00:05<00:13, 2558.90it/s]Loading:  30%|███       | 14717/49018 [00:05<00:13, 2557.09it/s]Loading:  31%|███       | 14976/49018 [00:05<00:13, 2564.83it/s]Loading:  31%|███       | 15233/49018 [00:05<00:13, 2560.57it/s]Loading:  32%|███▏      | 15490/49018 [00:05<00:13, 2559.74it/s]Loading:  32%|███▏      | 15746/49018 [00:06<00:13, 2557.83it/s]Loading:  33%|███▎      | 16003/49018 [00:06<00:12, 2559.77it/s]Loading:  33%|███▎      | 16261/49018 [00:06<00:12, 2564.88it/s]Loading:  34%|███▎      | 16518/49018 [00:06<00:12, 2563.28it/s]Loading:  34%|███▍      | 16775/49018 [00:06<00:12, 2563.75it/s]Loading:  35%|███▍      | 17032/49018 [00:06<00:12, 2556.54it/s]Loading:  35%|███▌      | 17290/49018 [00:06<00:12, 2563.49it/s]Loading:  36%|███▌      | 17548/49018 [00:06<00:12, 2565.24it/s]Loading:  36%|███▋      | 17805/49018 [00:06<00:12, 2561.45it/s]Loading:  37%|███▋      | 18062/49018 [00:07<00:24, 1266.49it/s]Loading:  37%|███▋      | 18316/49018 [00:07<00:20, 1487.74it/s]Loading:  38%|███▊      | 18569/49018 [00:07<00:17, 1694.31it/s]Loading:  38%|███▊      | 18826/49018 [00:07<00:16, 1886.86it/s]Loading:  39%|███▉      | 19082/49018 [00:07<00:14, 2046.50it/s]Loading:  39%|███▉      | 19338/49018 [00:07<00:13, 2175.76it/s]Loading:  40%|███▉      | 19591/49018 [00:07<00:12, 2268.72it/s]Loading:  40%|████      | 19845/49018 [00:07<00:12, 2343.14it/s]Loading:  41%|████      | 20101/49018 [00:08<00:12, 2402.69it/s]Loading:  42%|████▏     | 20357/49018 [00:08<00:11, 2447.59it/s]Loading:  42%|████▏     | 20613/49018 [00:08<00:11, 2478.94it/s]Loading:  43%|████▎     | 20868/49018 [00:08<00:11, 2499.12it/s]Loading:  43%|████▎     | 21126/49018 [00:08<00:11, 2522.69it/s]Loading:  44%|████▎     | 21383/49018 [00:08<00:10, 2535.91it/s]Loading:  44%|████▍     | 21639/49018 [00:08<00:10, 2539.90it/s]Loading:  45%|████▍     | 21895/49018 [00:08<00:10, 2542.39it/s]Loading:  45%|████▌     | 22151/49018 [00:08<00:10, 2542.08it/s]Loading:  46%|████▌     | 22406/49018 [00:08<00:10, 2539.17it/s]Loading:  46%|████▌     | 22662/49018 [00:09<00:10, 2544.03it/s]Loading:  47%|████▋     | 22917/49018 [00:09<00:10, 2540.34it/s]Loading:  47%|████▋     | 23174/49018 [00:09<00:10, 2547.28it/s]Loading:  48%|████▊     | 23430/49018 [00:09<00:10, 2549.89it/s]Loading:  48%|████▊     | 23686/49018 [00:09<00:09, 2548.39it/s]Loading:  49%|████▉     | 23942/49018 [00:09<00:09, 2549.32it/s]Loading:  49%|████▉     | 24197/49018 [00:09<00:09, 2548.84it/s]Loading:  50%|████▉     | 24453/49018 [00:09<00:09, 2552.02it/s]Loading:  50%|█████     | 24709/49018 [00:09<00:09, 2540.92it/s]Loading:  51%|█████     | 24966/49018 [00:09<00:09, 2548.94it/s]Loading:  51%|█████▏    | 25223/49018 [00:10<00:09, 2555.10it/s]Loading:  52%|█████▏    | 25481/49018 [00:10<00:09, 2559.78it/s]Loading:  53%|█████▎    | 25738/49018 [00:10<00:09, 2562.21it/s]Loading:  53%|█████▎    | 25996/49018 [00:10<00:08, 2567.06it/s]Loading:  54%|█████▎    | 26254/49018 [00:10<00:08, 2569.89it/s]Loading:  54%|█████▍    | 26511/49018 [00:10<00:08, 2566.58it/s]Loading:  55%|█████▍    | 26769/49018 [00:10<00:08, 2569.64it/s]Loading:  55%|█████▌    | 27028/49018 [00:10<00:08, 2573.33it/s]Loading:  56%|█████▌    | 27287/49018 [00:10<00:08, 2575.91it/s]Loading:  56%|█████▌    | 27545/49018 [00:10<00:08, 2568.58it/s]Loading:  57%|█████▋    | 27804/49018 [00:11<00:08, 2574.26it/s]Loading:  57%|█████▋    | 28062/49018 [00:11<00:08, 2572.22it/s]Loading:  58%|█████▊    | 28320/49018 [00:11<00:08, 2572.23it/s]Loading:  58%|█████▊    | 28578/49018 [00:11<00:07, 2569.25it/s]Loading:  59%|█████▉    | 28835/49018 [00:11<00:07, 2566.10it/s]Loading:  59%|█████▉    | 29092/49018 [00:11<00:07, 2566.83it/s]Loading:  60%|█████▉    | 29351/49018 [00:11<00:07, 2573.20it/s]Loading:  60%|██████    | 29609/49018 [00:11<00:07, 2571.38it/s]Loading:  61%|██████    | 29867/49018 [00:11<00:07, 2571.98it/s]Loading:  61%|██████▏   | 30125/49018 [00:11<00:07, 2565.72it/s]Loading:  62%|██████▏   | 30383/49018 [00:12<00:07, 2569.02it/s]Loading:  63%|██████▎   | 30642/49018 [00:12<00:07, 2573.80it/s]Loading:  63%|██████▎   | 30900/49018 [00:12<00:07, 2574.19it/s]Loading:  64%|██████▎   | 31158/49018 [00:12<00:06, 2573.83it/s]Loading:  64%|██████▍   | 31416/49018 [00:12<00:06, 2569.70it/s]Loading:  65%|██████▍   | 31674/49018 [00:12<00:06, 2571.91it/s]Loading:  65%|██████▌   | 31932/49018 [00:12<00:06, 2569.59it/s]Loading:  66%|██████▌   | 32189/49018 [00:12<00:06, 2569.13it/s]Loading:  66%|██████▌   | 32447/49018 [00:12<00:06, 2569.45it/s]Loading:  67%|██████▋   | 32705/49018 [00:12<00:06, 2571.14it/s]Loading:  67%|██████▋   | 32963/49018 [00:13<00:06, 2573.06it/s]Loading:  68%|██████▊   | 33221/49018 [00:13<00:06, 2572.24it/s]Loading:  68%|██████▊   | 33479/49018 [00:13<00:06, 2573.01it/s]Loading:  69%|██████▉   | 33737/49018 [00:13<00:05, 2566.47it/s]Loading:  69%|██████▉   | 33994/49018 [00:13<00:05, 2564.27it/s]Loading:  70%|██████▉   | 34251/49018 [00:13<00:05, 2537.76it/s]Loading:  70%|███████   | 34510/49018 [00:13<00:05, 2551.74it/s]Loading:  71%|███████   | 34769/49018 [00:13<00:05, 2560.40it/s]Loading:  71%|███████▏  | 35028/49018 [00:13<00:05, 2568.18it/s]Loading:  72%|███████▏  | 35285/49018 [00:14<00:05, 2557.21it/s]Loading:  73%|███████▎  | 35541/49018 [00:14<00:05, 2556.54it/s]Loading:  73%|███████▎  | 35799/49018 [00:14<00:05, 2562.51it/s]Loading:  74%|███████▎  | 36058/49018 [00:14<00:05, 2569.30it/s]Loading:  74%|███████▍  | 36315/49018 [00:14<00:04, 2568.71it/s]Loading:  75%|███████▍  | 36572/49018 [00:14<00:04, 2566.90it/s]Loading:  75%|███████▌  | 36829/49018 [00:14<00:04, 2565.99it/s]Loading:  76%|███████▌  | 37087/49018 [00:14<00:04, 2567.79it/s]Loading:  76%|███████▌  | 37345/49018 [00:14<00:04, 2569.22it/s]Loading:  77%|███████▋  | 37604/49018 [00:14<00:04, 2575.42it/s]Loading:  77%|███████▋  | 37862/49018 [00:15<00:04, 2573.07it/s]Loading:  78%|███████▊  | 38121/49018 [00:15<00:04, 2576.81it/s]Loading:  78%|███████▊  | 38381/49018 [00:15<00:04, 2581.69it/s]Loading:  79%|███████▉  | 38640/49018 [00:15<00:04, 2580.09it/s]Loading:  79%|███████▉  | 38899/49018 [00:15<00:03, 2569.60it/s]Loading:  80%|███████▉  | 39156/49018 [00:15<00:03, 2564.06it/s]Loading:  80%|████████  | 39413/49018 [00:15<00:03, 2565.53it/s]Loading:  81%|████████  | 39670/49018 [00:15<00:03, 2566.24it/s]Loading:  81%|████████▏ | 39927/49018 [00:15<00:03, 2559.26it/s]Loading:  82%|████████▏ | 40183/49018 [00:15<00:03, 2557.44it/s]Loading:  83%|████████▎ | 40441/49018 [00:16<00:03, 2561.53it/s]Loading:  83%|████████▎ | 40698/49018 [00:16<00:03, 2556.63it/s]Loading:  84%|████████▎ | 40957/49018 [00:16<00:03, 2564.43it/s]Loading:  84%|████████▍ | 41214/49018 [00:16<00:03, 2560.39it/s]Loading:  85%|████████▍ | 41471/49018 [00:16<00:02, 2558.73it/s]Loading:  85%|████████▌ | 41727/49018 [00:16<00:02, 2553.99it/s]Loading:  86%|████████▌ | 41984/49018 [00:16<00:02, 2557.83it/s]Loading:  86%|████████▌ | 42243/49018 [00:16<00:02, 2566.70it/s]Loading:  87%|████████▋ | 42502/49018 [00:16<00:02, 2571.02it/s]Loading:  87%|████████▋ | 42761/49018 [00:16<00:02, 2574.62it/s]Loading:  88%|████████▊ | 43019/49018 [00:17<00:02, 2566.81it/s]Loading:  88%|████████▊ | 43276/49018 [00:17<00:02, 2563.62it/s]Loading:  89%|████████▉ | 43533/49018 [00:17<00:02, 2560.99it/s]Loading:  89%|████████▉ | 43790/49018 [00:17<00:02, 2563.11it/s]Loading:  90%|████████▉ | 44047/49018 [00:17<00:01, 2549.64it/s]Loading:  90%|█████████ | 44304/49018 [00:17<00:01, 2554.35it/s]Loading:  91%|█████████ | 44561/49018 [00:17<00:01, 2556.15it/s]Loading:  91%|█████████▏| 44817/49018 [00:17<00:01, 2556.20it/s]Loading:  92%|█████████▏| 45076/49018 [00:17<00:01, 2565.17it/s]Loading:  92%|█████████▏| 45333/49018 [00:17<00:01, 2566.36it/s]Loading:  93%|█████████▎| 45590/49018 [00:18<00:01, 2563.24it/s]Loading:  94%|█████████▎| 45847/49018 [00:18<00:02, 1135.00it/s]Loading:  94%|█████████▍| 46100/49018 [00:18<00:02, 1356.46it/s]Loading:  95%|█████████▍| 46350/49018 [00:18<00:01, 1567.24it/s]Loading:  95%|█████████▌| 46606/49018 [00:18<00:01, 1773.76it/s]Loading:  96%|█████████▌| 46861/49018 [00:18<00:01, 1951.07it/s]Loading:  96%|█████████▌| 47118/49018 [00:19<00:00, 2103.39it/s]Loading:  97%|█████████▋| 47373/49018 [00:19<00:00, 2217.95it/s]Loading:  97%|█████████▋| 47628/49018 [00:19<00:00, 2308.02it/s]Loading:  98%|█████████▊| 47885/49018 [00:19<00:00, 2378.77it/s]Loading:  98%|█████████▊| 48141/49018 [00:19<00:00, 2430.35it/s]Loading:  99%|█████████▊| 48398/49018 [00:19<00:00, 2469.28it/s]Loading:  99%|█████████▉| 48653/49018 [00:19<00:00, 2490.08it/s]Loading: 100%|█████████▉| 48908/49018 [00:19<00:00, 2507.49it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2477.20it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   1%|          | 267/49018 [00:00<00:18, 2665.10it/s]Loading:   1%|          | 537/49018 [00:00<00:18, 2682.41it/s]Loading:   2%|▏         | 806/49018 [00:00<00:17, 2683.06it/s]Loading:   2%|▏         | 1076/49018 [00:00<00:17, 2686.60it/s]Loading:   3%|▎         | 1345/49018 [00:00<00:17, 2686.30it/s]Loading:   3%|▎         | 1615/49018 [00:00<00:17, 2690.71it/s]Loading:   4%|▍         | 1885/49018 [00:01<00:42, 1100.62it/s]Loading:   4%|▍         | 2152/49018 [00:01<00:34, 1351.50it/s]Loading:   5%|▍         | 2418/49018 [00:01<00:29, 1594.29it/s]Loading:   5%|▌         | 2686/49018 [00:01<00:25, 1821.27it/s]Loading:   6%|▌         | 2954/49018 [00:01<00:22, 2018.05it/s]Loading:   7%|▋         | 3222/49018 [00:01<00:20, 2181.66it/s]Loading:   7%|▋         | 3490/49018 [00:01<00:19, 2309.82it/s]Loading:   8%|▊         | 3755/49018 [00:01<00:18, 2401.65it/s]Loading:   8%|▊         | 4024/49018 [00:01<00:18, 2480.30it/s]Loading:   9%|▊         | 4289/49018 [00:02<00:17, 2528.13it/s]Loading:   9%|▉         | 4557/49018 [00:02<00:17, 2571.43it/s]Loading:  10%|▉         | 4824/49018 [00:02<00:17, 2598.12it/s]Loading:  10%|█         | 5090/49018 [00:02<00:16, 2593.43it/s]Loading:  11%|█         | 5357/49018 [00:02<00:16, 2615.45it/s]Loading:  11%|█▏        | 5626/49018 [00:02<00:16, 2635.57it/s]Loading:  12%|█▏        | 5893/49018 [00:02<00:16, 2643.05it/s]Loading:  13%|█▎        | 6160/49018 [00:02<00:16, 2650.55it/s]Loading:  13%|█▎        | 6427/49018 [00:02<00:16, 2649.07it/s]Loading:  14%|█▎        | 6693/49018 [00:02<00:15, 2650.14it/s]Loading:  14%|█▍        | 6959/49018 [00:03<00:15, 2645.21it/s]Loading:  15%|█▍        | 7225/49018 [00:03<00:15, 2647.95it/s]Loading:  15%|█▌        | 7491/49018 [00:03<00:15, 2644.51it/s]Loading:  16%|█▌        | 7756/49018 [00:03<00:15, 2643.84it/s]Loading:  16%|█▋        | 8022/49018 [00:03<00:15, 2646.88it/s]Loading:  17%|█▋        | 8287/49018 [00:03<00:15, 2646.61it/s]Loading:  17%|█▋        | 8552/49018 [00:03<00:15, 2642.93it/s]Loading:  18%|█▊        | 8818/49018 [00:03<00:15, 2646.85it/s]Loading:  19%|█▊        | 9083/49018 [00:03<00:15, 2639.10it/s]Loading:  19%|█▉        | 9349/49018 [00:03<00:15, 2643.28it/s]Loading:  20%|█▉        | 9614/49018 [00:04<00:14, 2641.32it/s]Loading:  20%|██        | 9879/49018 [00:04<00:14, 2642.63it/s]Loading:  21%|██        | 10144/49018 [00:04<00:14, 2643.07it/s]Loading:  21%|██        | 10409/49018 [00:04<00:14, 2641.03it/s]Loading:  22%|██▏       | 10674/49018 [00:04<00:14, 2642.28it/s]Loading:  22%|██▏       | 10940/49018 [00:04<00:14, 2646.32it/s]Loading:  23%|██▎       | 11207/49018 [00:04<00:14, 2651.76it/s]Loading:  23%|██▎       | 11473/49018 [00:04<00:14, 2651.99it/s]Loading:  24%|██▍       | 11739/49018 [00:04<00:14, 2651.61it/s]Loading:  24%|██▍       | 12005/49018 [00:04<00:13, 2650.56it/s]Loading:  25%|██▌       | 12271/49018 [00:05<00:13, 2645.03it/s]Loading:  26%|██▌       | 12536/49018 [00:05<00:13, 2646.00it/s]Loading:  26%|██▌       | 12803/49018 [00:05<00:13, 2650.38it/s]Loading:  27%|██▋       | 13069/49018 [00:05<00:13, 2646.05it/s]Loading:  27%|██▋       | 13335/49018 [00:05<00:13, 2647.24it/s]Loading:  28%|██▊       | 13600/49018 [00:05<00:13, 2647.22it/s]Loading:  28%|██▊       | 13866/49018 [00:05<00:13, 2648.43it/s]Loading:  29%|██▉       | 14133/49018 [00:05<00:13, 2652.59it/s]Loading:  29%|██▉       | 14399/49018 [00:05<00:13, 2646.67it/s]Loading:  30%|██▉       | 14665/49018 [00:05<00:12, 2648.37it/s]Loading:  30%|███       | 14931/49018 [00:06<00:12, 2651.11it/s]Loading:  31%|███       | 15197/49018 [00:06<00:12, 2653.22it/s]Loading:  32%|███▏      | 15464/49018 [00:06<00:12, 2657.32it/s]Loading:  32%|███▏      | 15730/49018 [00:06<00:12, 2654.86it/s]Loading:  33%|███▎      | 15996/49018 [00:06<00:12, 2650.03it/s]Loading:  33%|███▎      | 16262/49018 [00:06<00:12, 2645.67it/s]Loading:  34%|███▎      | 16528/49018 [00:06<00:12, 2647.67it/s]Loading:  34%|███▍      | 16793/49018 [00:06<00:12, 2647.40it/s]Loading:  35%|███▍      | 17058/49018 [00:06<00:12, 2644.90it/s]Loading:  35%|███▌      | 17323/49018 [00:06<00:12, 2639.23it/s]Loading:  36%|███▌      | 17588/49018 [00:07<00:11, 2640.65it/s]Loading:  36%|███▋      | 17853/49018 [00:07<00:11, 2640.67it/s]Loading:  37%|███▋      | 18118/49018 [00:07<00:11, 2627.66it/s]Loading:  37%|███▋      | 18381/49018 [00:07<00:11, 2610.95it/s]Loading:  38%|███▊      | 18643/49018 [00:07<00:11, 2590.65it/s]Loading:  39%|███▊      | 18903/49018 [00:07<00:11, 2587.28it/s]Loading:  39%|███▉      | 19162/49018 [00:07<00:11, 2577.53it/s]Loading:  40%|███▉      | 19422/49018 [00:07<00:11, 2582.39it/s]Loading:  40%|████      | 19681/49018 [00:07<00:11, 2574.43it/s]Loading:  41%|████      | 19939/49018 [00:07<00:11, 2573.51it/s]Loading:  41%|████      | 20197/49018 [00:08<00:11, 2571.85it/s]Loading:  42%|████▏     | 20455/49018 [00:08<00:11, 2571.28it/s]Loading:  42%|████▏     | 20714/49018 [00:08<00:10, 2573.86it/s]Loading:  43%|████▎     | 20972/49018 [00:08<00:10, 2574.83it/s]Loading:  43%|████▎     | 21230/49018 [00:08<00:10, 2568.11it/s]Loading:  44%|████▍     | 21489/49018 [00:08<00:10, 2572.40it/s]Loading:  44%|████▍     | 21747/49018 [00:08<00:10, 2568.15it/s]Loading:  45%|████▍     | 22006/49018 [00:08<00:10, 2573.71it/s]Loading:  45%|████▌     | 22264/49018 [00:08<00:10, 2574.03it/s]Loading:  46%|████▌     | 22522/49018 [00:08<00:10, 2572.93it/s]Loading:  46%|████▋     | 22781/49018 [00:09<00:10, 2575.65it/s]Loading:  47%|████▋     | 23039/49018 [00:09<00:10, 2570.70it/s]Loading:  48%|████▊     | 23297/49018 [00:09<00:10, 2566.36it/s]Loading:  48%|████▊     | 23555/49018 [00:09<00:09, 2567.84it/s]Loading:  49%|████▊     | 23812/49018 [00:09<00:09, 2550.77it/s]Loading:  49%|████▉     | 24071/49018 [00:09<00:09, 2559.88it/s]Loading:  50%|████▉     | 24329/49018 [00:09<00:09, 2564.95it/s]Loading:  50%|█████     | 24587/49018 [00:09<00:09, 2567.26it/s]Loading:  51%|█████     | 24845/49018 [00:09<00:09, 2569.46it/s]Loading:  51%|█████     | 25102/49018 [00:09<00:09, 2567.18it/s]Loading:  52%|█████▏    | 25361/49018 [00:10<00:09, 2571.48it/s]Loading:  52%|█████▏    | 25619/49018 [00:10<00:09, 2567.90it/s]Loading:  53%|█████▎    | 25877/49018 [00:10<00:09, 2569.17it/s]Loading:  53%|█████▎    | 26134/49018 [00:10<00:08, 2569.24it/s]Loading:  54%|█████▍    | 26392/49018 [00:10<00:08, 2571.56it/s]Loading:  54%|█████▍    | 26650/49018 [00:10<00:08, 2573.29it/s]Loading:  55%|█████▍    | 26908/49018 [00:10<00:08, 2569.09it/s]Loading:  55%|█████▌    | 27166/49018 [00:10<00:08, 2571.10it/s]Loading:  56%|█████▌    | 27424/49018 [00:10<00:08, 2572.07it/s]Loading:  56%|█████▋    | 27682/49018 [00:10<00:08, 2570.25it/s]Loading:  57%|█████▋    | 27940/49018 [00:11<00:08, 2570.50it/s]Loading:  58%|█████▊    | 28198/49018 [00:11<00:08, 2571.86it/s]Loading:  58%|█████▊    | 28456/49018 [00:11<00:07, 2571.86it/s]Loading:  59%|█████▊    | 28714/49018 [00:11<00:07, 2572.12it/s]Loading:  59%|█████▉    | 28972/49018 [00:11<00:07, 2572.78it/s]Loading:  60%|█████▉    | 29230/49018 [00:11<00:07, 2574.31it/s]Loading:  60%|██████    | 29489/49018 [00:11<00:07, 2576.86it/s]Loading:  61%|██████    | 29748/49018 [00:11<00:07, 2579.56it/s]Loading:  61%|██████    | 30006/49018 [00:11<00:07, 2576.28it/s]Loading:  62%|██████▏   | 30264/49018 [00:11<00:07, 2575.04it/s]Loading:  62%|██████▏   | 30522/49018 [00:12<00:07, 2572.83it/s]Loading:  63%|██████▎   | 30780/49018 [00:12<00:07, 2571.82it/s]Loading:  63%|██████▎   | 31038/49018 [00:12<00:07, 2566.75it/s]Loading:  64%|██████▍   | 31295/49018 [00:12<00:06, 2566.75it/s]Loading:  64%|██████▍   | 31554/49018 [00:12<00:06, 2571.33it/s]Loading:  65%|██████▍   | 31812/49018 [00:12<00:06, 2571.06it/s]Loading:  65%|██████▌   | 32071/49018 [00:12<00:06, 2576.36it/s]Loading:  66%|██████▌   | 32329/49018 [00:12<00:06, 2577.28it/s]Loading:  66%|██████▋   | 32588/49018 [00:12<00:06, 2579.57it/s]Loading:  67%|██████▋   | 32846/49018 [00:13<00:06, 2573.63it/s]Loading:  68%|██████▊   | 33105/49018 [00:13<00:06, 2576.92it/s]Loading:  68%|██████▊   | 33363/49018 [00:13<00:06, 2574.35it/s]Loading:  69%|██████▊   | 33621/49018 [00:13<00:05, 2574.49it/s]Loading:  69%|██████▉   | 33879/49018 [00:13<00:05, 2568.17it/s]Loading:  70%|██████▉   | 34137/49018 [00:13<00:05, 2569.74it/s]Loading:  70%|███████   | 34395/49018 [00:13<00:05, 2572.08it/s]Loading:  71%|███████   | 34653/49018 [00:13<00:05, 2571.20it/s]Loading:  71%|███████   | 34912/49018 [00:13<00:05, 2574.47it/s]Loading:  72%|███████▏  | 35170/49018 [00:13<00:05, 2570.50it/s]Loading:  72%|███████▏  | 35429/49018 [00:14<00:05, 2574.14it/s]Loading:  73%|███████▎  | 35687/49018 [00:14<00:05, 2572.83it/s]Loading:  73%|███████▎  | 35945/49018 [00:14<00:05, 2569.89it/s]Loading:  74%|███████▍  | 36202/49018 [00:14<00:04, 2569.53it/s]Loading:  74%|███████▍  | 36459/49018 [00:14<00:04, 2567.54it/s]Loading:  75%|███████▍  | 36716/49018 [00:14<00:04, 2566.09it/s]Loading:  75%|███████▌  | 36973/49018 [00:14<00:04, 2535.03it/s]Loading:  76%|███████▌  | 37230/49018 [00:14<00:04, 2544.84it/s]Loading:  76%|███████▋  | 37488/49018 [00:14<00:04, 2554.74it/s]Loading:  77%|███████▋  | 37747/49018 [00:14<00:04, 2562.99it/s]Loading:  78%|███████▊  | 38004/49018 [00:15<00:11, 983.68it/s] Loading:  78%|███████▊  | 38263/49018 [00:15<00:08, 1208.89it/s]Loading:  79%|███████▊  | 38521/49018 [00:15<00:07, 1437.90it/s]Loading:  79%|███████▉  | 38777/49018 [00:15<00:06, 1654.12it/s]Loading:  80%|███████▉  | 39032/49018 [00:15<00:05, 1847.27it/s]Loading:  80%|████████  | 39286/49018 [00:16<00:04, 2009.88it/s]Loading:  81%|████████  | 39540/49018 [00:16<00:04, 2142.84it/s]Loading:  81%|████████  | 39796/49018 [00:16<00:04, 2252.21it/s]Loading:  82%|████████▏ | 40050/49018 [00:16<00:03, 2330.23it/s]Loading:  82%|████████▏ | 40304/49018 [00:16<00:03, 2386.81it/s]Loading:  83%|████████▎ | 40558/49018 [00:16<00:03, 2430.68it/s]Loading:  83%|████████▎ | 40812/49018 [00:16<00:03, 2460.97it/s]Loading:  84%|████████▍ | 41068/49018 [00:16<00:03, 2487.62it/s]Loading:  84%|████████▍ | 41323/49018 [00:16<00:03, 2503.33it/s]Loading:  85%|████████▍ | 41580/49018 [00:16<00:02, 2522.55it/s]Loading:  85%|████████▌ | 41835/49018 [00:17<00:02, 2530.08it/s]Loading:  86%|████████▌ | 42090/49018 [00:17<00:02, 2530.83it/s]Loading:  86%|████████▋ | 42345/49018 [00:17<00:02, 2535.50it/s]Loading:  87%|████████▋ | 42600/49018 [00:17<00:02, 2536.62it/s]Loading:  87%|████████▋ | 42855/49018 [00:17<00:02, 2533.60it/s]Loading:  88%|████████▊ | 43111/49018 [00:17<00:02, 2538.83it/s]Loading:  88%|████████▊ | 43366/49018 [00:17<00:02, 2531.53it/s]Loading:  89%|████████▉ | 43620/49018 [00:17<00:02, 2529.63it/s]Loading:  90%|████████▉ | 43874/49018 [00:17<00:02, 2521.64it/s]Loading:  90%|█████████ | 44129/49018 [00:17<00:01, 2529.81it/s]Loading:  91%|█████████ | 44383/49018 [00:18<00:01, 2532.59it/s]Loading:  91%|█████████ | 44638/49018 [00:18<00:01, 2536.89it/s]Loading:  92%|█████████▏| 44895/49018 [00:18<00:01, 2545.21it/s]Loading:  92%|█████████▏| 45150/49018 [00:18<00:01, 2539.59it/s]Loading:  93%|█████████▎| 45405/49018 [00:18<00:01, 2540.91it/s]Loading:  93%|█████████▎| 45660/49018 [00:18<00:01, 2540.06it/s]Loading:  94%|█████████▎| 45915/49018 [00:18<00:01, 2542.97it/s]Loading:  94%|█████████▍| 46170/49018 [00:18<00:01, 2540.95it/s]Loading:  95%|█████████▍| 46426/49018 [00:18<00:01, 2544.00it/s]Loading:  95%|█████████▌| 46681/49018 [00:18<00:00, 2540.18it/s]Loading:  96%|█████████▌| 46936/49018 [00:19<00:00, 2539.99it/s]Loading:  96%|█████████▋| 47191/49018 [00:19<00:00, 2534.35it/s]Loading:  97%|█████████▋| 47446/49018 [00:19<00:00, 2537.59it/s]Loading:  97%|█████████▋| 47700/49018 [00:19<00:00, 2536.46it/s]Loading:  98%|█████████▊| 47956/49018 [00:19<00:00, 2541.66it/s]Loading:  98%|█████████▊| 48211/49018 [00:19<00:00, 2543.48it/s]Loading:  99%|█████████▉| 48466/49018 [00:19<00:00, 2543.21it/s]Loading:  99%|█████████▉| 48721/49018 [00:19<00:00, 2512.50it/s]Loading: 100%|█████████▉| 48976/49018 [00:19<00:00, 2521.99it/s]Loading: 100%|██████████| 49018/49018 [00:19<00:00, 2465.10it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank24]:[W424 17:54:57.163010738 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 17:54:57.193064038 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 17:54:57.194741060 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 17:54:57.196430727 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 17:54:57.204317052 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 17:54:57.206836733 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 17:54:57.213169400 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 17:54:57.113955107 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 17:54:57.130509886 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 17:54:57.145916963 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 17:54:57.159636801 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 17:54:57.119357280 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 17:54:57.144960047 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 17:54:57.147159616 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 17:54:57.147483240 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 17:54:57.150353229 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 17:54:57.151143156 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 17:54:57.899960427 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 17:54:57.899942042 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 17:54:57.901452141 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 17:54:57.901949763 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 17:54:57.905084026 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 17:54:57.905930479 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 17:54:57.906878062 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 17:54:57.907461607 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 17:54:58.478673941 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 17:54:58.478741599 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 17:54:58.478779741 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 17:54:58.478758070 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 17:54:58.478889399 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 17:54:58.478990731 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 17:54:59.098570369 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 17:54:59.727525386 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 17:54:59.099249049 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 17:54:59.839887371 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 17:54:59.099790278 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 17:54:59.728763462 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 17:54:59.057665173 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 17:54:59.318794744 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 17:55:00.268886495 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:24<20:08, 24.67s/it]Train:   4%|▍         | 2/50 [00:24<08:14, 10.29s/it]Train:   6%|▌         | 3/50 [00:25<04:27,  5.69s/it]Train:   8%|▊         | 4/50 [00:25<02:42,  3.53s/it]Train:  10%|█         | 5/50 [00:25<01:45,  2.34s/it]Train:  12%|█▏        | 6/50 [00:25<01:11,  1.62s/it]Train:  14%|█▍        | 7/50 [00:25<00:49,  1.16s/it]Train:  16%|█▌        | 8/50 [00:26<00:36,  1.16it/s]Train:  18%|█▊        | 9/50 [00:26<00:26,  1.52it/s]Train:  20%|██        | 10/50 [00:26<00:20,  1.91it/s]Train:  22%|██▏       | 11/50 [00:26<00:16,  2.32it/s]Train:  24%|██▍       | 12/50 [00:27<00:13,  2.73it/s]Train:  26%|██▌       | 13/50 [00:27<00:11,  3.12it/s]Train:  28%|██▊       | 14/50 [00:27<00:10,  3.46it/s]Train:  30%|███       | 15/50 [00:27<00:09,  3.73it/s]Train:  32%|███▏      | 16/50 [00:27<00:08,  3.95it/s]Train:  34%|███▍      | 17/50 [00:28<00:07,  4.13it/s]Train:  36%|███▌      | 18/50 [00:28<00:07,  4.28it/s]Train:  38%|███▊      | 19/50 [00:28<00:07,  4.37it/s]Train:  40%|████      | 20/50 [00:28<00:06,  4.43it/s]Train:  42%|████▏     | 21/50 [00:29<00:06,  4.49it/s]Train:  44%|████▍     | 22/50 [00:29<00:06,  4.52it/s]Train:  46%|████▌     | 23/50 [00:29<00:05,  4.54it/s]Train:  48%|████▊     | 24/50 [00:29<00:05,  4.56it/s]Train:  50%|█████     | 25/50 [00:29<00:05,  4.56it/s]Train:  52%|█████▏    | 26/50 [00:30<00:05,  4.56it/s]Train:  54%|█████▍    | 27/50 [00:30<00:05,  4.57it/s]Train:  56%|█████▌    | 28/50 [00:30<00:04,  4.58it/s]Train:  58%|█████▊    | 29/50 [00:30<00:04,  4.59it/s]Train:  60%|██████    | 30/50 [00:30<00:04,  4.60it/s]Train:  62%|██████▏   | 31/50 [00:31<00:04,  4.60it/s]Train:  64%|██████▍   | 32/50 [00:31<00:03,  4.61it/s]Train:  66%|██████▌   | 33/50 [00:31<00:03,  4.62it/s]Train:  68%|██████▊   | 34/50 [00:31<00:03,  4.62it/s]Train:  70%|███████   | 35/50 [00:32<00:03,  4.61it/s]Train:  72%|███████▏  | 36/50 [00:32<00:03,  4.60it/s]Train:  74%|███████▍  | 37/50 [00:32<00:02,  4.60it/s]Train:  76%|███████▌  | 38/50 [00:32<00:02,  4.59it/s]Train:  78%|███████▊  | 39/50 [00:32<00:02,  4.59it/s]Train:  80%|████████  | 40/50 [00:33<00:02,  4.60it/s]Train:  82%|████████▏ | 41/50 [00:33<00:01,  4.60it/s]Train:  84%|████████▍ | 42/50 [00:33<00:01,  4.61it/s]Train:  86%|████████▌ | 43/50 [00:33<00:01,  4.61it/s]Train:  88%|████████▊ | 44/50 [00:34<00:01,  4.60it/s]Train:  90%|█████████ | 45/50 [00:34<00:01,  4.59it/s]Train:  92%|█████████▏| 46/50 [00:34<00:00,  4.61it/s]Train:  94%|█████████▍| 47/50 [00:34<00:00,  4.63it/s]Train:  96%|█████████▌| 48/50 [00:34<00:00,  4.62it/s]Train:  98%|█████████▊| 49/50 [00:35<00:00,  4.60it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  4.60it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  1.41it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.28it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.95it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.20it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.35it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.43it/s]Train:  12%|█▏        | 6/50 [00:01<00:10,  4.39it/s]Train:  14%|█▍        | 7/50 [00:01<00:10,  4.29it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.38it/s]Train:  18%|█▊        | 9/50 [00:02<00:09,  4.45it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.51it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.55it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.58it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.59it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.60it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.61it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.63it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.62it/s]Train:  36%|███▌      | 18/50 [00:04<00:06,  4.61it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.61it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.61it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.61it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.60it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.60it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.60it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.61it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.62it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.61it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.60it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.62it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.62it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.62it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.63it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.62it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.61it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.61it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.62it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.61it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.62it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.61it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.60it/s]Train:  82%|████████▏ | 41/50 [00:09<00:01,  4.62it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.63it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.63it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.63it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.63it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.62it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.61it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.60it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.60it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.60it/s]Train: 100%|██████████| 50/50 [00:11<00:00,  4.54it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.27it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.95it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.23it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.34it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.44it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.48it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.52it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.53it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.57it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.58it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.59it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.60it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.60it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.61it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.60it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.61it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.61it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.62it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.62it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.61it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.60it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.59it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.59it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.60it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.60it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.60it/s]Train:  54%|█████▍    | 27/50 [00:05<00:05,  4.60it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.60it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.60it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.61it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.62it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.61it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.61it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.61it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.60it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.60it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.61it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.60it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.58it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.59it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.58it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.58it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.59it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.60it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.60it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.61it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.60it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.61it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.61it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.61it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.55it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.28it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.94it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.21it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.35it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.45it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.50it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.55it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.56it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.58it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.58it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.57it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.57it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.58it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.58it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.59it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.59it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.60it/s]Train:  36%|███▌      | 18/50 [00:04<00:06,  4.61it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.60it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.59it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.60it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.60it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.62it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.61it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.61it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.62it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.62it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.61it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.62it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.60it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.60it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.60it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.62it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.60it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.59it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.59it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.60it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.60it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.60it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.61it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.62it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.61it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.63it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.63it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.63it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.62it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.63it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.62it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.61it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.62it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.56it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:14,  3.29it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.96it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.24it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.37it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.44it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.49it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.53it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.56it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.57it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.59it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.59it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.59it/s]Train:  26%|██▌       | 13/50 [00:02<00:08,  4.60it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.60it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.60it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.59it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.60it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.60it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.60it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.62it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.62it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.61it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.61it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.62it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.62it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.62it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.62it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.61it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.61it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.61it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.61it/s]Train:  64%|██████▍   | 32/50 [00:07<00:03,  4.62it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.62it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.60it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.60it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.61it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.61it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.63it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.63it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.63it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.63it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.61it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.62it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.62it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.63it/s]Train:  92%|█████████▏| 46/50 [00:10<00:00,  4.62it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.62it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.62it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.61it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.61it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.56it/s]
0: Process 0 - Local timer:  load_data  :  86.2
0: Process 0 - Local timer:  train_validate_test  :  79.46
0: Process 0 - Local timer:  create_model  :  1.18
0: Minimum timers: 
0: load_data  :  86.2
0: train_validate_test  :  79.4
0: create_model  :  1.17
0: Maximum timers: 
0: load_data  :  87.25
0: train_validate_test  :  79.47
0: create_model  :  1.18
0: Average timers: 
0: load_data  :  86.86
0: train_validate_test  :  79.42
0: create_model  :  1.18
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 17:55:56.550337570 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 17:55:57.122896623 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 17:55:57.122987135 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 17:55:57.123087295 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 17:55:57.123121570 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 17:55:57.012842893 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 17:55:57.012899381 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 17:55:57.012893018 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 17:55:57.012912545 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 17:55:57.681353204 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 17:55:57.013012244 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 17:55:57.681358223 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 17:55:57.013007405 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 17:55:57.681324860 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 17:55:57.013036901 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 17:55:57.681385405 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 17:55:57.681397859 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 17:55:57.681413468 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 17:55:57.681370316 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 17:55:57.124531693 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 17:55:57.681365868 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 17:55:57.725890632 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 17:55:57.725882176 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 17:55:57.725932771 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 17:55:57.725921921 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 17:55:57.725915589 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 17:55:57.725963109 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 17:55:57.726090780 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 17:55:57.726146917 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 17:55:57.126035894 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 17:55:57.385926659 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 17:55:57.385932500 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 17:55:57.385921129 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 17:55:57.385902694 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 17:55:57.385910769 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 17:55:57.385943611 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 17:55:57.386438893 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 17:55:57.387021981 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 17:55:57.127456858 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 17:55:57.016909131 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 05:56:00 PM EDT 2025
Baseline tests with HydraGNN complete at Thu Apr 24 05:56:00 PM EDT 2025
Launching gaussian calculations at Thu Apr 24 05:56:00 PM EDT 2025
srun: warning: can't run 1 processes on 5 nodes, setting nnodes to 1
Starting at Thu Apr 24 05:56:01 PM EDT 2025: time srun --exclusive -n 120 -N 5 --ntasks-per-node=24 -c 1 -o logs/%t.stdout -e logs/%t.stderr --open-mode=append python3 ./main.py ../settings_local.json
Launching HydraGNN for interference tests at Thu Apr 24 06:06:00 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 06:06:00 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_1 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 18:06:12.713925909 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714128915 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714139475 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714179190 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714187416 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714234785 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.714246818 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.715549160 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047392428 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047741640 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047779231 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047820309 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047813987 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.047817133 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.049107808 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.049282990 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494144291 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494399636 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494426577 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494469870 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494485690 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494489467 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.494560763 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.495911991 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 18:06:12.334940888 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.335060305 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.335052099 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.335055826 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.335096112 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.336740219 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.336758855 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.351072922 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.974852862 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975055655 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975151878 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975166926 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975187014 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975228322 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975233452 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:06:12.975302353 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_1 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
0: Read attr time (sec):  0.0016589164733886719
0: read and bcast: trainset/x/variable_count 0.18608713150024414
0: read and bcast: trainset/x/variable_offset 0.36937737464904785
0: read and bcast: trainset/x/variable_dim 0.36977124214172363
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5881068706512451
0: read and bcast: trainset/edge_index/variable_offset 0.7727899551391602
0: read and bcast: trainset/edge_index/variable_dim 0.7839643955230713
0: read and bcast: trainset/edge_attr/variable_count 0.9660933017730713
0: read and bcast: trainset/edge_attr/variable_offset 1.1502208709716797
0: read and bcast: trainset/edge_attr/variable_dim 1.1618902683258057
0: read and bcast: trainset/pos/variable_count 1.3450324535369873
0: read and bcast: trainset/pos/variable_offset 1.5278494358062744
0: read and bcast: trainset/pos/variable_dim 1.540015697479248
0: read and bcast: trainset/energy/variable_count 1.7222721576690674
0: read and bcast: trainset/energy/variable_offset 1.9056217670440674
0: read and bcast: trainset/energy/variable_dim 1.9168915748596191
0: read and bcast: trainset/forces/variable_count 2.0994873046875
0: read and bcast: trainset/forces/variable_offset 2.2838478088378906
0: read and bcast: trainset/forces/variable_dim 2.294992208480835
0: read and bcast: trainset/y/variable_count 2.475942611694336
0: read and bcast: trainset/y/variable_offset 2.6647753715515137
0: read and bcast: trainset/y/variable_dim 2.676931142807007
0: Overall time (sec):  2.678814172744751
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.6831307411193848
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007905960083007812
0: read and bcast: valset/x/variable_count 0.00791621208190918
0: read and bcast: valset/x/variable_offset 0.015362739562988281
0: read and bcast: valset/x/variable_dim 0.015554666519165039
0: read and bcast: valset/edge_index/variable_count 0.023007631301879883
0: read and bcast: valset/edge_index/variable_offset 0.030083179473876953
0: read and bcast: valset/edge_index/variable_dim 0.030269145965576172
0: read and bcast: valset/edge_attr/variable_count 0.0373380184173584
0: read and bcast: valset/edge_attr/variable_offset 0.044803619384765625
0: read and bcast: valset/edge_attr/variable_dim 0.04498863220214844
0: read and bcast: valset/pos/variable_count 0.05240941047668457
0: read and bcast: valset/pos/variable_offset 0.05951571464538574
0: read and bcast: valset/pos/variable_dim 0.059699296951293945
0: read and bcast: valset/energy/variable_count 0.06739306449890137
0: read and bcast: valset/energy/variable_offset 0.0748136043548584
0: read and bcast: valset/energy/variable_dim 0.07499814033508301
0: read and bcast: valset/forces/variable_count 0.08198904991149902
0: read and bcast: valset/forces/variable_offset 0.08939599990844727
0: read and bcast: valset/forces/variable_dim 0.0895845890045166
0: read and bcast: valset/y/variable_count 0.09677696228027344
0: read and bcast: valset/y/variable_offset 0.10379195213317871
0: read and bcast: valset/y/variable_dim 0.10396814346313477
0: Overall time (sec):  0.10495424270629883
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10783815383911133
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007710456848144531
0: read and bcast: testset/x/variable_count 0.00783991813659668
0: read and bcast: testset/x/variable_offset 0.014772176742553711
0: read and bcast: testset/x/variable_dim 0.014950037002563477
0: read and bcast: testset/edge_index/variable_count 0.022436141967773438
0: read and bcast: testset/edge_index/variable_offset 0.02968597412109375
0: read and bcast: testset/edge_index/variable_dim 0.029885292053222656
0: read and bcast: testset/edge_attr/variable_count 0.03709053993225098
0: read and bcast: testset/edge_attr/variable_offset 0.04401588439941406
0: read and bcast: testset/edge_attr/variable_dim 0.0441892147064209
0: read and bcast: testset/pos/variable_count 0.0513920783996582
0: read and bcast: testset/pos/variable_offset 0.05856776237487793
0: read and bcast: testset/pos/variable_dim 0.05876421928405762
0: read and bcast: testset/energy/variable_count 0.06601619720458984
0: read and bcast: testset/energy/variable_offset 0.07323193550109863
0: read and bcast: testset/energy/variable_dim 0.07340741157531738
0: read and bcast: testset/forces/variable_count 0.08068656921386719
0: read and bcast: testset/forces/variable_offset 0.08815693855285645
0: read and bcast: testset/forces/variable_dim 0.08834648132324219
0: read and bcast: testset/y/variable_count 0.09512090682983398
0: read and bcast: testset/y/variable_offset 0.10267210006713867
0: read and bcast: testset/y/variable_dim 0.1028585433959961
0: Overall time (sec):  0.1038215160369873
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10672569274902344
4 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
1 ani1x nsplit: 4460384 6 743398
3 ani1x nsplit: 4460384 6 743397
2 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
11 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
34 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
33 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
36 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
19 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
22 alexandria nsplit: 9705384 11 882308
26 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
20 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
0: Adios reading time (sec):  0.47460293769836426
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.14298582077026367
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.13802886009216309
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 221/64000 [00:00<00:28, 2202.12it/s]Loading:   1%|          | 450/64000 [00:00<00:28, 2251.19it/s]Loading:   1%|          | 678/64000 [00:00<00:28, 2261.31it/s]Loading:   1%|▏         | 909/64000 [00:00<00:27, 2277.94it/s]Loading:   2%|▏         | 1139/64000 [00:00<00:27, 2282.80it/s]Loading:   2%|▏         | 1368/64000 [00:00<00:27, 2269.75it/s]Loading:   2%|▏         | 1595/64000 [00:00<00:27, 2248.30it/s]Loading:   3%|▎         | 1820/64000 [00:00<00:27, 2232.73it/s]Loading:   3%|▎         | 2044/64000 [00:00<00:27, 2228.05it/s]Loading:   4%|▎         | 2267/64000 [00:01<00:27, 2219.45it/s]Loading:   4%|▍         | 2490/64000 [00:01<00:27, 2220.87it/s]Loading:   4%|▍         | 2713/64000 [00:01<00:27, 2217.26it/s]Loading:   5%|▍         | 2935/64000 [00:01<00:27, 2216.23it/s]Loading:   5%|▍         | 3157/64000 [00:01<00:27, 2212.93it/s]Loading:   5%|▌         | 3379/64000 [00:01<00:27, 2209.19it/s]Loading:   6%|▌         | 3601/64000 [00:01<00:27, 2210.20it/s]Loading:   6%|▌         | 3823/64000 [00:01<00:27, 2202.01it/s]Loading:   6%|▋         | 4044/64000 [00:01<00:38, 1556.34it/s]Loading:   7%|▋         | 4264/64000 [00:02<00:35, 1704.04it/s]Loading:   7%|▋         | 4483/64000 [00:02<00:32, 1823.00it/s]Loading:   7%|▋         | 4704/64000 [00:02<00:30, 1922.94it/s]Loading:   8%|▊         | 4925/64000 [00:02<00:29, 1999.43it/s]Loading:   8%|▊         | 5145/64000 [00:02<00:28, 2053.86it/s]Loading:   8%|▊         | 5365/64000 [00:02<00:27, 2094.23it/s]Loading:   9%|▊         | 5587/64000 [00:02<00:27, 2128.56it/s]Loading:   9%|▉         | 5809/64000 [00:02<00:27, 2153.06it/s]Loading:   9%|▉         | 6032/64000 [00:02<00:26, 2173.40it/s]Loading:  10%|▉         | 6253/64000 [00:02<00:26, 2182.81it/s]Loading:  10%|█         | 6475/64000 [00:03<00:26, 2191.53it/s]Loading:  10%|█         | 6696/64000 [00:03<00:26, 2196.88it/s]Loading:  11%|█         | 6918/64000 [00:03<00:25, 2202.33it/s]Loading:  11%|█         | 7140/64000 [00:03<00:25, 2205.70it/s]Loading:  12%|█▏        | 7361/64000 [00:03<00:25, 2204.37it/s]Loading:  12%|█▏        | 7582/64000 [00:03<00:25, 2205.99it/s]Loading:  12%|█▏        | 7803/64000 [00:03<00:25, 2205.27it/s]Loading:  13%|█▎        | 8025/64000 [00:03<00:25, 2206.82it/s]Loading:  13%|█▎        | 8246/64000 [00:03<00:25, 2201.44it/s]Loading:  13%|█▎        | 8468/64000 [00:03<00:25, 2205.54it/s]Loading:  14%|█▎        | 8690/64000 [00:04<00:25, 2208.79it/s]Loading:  14%|█▍        | 8911/64000 [00:04<00:24, 2206.58it/s]Loading:  14%|█▍        | 9132/64000 [00:04<00:24, 2207.32it/s]Loading:  15%|█▍        | 9353/64000 [00:04<00:24, 2205.88it/s]Loading:  15%|█▍        | 9574/64000 [00:04<00:24, 2204.11it/s]Loading:  15%|█▌        | 9795/64000 [00:04<00:24, 2205.42it/s]Loading:  16%|█▌        | 10016/64000 [00:04<00:24, 2201.94it/s]Loading:  16%|█▌        | 10237/64000 [00:04<00:24, 2204.13it/s]Loading:  16%|█▋        | 10458/64000 [00:04<00:24, 2204.53it/s]Loading:  17%|█▋        | 10679/64000 [00:04<00:24, 2203.25it/s]Loading:  17%|█▋        | 10901/64000 [00:05<00:24, 2205.76it/s]Loading:  17%|█▋        | 11122/64000 [00:05<00:23, 2205.76it/s]Loading:  18%|█▊        | 11343/64000 [00:05<00:23, 2206.42it/s]Loading:  18%|█▊        | 11564/64000 [00:05<00:23, 2205.28it/s]Loading:  18%|█▊        | 11785/64000 [00:05<00:23, 2202.10it/s]Loading:  19%|█▉        | 12006/64000 [00:05<00:23, 2199.76it/s]Loading:  19%|█▉        | 12226/64000 [00:05<00:34, 1491.10it/s]Loading:  19%|█▉        | 12446/64000 [00:05<00:31, 1648.86it/s]Loading:  20%|█▉        | 12666/64000 [00:06<00:28, 1782.57it/s]Loading:  20%|██        | 12887/64000 [00:06<00:27, 1890.73it/s]Loading:  20%|██        | 13109/64000 [00:06<00:25, 1978.04it/s]Loading:  21%|██        | 13330/64000 [00:06<00:24, 2041.78it/s]Loading:  21%|██        | 13552/64000 [00:06<00:24, 2091.20it/s]Loading:  22%|██▏       | 13773/64000 [00:06<00:23, 2124.49it/s]Loading:  22%|██▏       | 13995/64000 [00:06<00:23, 2150.41it/s]Loading:  22%|██▏       | 14217/64000 [00:06<00:22, 2169.53it/s]Loading:  23%|██▎       | 14438/64000 [00:06<00:22, 2179.18it/s]Loading:  23%|██▎       | 14661/64000 [00:06<00:22, 2191.19it/s]Loading:  23%|██▎       | 14883/64000 [00:07<00:22, 2199.33it/s]Loading:  24%|██▎       | 15104/64000 [00:07<00:22, 2198.42it/s]Loading:  24%|██▍       | 15325/64000 [00:07<00:22, 2200.58it/s]Loading:  24%|██▍       | 15546/64000 [00:07<00:22, 2200.87it/s]Loading:  25%|██▍       | 15769/64000 [00:07<00:21, 2207.67it/s]Loading:  25%|██▍       | 15991/64000 [00:07<00:21, 2208.80it/s]Loading:  25%|██▌       | 16213/64000 [00:07<00:21, 2205.39it/s]Loading:  26%|██▌       | 16434/64000 [00:07<00:21, 2206.26it/s]Loading:  26%|██▌       | 16655/64000 [00:07<00:21, 2197.58it/s]Loading:  26%|██▋       | 16876/64000 [00:07<00:21, 2199.04it/s]Loading:  27%|██▋       | 17099/64000 [00:08<00:21, 2205.00it/s]Loading:  27%|██▋       | 17320/64000 [00:08<00:21, 2203.72it/s]Loading:  27%|██▋       | 17541/64000 [00:08<00:21, 2204.95it/s]Loading:  28%|██▊       | 17762/64000 [00:08<00:21, 2196.27it/s]Loading:  28%|██▊       | 17983/64000 [00:08<00:20, 2198.25it/s]Loading:  28%|██▊       | 18204/64000 [00:08<00:20, 2199.53it/s]Loading:  29%|██▉       | 18424/64000 [00:08<00:20, 2197.03it/s]Loading:  29%|██▉       | 18646/64000 [00:08<00:20, 2203.49it/s]Loading:  29%|██▉       | 18867/64000 [00:08<00:20, 2203.03it/s]Loading:  30%|██▉       | 19089/64000 [00:08<00:20, 2206.47it/s]Loading:  30%|███       | 19311/64000 [00:09<00:20, 2208.16it/s]Loading:  31%|███       | 19532/64000 [00:09<00:20, 2203.46it/s]Loading:  31%|███       | 19753/64000 [00:09<00:20, 2199.97it/s]Loading:  31%|███       | 19973/64000 [00:09<00:20, 2197.09it/s]Loading:  32%|███▏      | 20193/64000 [00:09<00:19, 2196.20it/s]Loading:  32%|███▏      | 20413/64000 [00:09<00:19, 2195.36it/s]Loading:  32%|███▏      | 20633/64000 [00:09<00:19, 2195.23it/s]Loading:  33%|███▎      | 20855/64000 [00:09<00:19, 2200.61it/s]Loading:  33%|███▎      | 21076/64000 [00:09<00:19, 2198.62it/s]Loading:  33%|███▎      | 21298/64000 [00:09<00:19, 2202.45it/s]Loading:  34%|███▎      | 21520/64000 [00:10<00:19, 2207.21it/s]Loading:  34%|███▍      | 21741/64000 [00:10<00:19, 2207.15it/s]Loading:  34%|███▍      | 21964/64000 [00:10<00:18, 2212.57it/s]Loading:  35%|███▍      | 22186/64000 [00:10<00:29, 1406.64it/s]Loading:  35%|███▌      | 22406/64000 [00:10<00:26, 1575.66it/s]Loading:  35%|███▌      | 22627/64000 [00:10<00:24, 1723.11it/s]Loading:  36%|███▌      | 22847/64000 [00:10<00:22, 1841.47it/s]Loading:  36%|███▌      | 23069/64000 [00:10<00:21, 1939.26it/s]Loading:  36%|███▋      | 23288/64000 [00:11<00:20, 2007.02it/s]Loading:  37%|███▋      | 23511/64000 [00:11<00:19, 2067.81it/s]Loading:  37%|███▋      | 23733/64000 [00:11<00:19, 2109.40it/s]Loading:  37%|███▋      | 23954/64000 [00:11<00:18, 2136.87it/s]Loading:  38%|███▊      | 24176/64000 [00:11<00:18, 2160.43it/s]Loading:  38%|███▊      | 24398/64000 [00:11<00:18, 2175.43it/s]Loading:  38%|███▊      | 24619/64000 [00:11<00:18, 2183.95it/s]Loading:  39%|███▉      | 24840/64000 [00:11<00:17, 2189.99it/s]Loading:  39%|███▉      | 25061/64000 [00:11<00:17, 2192.05it/s]Loading:  40%|███▉      | 25283/64000 [00:11<00:17, 2198.99it/s]Loading:  40%|███▉      | 25504/64000 [00:12<00:17, 2198.74it/s]Loading:  40%|████      | 25725/64000 [00:12<00:17, 2201.91it/s]Loading:  41%|████      | 25947/64000 [00:12<00:17, 2206.77it/s]Loading:  41%|████      | 26168/64000 [00:12<00:17, 2207.45it/s]Loading:  41%|████      | 26390/64000 [00:12<00:17, 2210.77it/s]Loading:  42%|████▏     | 26612/64000 [00:12<00:16, 2206.75it/s]Loading:  42%|████▏     | 26834/64000 [00:12<00:16, 2208.02it/s]Loading:  42%|████▏     | 27056/64000 [00:12<00:16, 2210.16it/s]Loading:  43%|████▎     | 27278/64000 [00:12<00:16, 2208.52it/s]Loading:  43%|████▎     | 27501/64000 [00:12<00:16, 2213.31it/s]Loading:  43%|████▎     | 27723/64000 [00:13<00:16, 2208.64it/s]Loading:  44%|████▎     | 27945/64000 [00:13<00:16, 2210.18it/s]Loading:  44%|████▍     | 28167/64000 [00:13<00:16, 2212.57it/s]Loading:  44%|████▍     | 28389/64000 [00:13<00:16, 2208.67it/s]Loading:  45%|████▍     | 28612/64000 [00:13<00:15, 2212.04it/s]Loading:  45%|████▌     | 28834/64000 [00:13<00:15, 2207.83it/s]Loading:  45%|████▌     | 29056/64000 [00:13<00:15, 2211.10it/s]Loading:  46%|████▌     | 29279/64000 [00:13<00:15, 2214.58it/s]Loading:  46%|████▌     | 29501/64000 [00:13<00:15, 2210.81it/s]Loading:  46%|████▋     | 29723/64000 [00:13<00:15, 2213.03it/s]Loading:  47%|████▋     | 29945/64000 [00:14<00:15, 2211.56it/s]Loading:  47%|████▋     | 30167/64000 [00:14<00:15, 2211.40it/s]Loading:  47%|████▋     | 30390/64000 [00:14<00:15, 2214.04it/s]Loading:  48%|████▊     | 30612/64000 [00:14<00:15, 2212.21it/s]Loading:  48%|████▊     | 30835/64000 [00:14<00:14, 2217.14it/s]Loading:  49%|████▊     | 31057/64000 [00:14<00:14, 2215.73it/s]Loading:  49%|████▉     | 31280/64000 [00:14<00:14, 2217.60it/s]Loading:  49%|████▉     | 31502/64000 [00:14<00:14, 2215.22it/s]Loading:  50%|████▉     | 31724/64000 [00:14<00:14, 2206.66it/s]Loading:  50%|████▉     | 31946/64000 [00:14<00:14, 2208.16it/s]Loading:  50%|█████     | 32167/64000 [00:15<00:14, 2205.02it/s]Loading:  51%|█████     | 32388/64000 [00:15<00:14, 2205.43it/s]Loading:  51%|█████     | 32610/64000 [00:15<00:14, 2207.96it/s]Loading:  51%|█████▏    | 32831/64000 [00:15<00:14, 2205.98it/s]Loading:  52%|█████▏    | 33053/64000 [00:15<00:14, 2207.91it/s]Loading:  52%|█████▏    | 33274/64000 [00:15<00:13, 2206.83it/s]Loading:  52%|█████▏    | 33495/64000 [00:15<00:13, 2206.51it/s]Loading:  53%|█████▎    | 33717/64000 [00:15<00:13, 2207.48it/s]Loading:  53%|█████▎    | 33938/64000 [00:15<00:13, 2207.57it/s]Loading:  53%|█████▎    | 34160/64000 [00:15<00:13, 2208.29it/s]Loading:  54%|█████▎    | 34381/64000 [00:16<00:13, 2206.78it/s]Loading:  54%|█████▍    | 34604/64000 [00:16<00:13, 2211.67it/s]Loading:  54%|█████▍    | 34826/64000 [00:16<00:13, 2213.99it/s]Loading:  55%|█████▍    | 35048/64000 [00:16<00:22, 1310.54it/s]Loading:  55%|█████▌    | 35269/64000 [00:16<00:19, 1491.02it/s]Loading:  55%|█████▌    | 35489/64000 [00:16<00:17, 1649.33it/s]Loading:  56%|█████▌    | 35711/64000 [00:16<00:15, 1787.08it/s]Loading:  56%|█████▌    | 35933/64000 [00:16<00:14, 1897.13it/s]Loading:  56%|█████▋    | 36153/64000 [00:17<00:14, 1977.47it/s]Loading:  57%|█████▋    | 36374/64000 [00:17<00:13, 2041.52it/s]Loading:  57%|█████▋    | 36594/64000 [00:17<00:13, 2084.00it/s]Loading:  58%|█████▊    | 36816/64000 [00:17<00:12, 2122.18it/s]Loading:  58%|█████▊    | 37038/64000 [00:17<00:12, 2148.51it/s]Loading:  58%|█████▊    | 37258/64000 [00:17<00:12, 2163.47it/s]Loading:  59%|█████▊    | 37480/64000 [00:17<00:12, 2178.04it/s]Loading:  59%|█████▉    | 37701/64000 [00:17<00:12, 2185.43it/s]Loading:  59%|█████▉    | 37923/64000 [00:17<00:11, 2194.28it/s]Loading:  60%|█████▉    | 38145/64000 [00:17<00:11, 2199.41it/s]Loading:  60%|█████▉    | 38367/64000 [00:18<00:11, 2202.97it/s]Loading:  60%|██████    | 38590/64000 [00:18<00:11, 2210.39it/s]Loading:  61%|██████    | 38812/64000 [00:18<00:11, 2206.51it/s]Loading:  61%|██████    | 39035/64000 [00:18<00:11, 2212.27it/s]Loading:  61%|██████▏   | 39258/64000 [00:18<00:11, 2215.94it/s]Loading:  62%|██████▏   | 39480/64000 [00:18<00:11, 2211.84it/s]Loading:  62%|██████▏   | 39702/64000 [00:18<00:10, 2213.24it/s]Loading:  62%|██████▏   | 39924/64000 [00:18<00:10, 2209.71it/s]Loading:  63%|██████▎   | 40146/64000 [00:18<00:10, 2210.40it/s]Loading:  63%|██████▎   | 40369/64000 [00:18<00:10, 2213.88it/s]Loading:  63%|██████▎   | 40591/64000 [00:19<00:10, 2211.91it/s]Loading:  64%|██████▍   | 40813/64000 [00:19<00:10, 2206.80it/s]Loading:  64%|██████▍   | 41034/64000 [00:19<00:10, 2202.38it/s]Loading:  64%|██████▍   | 41255/64000 [00:19<00:10, 2204.56it/s]Loading:  65%|██████▍   | 41477/64000 [00:19<00:10, 2206.26it/s]Loading:  65%|██████▌   | 41698/64000 [00:19<00:10, 2202.33it/s]Loading:  66%|██████▌   | 41920/64000 [00:19<00:10, 2207.11it/s]Loading:  66%|██████▌   | 42141/64000 [00:19<00:09, 2204.46it/s]Loading:  66%|██████▌   | 42362/64000 [00:19<00:09, 2205.91it/s]Loading:  67%|██████▋   | 42583/64000 [00:19<00:09, 2204.97it/s]Loading:  67%|██████▋   | 42804/64000 [00:20<00:09, 2202.05it/s]Loading:  67%|██████▋   | 43026/64000 [00:20<00:09, 2204.35it/s]Loading:  68%|██████▊   | 43247/64000 [00:20<00:09, 2202.31it/s]Loading:  68%|██████▊   | 43469/64000 [00:20<00:09, 2206.18it/s]Loading:  68%|██████▊   | 43690/64000 [00:20<00:09, 2206.03it/s]Loading:  69%|██████▊   | 43911/64000 [00:20<00:09, 2205.18it/s]Loading:  69%|██████▉   | 44133/64000 [00:20<00:08, 2207.64it/s]Loading:  69%|██████▉   | 44354/64000 [00:20<00:08, 2207.04it/s]Loading:  70%|██████▉   | 44575/64000 [00:20<00:08, 2206.57it/s]Loading:  70%|██████▉   | 44796/64000 [00:21<00:08, 2204.90it/s]Loading:  70%|███████   | 45017/64000 [00:21<00:08, 2200.88it/s]Loading:  71%|███████   | 45238/64000 [00:21<00:08, 2203.27it/s]Loading:  71%|███████   | 45459/64000 [00:21<00:08, 2197.09it/s]Loading:  71%|███████▏  | 45680/64000 [00:21<00:08, 2200.89it/s]Loading:  72%|███████▏  | 45902/64000 [00:21<00:08, 2205.34it/s]Loading:  72%|███████▏  | 46123/64000 [00:21<00:08, 2204.25it/s]Loading:  72%|███████▏  | 46344/64000 [00:21<00:08, 2201.49it/s]Loading:  73%|███████▎  | 46565/64000 [00:21<00:07, 2200.66it/s]Loading:  73%|███████▎  | 46786/64000 [00:21<00:07, 2203.37it/s]Loading:  73%|███████▎  | 47007/64000 [00:22<00:07, 2203.19it/s]Loading:  74%|███████▍  | 47228/64000 [00:22<00:07, 2202.73it/s]Loading:  74%|███████▍  | 47449/64000 [00:22<00:07, 2201.94it/s]Loading:  74%|███████▍  | 47670/64000 [00:22<00:07, 2199.11it/s]Loading:  75%|███████▍  | 47891/64000 [00:22<00:07, 2200.76it/s]Loading:  75%|███████▌  | 48112/64000 [00:22<00:07, 2203.25it/s]Loading:  76%|███████▌  | 48333/64000 [00:22<00:07, 2202.89it/s]Loading:  76%|███████▌  | 48554/64000 [00:22<00:07, 2204.82it/s]Loading:  76%|███████▌  | 48775/64000 [00:22<00:06, 2205.75it/s]Loading:  77%|███████▋  | 48997/64000 [00:22<00:06, 2207.91it/s]Loading:  77%|███████▋  | 49219/64000 [00:23<00:06, 2210.83it/s]Loading:  77%|███████▋  | 49441/64000 [00:23<00:06, 2206.33it/s]Loading:  78%|███████▊  | 49662/64000 [00:23<00:06, 2200.29it/s]Loading:  78%|███████▊  | 49883/64000 [00:23<00:06, 2195.03it/s]Loading:  78%|███████▊  | 50103/64000 [00:23<00:06, 2195.48it/s]Loading:  79%|███████▊  | 50323/64000 [00:23<00:06, 2195.51it/s]Loading:  79%|███████▉  | 50544/64000 [00:23<00:06, 2197.40it/s]Loading:  79%|███████▉  | 50765/64000 [00:23<00:06, 2199.63it/s]Loading:  80%|███████▉  | 50985/64000 [00:24<00:11, 1177.88it/s]Loading:  80%|████████  | 51203/64000 [00:24<00:09, 1363.83it/s]Loading:  80%|████████  | 51422/64000 [00:24<00:08, 1536.90it/s]Loading:  81%|████████  | 51641/64000 [00:24<00:07, 1686.79it/s]Loading:  81%|████████  | 51861/64000 [00:24<00:06, 1812.25it/s]Loading:  81%|████████▏ | 52079/64000 [00:24<00:06, 1908.31it/s]Loading:  82%|████████▏ | 52300/64000 [00:24<00:05, 1987.87it/s]Loading:  82%|████████▏ | 52520/64000 [00:24<00:05, 2046.73it/s]Loading:  82%|████████▏ | 52739/64000 [00:24<00:05, 2086.58it/s]Loading:  83%|████████▎ | 52960/64000 [00:25<00:05, 2122.21it/s]Loading:  83%|████████▎ | 53181/64000 [00:25<00:05, 2145.99it/s]Loading:  83%|████████▎ | 53401/64000 [00:25<00:04, 2161.23it/s]Loading:  84%|████████▍ | 53621/64000 [00:25<00:04, 2171.48it/s]Loading:  84%|████████▍ | 53841/64000 [00:25<00:04, 2176.22it/s]Loading:  84%|████████▍ | 54061/64000 [00:25<00:04, 2180.71it/s]Loading:  85%|████████▍ | 54281/64000 [00:25<00:04, 2182.51it/s]Loading:  85%|████████▌ | 54502/64000 [00:25<00:04, 2188.41it/s]Loading:  86%|████████▌ | 54722/64000 [00:25<00:04, 2190.03it/s]Loading:  86%|████████▌ | 54942/64000 [00:25<00:04, 2188.48it/s]Loading:  86%|████████▌ | 55163/64000 [00:26<00:04, 2192.21it/s]Loading:  87%|████████▋ | 55383/64000 [00:26<00:03, 2191.00it/s]Loading:  87%|████████▋ | 55604/64000 [00:26<00:03, 2196.43it/s]Loading:  87%|████████▋ | 55825/64000 [00:26<00:03, 2198.34it/s]Loading:  88%|████████▊ | 56045/64000 [00:26<00:03, 2193.64it/s]Loading:  88%|████████▊ | 56265/64000 [00:26<00:03, 2193.16it/s]Loading:  88%|████████▊ | 56485/64000 [00:26<00:03, 2189.33it/s]Loading:  89%|████████▊ | 56705/64000 [00:26<00:03, 2191.45it/s]Loading:  89%|████████▉ | 56926/64000 [00:26<00:03, 2193.93it/s]Loading:  89%|████████▉ | 57146/64000 [00:26<00:03, 2192.55it/s]Loading:  90%|████████▉ | 57366/64000 [00:27<00:03, 2194.07it/s]Loading:  90%|████████▉ | 57586/64000 [00:27<00:02, 2192.05it/s]Loading:  90%|█████████ | 57807/64000 [00:27<00:02, 2195.31it/s]Loading:  91%|█████████ | 58027/64000 [00:27<00:02, 2194.34it/s]Loading:  91%|█████████ | 58247/64000 [00:27<00:02, 2189.98it/s]Loading:  91%|█████████▏| 58468/64000 [00:27<00:02, 2193.06it/s]Loading:  92%|█████████▏| 58688/64000 [00:27<00:02, 2192.69it/s]Loading:  92%|█████████▏| 58908/64000 [00:27<00:02, 2193.62it/s]Loading:  92%|█████████▏| 59128/64000 [00:27<00:02, 2193.44it/s]Loading:  93%|█████████▎| 59348/64000 [00:27<00:02, 2192.40it/s]Loading:  93%|█████████▎| 59569/64000 [00:28<00:02, 2194.78it/s]Loading:  93%|█████████▎| 59789/64000 [00:28<00:01, 2194.42it/s]Loading:  94%|█████████▍| 60009/64000 [00:28<00:01, 2195.22it/s]Loading:  94%|█████████▍| 60229/64000 [00:28<00:01, 2193.51it/s]Loading:  94%|█████████▍| 60449/64000 [00:28<00:01, 2187.75it/s]Loading:  95%|█████████▍| 60669/64000 [00:28<00:01, 2190.68it/s]Loading:  95%|█████████▌| 60889/64000 [00:28<00:01, 2188.46it/s]Loading:  95%|█████████▌| 61112/64000 [00:28<00:01, 2198.93it/s]Loading:  96%|█████████▌| 61334/64000 [00:28<00:01, 2205.02it/s]Loading:  96%|█████████▌| 61555/64000 [00:28<00:01, 2203.75it/s]Loading:  97%|█████████▋| 61776/64000 [00:29<00:01, 2203.63it/s]Loading:  97%|█████████▋| 61997/64000 [00:29<00:00, 2199.62it/s]Loading:  97%|█████████▋| 62220/64000 [00:29<00:00, 2207.50it/s]Loading:  98%|█████████▊| 62442/64000 [00:29<00:00, 2210.93it/s]Loading:  98%|█████████▊| 62664/64000 [00:29<00:00, 2212.50it/s]Loading:  98%|█████████▊| 62887/64000 [00:29<00:00, 2215.37it/s]Loading:  99%|█████████▊| 63109/64000 [00:29<00:00, 2214.11it/s]Loading:  99%|█████████▉| 63332/64000 [00:29<00:00, 2216.10it/s]Loading:  99%|█████████▉| 63555/64000 [00:29<00:00, 2218.54it/s]Loading: 100%|█████████▉| 63777/64000 [00:29<00:00, 2217.02it/s]Loading: 100%|█████████▉| 63999/64000 [00:30<00:00, 2216.27it/s]Loading: 100%|██████████| 64000/64000 [00:30<00:00, 2131.45it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 228/49018 [00:00<00:21, 2274.97it/s]Loading:   1%|          | 460/49018 [00:00<00:21, 2297.26it/s]Loading:   1%|▏         | 694/49018 [00:00<00:20, 2316.43it/s]Loading:   2%|▏         | 927/49018 [00:00<00:20, 2319.74it/s]Loading:   2%|▏         | 1161/49018 [00:00<00:20, 2324.32it/s]Loading:   3%|▎         | 1394/49018 [00:00<00:20, 2322.99it/s]Loading:   3%|▎         | 1630/49018 [00:00<00:20, 2334.08it/s]Loading:   4%|▍         | 1868/49018 [00:00<00:20, 2345.54it/s]Loading:   4%|▍         | 2103/49018 [00:00<00:20, 2345.21it/s]Loading:   5%|▍         | 2341/49018 [00:01<00:19, 2353.28it/s]Loading:   5%|▌         | 2577/49018 [00:01<00:19, 2351.70it/s]Loading:   6%|▌         | 2813/49018 [00:01<00:19, 2350.94it/s]Loading:   6%|▌         | 3049/49018 [00:01<00:19, 2345.74it/s]Loading:   7%|▋         | 3286/49018 [00:01<00:19, 2351.49it/s]Loading:   7%|▋         | 3523/49018 [00:01<00:19, 2356.73it/s]Loading:   8%|▊         | 3759/49018 [00:01<00:19, 2356.31it/s]Loading:   8%|▊         | 3995/49018 [00:01<00:19, 2355.24it/s]Loading:   9%|▊         | 4231/49018 [00:01<00:19, 2353.06it/s]Loading:   9%|▉         | 4467/49018 [00:01<00:18, 2354.92it/s]Loading:  10%|▉         | 4703/49018 [00:02<00:18, 2353.07it/s]Loading:  10%|█         | 4941/49018 [00:02<00:18, 2359.79it/s]Loading:  11%|█         | 5179/49018 [00:02<00:18, 2365.23it/s]Loading:  11%|█         | 5416/49018 [00:02<00:18, 2362.18it/s]Loading:  12%|█▏        | 5653/49018 [00:02<00:18, 2361.29it/s]Loading:  12%|█▏        | 5890/49018 [00:02<00:18, 2355.66it/s]Loading:  12%|█▏        | 6126/49018 [00:02<00:18, 2355.17it/s]Loading:  13%|█▎        | 6362/49018 [00:02<00:18, 2354.04it/s]Loading:  13%|█▎        | 6598/49018 [00:02<00:18, 2355.71it/s]Loading:  14%|█▍        | 6834/49018 [00:02<00:17, 2353.61it/s]Loading:  14%|█▍        | 7070/49018 [00:03<00:17, 2353.07it/s]Loading:  15%|█▍        | 7306/49018 [00:03<00:17, 2352.92it/s]Loading:  15%|█▌        | 7542/49018 [00:03<00:17, 2349.78it/s]Loading:  16%|█▌        | 7777/49018 [00:03<00:17, 2349.82it/s]Loading:  16%|█▋        | 8012/49018 [00:03<00:17, 2345.83it/s]Loading:  17%|█▋        | 8248/49018 [00:03<00:17, 2348.41it/s]Loading:  17%|█▋        | 8484/49018 [00:03<00:17, 2348.89it/s]Loading:  18%|█▊        | 8719/49018 [00:03<00:17, 2341.87it/s]Loading:  18%|█▊        | 8954/49018 [00:03<00:17, 2342.25it/s]Loading:  19%|█▊        | 9189/49018 [00:03<00:17, 2338.38it/s]Loading:  19%|█▉        | 9425/49018 [00:04<00:16, 2342.67it/s]Loading:  20%|█▉        | 9660/49018 [00:04<00:16, 2337.96it/s]Loading:  20%|██        | 9895/49018 [00:04<00:16, 2339.30it/s]Loading:  21%|██        | 10130/49018 [00:04<00:16, 2341.45it/s]Loading:  21%|██        | 10365/49018 [00:04<00:16, 2338.32it/s]Loading:  22%|██▏       | 10600/49018 [00:04<00:16, 2341.58it/s]Loading:  22%|██▏       | 10835/49018 [00:04<00:16, 2341.22it/s]Loading:  23%|██▎       | 11070/49018 [00:04<00:16, 2341.95it/s]Loading:  23%|██▎       | 11305/49018 [00:04<00:16, 2340.65it/s]Loading:  24%|██▎       | 11540/49018 [00:04<00:16, 2334.94it/s]Loading:  24%|██▍       | 11775/49018 [00:05<00:15, 2338.32it/s]Loading:  24%|██▍       | 12009/49018 [00:05<00:15, 2336.20it/s]Loading:  25%|██▍       | 12244/49018 [00:05<00:15, 2340.06it/s]Loading:  25%|██▌       | 12479/49018 [00:05<00:15, 2340.57it/s]Loading:  26%|██▌       | 12714/49018 [00:05<00:15, 2342.83it/s]Loading:  26%|██▋       | 12949/49018 [00:05<00:15, 2340.16it/s]Loading:  27%|██▋       | 13184/49018 [00:05<00:15, 2318.84it/s]Loading:  27%|██▋       | 13416/49018 [00:05<00:15, 2309.87it/s]Loading:  28%|██▊       | 13648/49018 [00:05<00:15, 2299.57it/s]Loading:  28%|██▊       | 13878/49018 [00:05<00:15, 2295.33it/s]Loading:  29%|██▉       | 14108/49018 [00:06<00:15, 2287.62it/s]Loading:  29%|██▉       | 14337/49018 [00:06<00:15, 2287.51it/s]Loading:  30%|██▉       | 14566/49018 [00:06<00:15, 2287.43it/s]Loading:  30%|███       | 14795/49018 [00:06<00:14, 2284.21it/s]Loading:  31%|███       | 15024/49018 [00:06<00:14, 2285.40it/s]Loading:  31%|███       | 15253/49018 [00:06<00:14, 2280.19it/s]Loading:  32%|███▏      | 15482/49018 [00:06<00:14, 2279.47it/s]Loading:  32%|███▏      | 15710/49018 [00:06<00:14, 2279.61it/s]Loading:  33%|███▎      | 15938/49018 [00:06<00:14, 2278.15it/s]Loading:  33%|███▎      | 16168/49018 [00:06<00:14, 2282.41it/s]Loading:  33%|███▎      | 16397/49018 [00:07<00:14, 2281.80it/s]Loading:  34%|███▍      | 16626/49018 [00:07<00:14, 2283.36it/s]Loading:  34%|███▍      | 16855/49018 [00:07<00:14, 2275.97it/s]Loading:  35%|███▍      | 17083/49018 [00:07<00:14, 2274.64it/s]Loading:  35%|███▌      | 17312/49018 [00:07<00:13, 2276.64it/s]Loading:  36%|███▌      | 17540/49018 [00:07<00:13, 2274.42it/s]Loading:  36%|███▌      | 17769/49018 [00:07<00:13, 2276.97it/s]Loading:  37%|███▋      | 17997/49018 [00:08<00:29, 1065.68it/s]Loading:  37%|███▋      | 18224/49018 [00:08<00:24, 1265.79it/s]Loading:  38%|███▊      | 18451/49018 [00:08<00:20, 1458.38it/s]Loading:  38%|███▊      | 18678/49018 [00:08<00:18, 1632.14it/s]Loading:  39%|███▊      | 18907/49018 [00:08<00:16, 1785.71it/s]Loading:  39%|███▉      | 19135/49018 [00:08<00:15, 1908.88it/s]Loading:  40%|███▉      | 19363/49018 [00:08<00:14, 2006.52it/s]Loading:  40%|███▉      | 19590/49018 [00:08<00:14, 2078.35it/s]Loading:  40%|████      | 19817/49018 [00:08<00:13, 2130.86it/s]Loading:  41%|████      | 20046/49018 [00:09<00:13, 2174.73it/s]Loading:  41%|████▏     | 20274/49018 [00:09<00:13, 2204.36it/s]Loading:  42%|████▏     | 20502/49018 [00:09<00:12, 2225.32it/s]Loading:  42%|████▏     | 20730/49018 [00:09<00:12, 2240.91it/s]Loading:  43%|████▎     | 20957/49018 [00:09<00:12, 2246.83it/s]Loading:  43%|████▎     | 21185/49018 [00:09<00:12, 2255.94it/s]Loading:  44%|████▎     | 21413/49018 [00:09<00:12, 2261.96it/s]Loading:  44%|████▍     | 21643/49018 [00:09<00:12, 2271.17it/s]Loading:  45%|████▍     | 21871/49018 [00:09<00:11, 2268.18it/s]Loading:  45%|████▌     | 22099/49018 [00:09<00:11, 2268.08it/s]Loading:  46%|████▌     | 22327/49018 [00:10<00:11, 2270.36it/s]Loading:  46%|████▌     | 22555/49018 [00:10<00:11, 2267.20it/s]Loading:  46%|████▋     | 22784/49018 [00:10<00:11, 2272.71it/s]Loading:  47%|████▋     | 23012/49018 [00:10<00:11, 2271.93it/s]Loading:  47%|████▋     | 23240/49018 [00:10<00:11, 2272.20it/s]Loading:  48%|████▊     | 23468/49018 [00:10<00:11, 2274.48it/s]Loading:  48%|████▊     | 23696/49018 [00:10<00:11, 2271.59it/s]Loading:  49%|████▉     | 23925/49018 [00:10<00:11, 2274.87it/s]Loading:  49%|████▉     | 24153/49018 [00:10<00:10, 2273.18it/s]Loading:  50%|████▉     | 24382/49018 [00:10<00:10, 2277.74it/s]Loading:  50%|█████     | 24610/49018 [00:11<00:10, 2275.39it/s]Loading:  51%|█████     | 24839/49018 [00:11<00:10, 2276.90it/s]Loading:  51%|█████     | 25068/49018 [00:11<00:10, 2280.71it/s]Loading:  52%|█████▏    | 25297/49018 [00:11<00:10, 2279.42it/s]Loading:  52%|█████▏    | 25525/49018 [00:11<00:10, 2278.65it/s]Loading:  53%|█████▎    | 25753/49018 [00:11<00:10, 2273.78it/s]Loading:  53%|█████▎    | 25982/49018 [00:11<00:10, 2277.18it/s]Loading:  53%|█████▎    | 26211/49018 [00:11<00:10, 2279.02it/s]Loading:  54%|█████▍    | 26439/49018 [00:11<00:09, 2273.25it/s]Loading:  54%|█████▍    | 26668/49018 [00:11<00:09, 2275.62it/s]Loading:  55%|█████▍    | 26896/49018 [00:12<00:09, 2273.84it/s]Loading:  55%|█████▌    | 27125/49018 [00:12<00:09, 2278.09it/s]Loading:  56%|█████▌    | 27354/49018 [00:12<00:09, 2280.23it/s]Loading:  56%|█████▋    | 27583/49018 [00:12<00:09, 2281.59it/s]Loading:  57%|█████▋    | 27813/49018 [00:12<00:09, 2285.83it/s]Loading:  57%|█████▋    | 28042/49018 [00:12<00:09, 2282.43it/s]Loading:  58%|█████▊    | 28271/49018 [00:12<00:09, 2282.80it/s]Loading:  58%|█████▊    | 28500/49018 [00:12<00:08, 2282.63it/s]Loading:  59%|█████▊    | 28730/49018 [00:12<00:08, 2285.92it/s]Loading:  59%|█████▉    | 28959/49018 [00:12<00:08, 2286.31it/s]Loading:  60%|█████▉    | 29188/49018 [00:13<00:08, 2284.71it/s]Loading:  60%|██████    | 29417/49018 [00:13<00:08, 2284.47it/s]Loading:  60%|██████    | 29646/49018 [00:13<00:08, 2280.98it/s]Loading:  61%|██████    | 29876/49018 [00:13<00:08, 2285.80it/s]Loading:  61%|██████▏   | 30105/49018 [00:13<00:08, 2284.16it/s]Loading:  62%|██████▏   | 30334/49018 [00:13<00:08, 2281.27it/s]Loading:  62%|██████▏   | 30563/49018 [00:13<00:08, 2282.55it/s]Loading:  63%|██████▎   | 30792/49018 [00:13<00:07, 2279.85it/s]Loading:  63%|██████▎   | 31022/49018 [00:13<00:07, 2285.24it/s]Loading:  64%|██████▍   | 31251/49018 [00:13<00:07, 2283.36it/s]Loading:  64%|██████▍   | 31481/49018 [00:14<00:07, 2285.30it/s]Loading:  65%|██████▍   | 31710/49018 [00:14<00:07, 2286.55it/s]Loading:  65%|██████▌   | 31939/49018 [00:14<00:07, 2285.19it/s]Loading:  66%|██████▌   | 32168/49018 [00:14<00:07, 2286.39it/s]Loading:  66%|██████▌   | 32397/49018 [00:14<00:07, 2282.66it/s]Loading:  67%|██████▋   | 32627/49018 [00:14<00:07, 2284.93it/s]Loading:  67%|██████▋   | 32857/49018 [00:14<00:07, 2287.33it/s]Loading:  67%|██████▋   | 33086/49018 [00:14<00:06, 2284.31it/s]Loading:  68%|██████▊   | 33315/49018 [00:14<00:06, 2284.85it/s]Loading:  68%|██████▊   | 33544/49018 [00:14<00:06, 2281.99it/s]Loading:  69%|██████▉   | 33773/49018 [00:15<00:06, 2283.66it/s]Loading:  69%|██████▉   | 34002/49018 [00:15<00:06, 2278.93it/s]Loading:  70%|██████▉   | 34231/49018 [00:15<00:06, 2281.21it/s]Loading:  70%|███████   | 34460/49018 [00:15<00:06, 2283.71it/s]Loading:  71%|███████   | 34689/49018 [00:15<00:06, 2283.83it/s]Loading:  71%|███████   | 34918/49018 [00:15<00:06, 2284.89it/s]Loading:  72%|███████▏  | 35147/49018 [00:15<00:06, 2282.08it/s]Loading:  72%|███████▏  | 35376/49018 [00:15<00:05, 2283.47it/s]Loading:  73%|███████▎  | 35605/49018 [00:15<00:05, 2282.49it/s]Loading:  73%|███████▎  | 35834/49018 [00:15<00:05, 2280.59it/s]Loading:  74%|███████▎  | 36064/49018 [00:16<00:05, 2284.21it/s]Loading:  74%|███████▍  | 36293/49018 [00:16<00:05, 2280.16it/s]Loading:  75%|███████▍  | 36522/49018 [00:16<00:05, 2279.40it/s]Loading:  75%|███████▍  | 36750/49018 [00:16<00:05, 2275.32it/s]Loading:  75%|███████▌  | 36979/49018 [00:16<00:05, 2278.53it/s]Loading:  76%|███████▌  | 37208/49018 [00:16<00:05, 2279.03it/s]Loading:  76%|███████▋  | 37436/49018 [00:16<00:05, 2276.61it/s]Loading:  77%|███████▋  | 37666/49018 [00:16<00:04, 2281.26it/s]Loading:  77%|███████▋  | 37895/49018 [00:16<00:04, 2281.97it/s]Loading:  78%|███████▊  | 38124/49018 [00:16<00:04, 2282.84it/s]Loading:  78%|███████▊  | 38354/49018 [00:17<00:04, 2285.55it/s]Loading:  79%|███████▊  | 38583/49018 [00:17<00:04, 2282.96it/s]Loading:  79%|███████▉  | 38812/49018 [00:17<00:04, 2284.42it/s]Loading:  80%|███████▉  | 39041/49018 [00:17<00:04, 2284.75it/s]Loading:  80%|████████  | 39270/49018 [00:17<00:04, 2284.80it/s]Loading:  81%|████████  | 39500/49018 [00:17<00:04, 2287.06it/s]Loading:  81%|████████  | 39729/49018 [00:17<00:04, 2286.31it/s]Loading:  82%|████████▏ | 39959/49018 [00:17<00:03, 2288.43it/s]Loading:  82%|████████▏ | 40188/49018 [00:17<00:03, 2284.48it/s]Loading:  82%|████████▏ | 40418/49018 [00:17<00:03, 2286.85it/s]Loading:  83%|████████▎ | 40647/49018 [00:18<00:03, 2285.29it/s]Loading:  83%|████████▎ | 40877/49018 [00:18<00:03, 2288.20it/s]Loading:  84%|████████▍ | 41107/49018 [00:18<00:03, 2289.46it/s]Loading:  84%|████████▍ | 41336/49018 [00:18<00:03, 2286.42it/s]Loading:  85%|████████▍ | 41565/49018 [00:18<00:03, 2286.58it/s]Loading:  85%|████████▌ | 41794/49018 [00:18<00:03, 2284.61it/s]Loading:  86%|████████▌ | 42023/49018 [00:18<00:03, 2286.05it/s]Loading:  86%|████████▌ | 42253/49018 [00:18<00:02, 2289.09it/s]Loading:  87%|████████▋ | 42482/49018 [00:18<00:02, 2286.06it/s]Loading:  87%|████████▋ | 42712/49018 [00:18<00:02, 2287.32it/s]Loading:  88%|████████▊ | 42941/49018 [00:19<00:02, 2284.84it/s]Loading:  88%|████████▊ | 43170/49018 [00:19<00:02, 2285.10it/s]Loading:  89%|████████▊ | 43399/49018 [00:19<00:02, 2283.65it/s]Loading:  89%|████████▉ | 43629/49018 [00:19<00:02, 2286.75it/s]Loading:  89%|████████▉ | 43858/49018 [00:19<00:02, 2286.60it/s]Loading:  90%|████████▉ | 44087/49018 [00:19<00:02, 2287.44it/s]Loading:  90%|█████████ | 44317/49018 [00:19<00:02, 2290.45it/s]Loading:  91%|█████████ | 44547/49018 [00:19<00:01, 2285.10it/s]Loading:  91%|█████████▏| 44777/49018 [00:19<00:01, 2287.03it/s]Loading:  92%|█████████▏| 45007/49018 [00:19<00:01, 2289.60it/s]Loading:  92%|█████████▏| 45236/49018 [00:20<00:01, 2285.92it/s]Loading:  93%|█████████▎| 45465/49018 [00:20<00:01, 2286.52it/s]Loading:  93%|█████████▎| 45694/49018 [00:20<00:03, 959.26it/s] Loading:  94%|█████████▎| 45922/49018 [00:20<00:02, 1159.85it/s]Loading:  94%|█████████▍| 46151/49018 [00:20<00:02, 1361.34it/s]Loading:  95%|█████████▍| 46379/49018 [00:21<00:01, 1547.68it/s]Loading:  95%|█████████▌| 46608/49018 [00:21<00:01, 1713.64it/s]Loading:  96%|█████████▌| 46835/49018 [00:21<00:01, 1848.76it/s]Loading:  96%|█████████▌| 47064/49018 [00:21<00:00, 1962.21it/s]Loading:  96%|█████████▋| 47293/49018 [00:21<00:00, 2049.78it/s]Loading:  97%|█████████▋| 47523/49018 [00:21<00:00, 2117.01it/s]Loading:  97%|█████████▋| 47753/49018 [00:21<00:00, 2166.94it/s]Loading:  98%|█████████▊| 47982/49018 [00:21<00:00, 2199.81it/s]Loading:  98%|█████████▊| 48212/49018 [00:21<00:00, 2226.32it/s]Loading:  99%|█████████▉| 48440/49018 [00:21<00:00, 2241.43it/s]Loading:  99%|█████████▉| 48670/49018 [00:22<00:00, 2256.98it/s]Loading: 100%|█████████▉| 48900/49018 [00:22<00:00, 2268.26it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2210.41it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 234/49018 [00:00<00:20, 2337.20it/s]Loading:   1%|          | 473/49018 [00:00<00:20, 2367.90it/s]Loading:   1%|▏         | 711/49018 [00:00<00:20, 2369.73it/s]Loading:   2%|▏         | 950/49018 [00:00<00:20, 2375.64it/s]Loading:   2%|▏         | 1189/49018 [00:00<00:20, 2379.85it/s]Loading:   3%|▎         | 1427/49018 [00:00<00:20, 2378.78it/s]Loading:   3%|▎         | 1666/49018 [00:00<00:19, 2380.23it/s]Loading:   4%|▍         | 1905/49018 [00:01<00:49, 943.94it/s] Loading:   4%|▍         | 2141/49018 [00:01<00:40, 1159.45it/s]Loading:   5%|▍         | 2379/49018 [00:01<00:33, 1376.79it/s]Loading:   5%|▌         | 2616/49018 [00:01<00:29, 1577.50it/s]Loading:   6%|▌         | 2855/49018 [00:01<00:26, 1758.66it/s]Loading:   6%|▋         | 3093/49018 [00:01<00:24, 1908.94it/s]Loading:   7%|▋         | 3332/49018 [00:01<00:22, 2031.68it/s]Loading:   7%|▋         | 3569/49018 [00:01<00:21, 2121.43it/s]Loading:   8%|▊         | 3807/49018 [00:02<00:20, 2191.71it/s]Loading:   8%|▊         | 4045/49018 [00:02<00:20, 2242.28it/s]Loading:   9%|▊         | 4283/49018 [00:02<00:19, 2279.48it/s]Loading:   9%|▉         | 4521/49018 [00:02<00:19, 2308.28it/s]Loading:  10%|▉         | 4758/49018 [00:02<00:19, 2325.63it/s]Loading:  10%|█         | 4997/49018 [00:02<00:18, 2342.56it/s]Loading:  11%|█         | 5235/49018 [00:02<00:18, 2351.05it/s]Loading:  11%|█         | 5474/49018 [00:02<00:18, 2361.69it/s]Loading:  12%|█▏        | 5712/49018 [00:02<00:18, 2364.58it/s]Loading:  12%|█▏        | 5950/49018 [00:02<00:18, 2368.54it/s]Loading:  13%|█▎        | 6189/49018 [00:03<00:18, 2372.65it/s]Loading:  13%|█▎        | 6427/49018 [00:03<00:17, 2367.85it/s]Loading:  14%|█▎        | 6665/49018 [00:03<00:17, 2365.61it/s]Loading:  14%|█▍        | 6902/49018 [00:03<00:17, 2357.60it/s]Loading:  15%|█▍        | 7138/49018 [00:03<00:17, 2357.80it/s]Loading:  15%|█▌        | 7374/49018 [00:03<00:17, 2352.00it/s]Loading:  16%|█▌        | 7610/49018 [00:03<00:17, 2353.24it/s]Loading:  16%|█▌        | 7846/49018 [00:03<00:17, 2353.90it/s]Loading:  16%|█▋        | 8082/49018 [00:03<00:17, 2352.83it/s]Loading:  17%|█▋        | 8318/49018 [00:03<00:17, 2350.87it/s]Loading:  17%|█▋        | 8554/49018 [00:04<00:17, 2347.23it/s]Loading:  18%|█▊        | 8791/49018 [00:04<00:17, 2352.41it/s]Loading:  18%|█▊        | 9027/49018 [00:04<00:17, 2351.48it/s]Loading:  19%|█▉        | 9263/49018 [00:04<00:16, 2352.88it/s]Loading:  19%|█▉        | 9499/49018 [00:04<00:16, 2353.22it/s]Loading:  20%|█▉        | 9735/49018 [00:04<00:16, 2353.13it/s]Loading:  20%|██        | 9972/49018 [00:04<00:16, 2356.23it/s]Loading:  21%|██        | 10208/49018 [00:04<00:16, 2352.63it/s]Loading:  21%|██▏       | 10444/49018 [00:04<00:16, 2350.10it/s]Loading:  22%|██▏       | 10680/49018 [00:04<00:16, 2351.39it/s]Loading:  22%|██▏       | 10916/49018 [00:05<00:16, 2352.53it/s]Loading:  23%|██▎       | 11153/49018 [00:05<00:16, 2355.11it/s]Loading:  23%|██▎       | 11389/49018 [00:05<00:15, 2352.26it/s]Loading:  24%|██▎       | 11626/49018 [00:05<00:15, 2354.69it/s]Loading:  24%|██▍       | 11862/49018 [00:05<00:15, 2351.04it/s]Loading:  25%|██▍       | 12098/49018 [00:05<00:15, 2351.36it/s]Loading:  25%|██▌       | 12334/49018 [00:05<00:15, 2352.79it/s]Loading:  26%|██▌       | 12572/49018 [00:05<00:15, 2358.50it/s]Loading:  26%|██▌       | 12809/49018 [00:05<00:15, 2360.50it/s]Loading:  27%|██▋       | 13046/49018 [00:05<00:15, 2358.37it/s]Loading:  27%|██▋       | 13284/49018 [00:06<00:15, 2364.07it/s]Loading:  28%|██▊       | 13521/49018 [00:06<00:15, 2361.43it/s]Loading:  28%|██▊       | 13758/49018 [00:06<00:14, 2361.72it/s]Loading:  29%|██▊       | 13995/49018 [00:06<00:14, 2353.98it/s]Loading:  29%|██▉       | 14231/49018 [00:06<00:14, 2355.77it/s]Loading:  30%|██▉       | 14468/49018 [00:06<00:14, 2357.30it/s]Loading:  30%|██▉       | 14704/49018 [00:06<00:14, 2354.59it/s]Loading:  30%|███       | 14940/49018 [00:06<00:14, 2356.02it/s]Loading:  31%|███       | 15176/49018 [00:06<00:14, 2353.92it/s]Loading:  31%|███▏      | 15412/49018 [00:07<00:14, 2355.17it/s]Loading:  32%|███▏      | 15648/49018 [00:07<00:14, 2352.31it/s]Loading:  32%|███▏      | 15884/49018 [00:07<00:14, 2354.13it/s]Loading:  33%|███▎      | 16121/49018 [00:07<00:13, 2356.38it/s]Loading:  33%|███▎      | 16357/49018 [00:07<00:13, 2354.38it/s]Loading:  34%|███▍      | 16593/49018 [00:07<00:13, 2354.95it/s]Loading:  34%|███▍      | 16829/49018 [00:07<00:13, 2349.11it/s]Loading:  35%|███▍      | 17064/49018 [00:07<00:13, 2347.84it/s]Loading:  35%|███▌      | 17299/49018 [00:07<00:13, 2344.08it/s]Loading:  36%|███▌      | 17534/49018 [00:07<00:13, 2343.70it/s]Loading:  36%|███▌      | 17769/49018 [00:08<00:13, 2343.81it/s]Loading:  37%|███▋      | 18004/49018 [00:08<00:13, 2337.34it/s]Loading:  37%|███▋      | 18238/49018 [00:08<00:13, 2320.37it/s]Loading:  38%|███▊      | 18471/49018 [00:08<00:13, 2303.58it/s]Loading:  38%|███▊      | 18702/49018 [00:08<00:13, 2298.09it/s]Loading:  39%|███▊      | 18932/49018 [00:08<00:13, 2295.11it/s]Loading:  39%|███▉      | 19162/49018 [00:08<00:13, 2292.60it/s]Loading:  40%|███▉      | 19392/49018 [00:08<00:12, 2291.72it/s]Loading:  40%|████      | 19622/49018 [00:08<00:12, 2283.34it/s]Loading:  40%|████      | 19851/49018 [00:08<00:12, 2281.56it/s]Loading:  41%|████      | 20080/49018 [00:09<00:12, 2278.87it/s]Loading:  41%|████▏     | 20308/49018 [00:09<00:12, 2278.42it/s]Loading:  42%|████▏     | 20536/49018 [00:09<00:12, 2276.45it/s]Loading:  42%|████▏     | 20764/49018 [00:09<00:12, 2273.29it/s]Loading:  43%|████▎     | 20993/49018 [00:09<00:12, 2275.45it/s]Loading:  43%|████▎     | 21222/49018 [00:09<00:12, 2278.00it/s]Loading:  44%|████▍     | 21451/49018 [00:09<00:12, 2280.35it/s]Loading:  44%|████▍     | 21680/49018 [00:09<00:12, 2276.74it/s]Loading:  45%|████▍     | 21908/49018 [00:09<00:11, 2273.10it/s]Loading:  45%|████▌     | 22136/49018 [00:09<00:11, 2272.15it/s]Loading:  46%|████▌     | 22364/49018 [00:10<00:11, 2269.94it/s]Loading:  46%|████▌     | 22593/49018 [00:10<00:11, 2273.04it/s]Loading:  47%|████▋     | 22821/49018 [00:10<00:11, 2270.20it/s]Loading:  47%|████▋     | 23050/49018 [00:10<00:11, 2275.35it/s]Loading:  47%|████▋     | 23279/49018 [00:10<00:11, 2279.56it/s]Loading:  48%|████▊     | 23507/49018 [00:10<00:11, 2273.15it/s]Loading:  48%|████▊     | 23735/49018 [00:10<00:11, 2271.70it/s]Loading:  49%|████▉     | 23963/49018 [00:10<00:11, 2273.76it/s]Loading:  49%|████▉     | 24191/49018 [00:10<00:10, 2275.27it/s]Loading:  50%|████▉     | 24420/49018 [00:10<00:10, 2277.84it/s]Loading:  50%|█████     | 24648/49018 [00:11<00:10, 2274.70it/s]Loading:  51%|█████     | 24876/49018 [00:11<00:10, 2271.76it/s]Loading:  51%|█████     | 25104/49018 [00:11<00:10, 2265.02it/s]Loading:  52%|█████▏    | 25333/49018 [00:11<00:10, 2269.86it/s]Loading:  52%|█████▏    | 25561/49018 [00:11<00:10, 2270.69it/s]Loading:  53%|█████▎    | 25789/49018 [00:11<00:10, 2264.75it/s]Loading:  53%|█████▎    | 26016/49018 [00:11<00:10, 2265.87it/s]Loading:  54%|█████▎    | 26243/49018 [00:11<00:10, 2259.04it/s]Loading:  54%|█████▍    | 26470/49018 [00:11<00:09, 2260.95it/s]Loading:  54%|█████▍    | 26697/49018 [00:11<00:09, 2258.53it/s]Loading:  55%|█████▍    | 26925/49018 [00:12<00:09, 2262.94it/s]Loading:  55%|█████▌    | 27153/49018 [00:12<00:09, 2265.29it/s]Loading:  56%|█████▌    | 27380/49018 [00:12<00:09, 2264.38it/s]Loading:  56%|█████▋    | 27608/49018 [00:12<00:09, 2266.85it/s]Loading:  57%|█████▋    | 27835/49018 [00:12<00:09, 2261.91it/s]Loading:  57%|█████▋    | 28063/49018 [00:12<00:09, 2264.74it/s]Loading:  58%|█████▊    | 28291/49018 [00:12<00:09, 2268.65it/s]Loading:  58%|█████▊    | 28518/49018 [00:12<00:09, 2267.38it/s]Loading:  59%|█████▊    | 28747/49018 [00:12<00:08, 2272.69it/s]Loading:  59%|█████▉    | 28975/49018 [00:12<00:08, 2271.73it/s]Loading:  60%|█████▉    | 29203/49018 [00:13<00:08, 2273.85it/s]Loading:  60%|██████    | 29432/49018 [00:13<00:08, 2275.95it/s]Loading:  61%|██████    | 29660/49018 [00:13<00:08, 2268.21it/s]Loading:  61%|██████    | 29888/49018 [00:13<00:08, 2270.92it/s]Loading:  61%|██████▏   | 30116/49018 [00:13<00:08, 2270.37it/s]Loading:  62%|██████▏   | 30345/49018 [00:13<00:08, 2276.08it/s]Loading:  62%|██████▏   | 30573/49018 [00:13<00:08, 2269.51it/s]Loading:  63%|██████▎   | 30800/49018 [00:13<00:08, 2269.24it/s]Loading:  63%|██████▎   | 31028/49018 [00:13<00:07, 2270.20it/s]Loading:  64%|██████▍   | 31256/49018 [00:13<00:07, 2267.33it/s]Loading:  64%|██████▍   | 31484/49018 [00:14<00:07, 2270.89it/s]Loading:  65%|██████▍   | 31712/49018 [00:14<00:07, 2266.36it/s]Loading:  65%|██████▌   | 31942/49018 [00:14<00:07, 2273.53it/s]Loading:  66%|██████▌   | 32170/49018 [00:14<00:07, 2272.72it/s]Loading:  66%|██████▌   | 32398/49018 [00:14<00:07, 2265.03it/s]Loading:  67%|██████▋   | 32625/49018 [00:14<00:07, 2265.91it/s]Loading:  67%|██████▋   | 32852/49018 [00:14<00:07, 2264.21it/s]Loading:  67%|██████▋   | 33080/49018 [00:14<00:07, 2266.60it/s]Loading:  68%|██████▊   | 33307/49018 [00:14<00:06, 2266.60it/s]Loading:  68%|██████▊   | 33535/49018 [00:14<00:06, 2269.27it/s]Loading:  69%|██████▉   | 33763/49018 [00:15<00:06, 2269.98it/s]Loading:  69%|██████▉   | 33990/49018 [00:15<00:06, 2269.07it/s]Loading:  70%|██████▉   | 34219/49018 [00:15<00:06, 2273.93it/s]Loading:  70%|███████   | 34447/49018 [00:15<00:06, 2268.39it/s]Loading:  71%|███████   | 34675/49018 [00:15<00:06, 2269.22it/s]Loading:  71%|███████   | 34904/49018 [00:15<00:06, 2273.26it/s]Loading:  72%|███████▏  | 35132/49018 [00:15<00:06, 2266.11it/s]Loading:  72%|███████▏  | 35360/49018 [00:15<00:06, 2267.97it/s]Loading:  73%|███████▎  | 35587/49018 [00:15<00:05, 2265.30it/s]Loading:  73%|███████▎  | 35815/49018 [00:15<00:05, 2269.44it/s]Loading:  74%|███████▎  | 36043/49018 [00:16<00:05, 2270.31it/s]Loading:  74%|███████▍  | 36271/49018 [00:16<00:05, 2263.76it/s]Loading:  74%|███████▍  | 36498/49018 [00:16<00:05, 2263.87it/s]Loading:  75%|███████▍  | 36725/49018 [00:16<00:05, 2262.95it/s]Loading:  75%|███████▌  | 36953/49018 [00:16<00:05, 2265.87it/s]Loading:  76%|███████▌  | 37180/49018 [00:16<00:05, 2261.68it/s]Loading:  76%|███████▋  | 37407/49018 [00:16<00:05, 2263.11it/s]Loading:  77%|███████▋  | 37635/49018 [00:16<00:05, 2266.64it/s]Loading:  77%|███████▋  | 37862/49018 [00:17<00:13, 816.74it/s] Loading:  78%|███████▊  | 38088/49018 [00:17<00:10, 1008.77it/s]Loading:  78%|███████▊  | 38314/49018 [00:17<00:08, 1208.22it/s]Loading:  79%|███████▊  | 38541/49018 [00:17<00:07, 1404.70it/s]Loading:  79%|███████▉  | 38770/49018 [00:17<00:06, 1589.86it/s]Loading:  80%|███████▉  | 38996/49018 [00:17<00:05, 1742.60it/s]Loading:  80%|████████  | 39223/49018 [00:18<00:05, 1872.75it/s]Loading:  80%|████████  | 39448/49018 [00:18<00:04, 1970.36it/s]Loading:  81%|████████  | 39675/49018 [00:18<00:04, 2051.03it/s]Loading:  81%|████████▏ | 39902/49018 [00:18<00:04, 2111.66it/s]Loading:  82%|████████▏ | 40128/49018 [00:18<00:04, 2152.31it/s]Loading:  82%|████████▏ | 40355/49018 [00:18<00:03, 2186.05it/s]Loading:  83%|████████▎ | 40582/49018 [00:18<00:03, 2208.74it/s]Loading:  83%|████████▎ | 40810/49018 [00:18<00:03, 2226.91it/s]Loading:  84%|████████▎ | 41036/49018 [00:18<00:03, 2235.33it/s]Loading:  84%|████████▍ | 41262/49018 [00:18<00:03, 2238.88it/s]Loading:  85%|████████▍ | 41490/49018 [00:19<00:03, 2248.61it/s]Loading:  85%|████████▌ | 41716/49018 [00:19<00:03, 2248.51it/s]Loading:  86%|████████▌ | 41944/49018 [00:19<00:03, 2255.98it/s]Loading:  86%|████████▌ | 42173/49018 [00:19<00:03, 2258.12it/s]Loading:  86%|████████▋ | 42400/49018 [00:19<00:02, 2260.54it/s]Loading:  87%|████████▋ | 42628/49018 [00:19<00:02, 2265.61it/s]Loading:  87%|████████▋ | 42855/49018 [00:19<00:02, 2263.71it/s]Loading:  88%|████████▊ | 43083/49018 [00:19<00:02, 2268.43it/s]Loading:  88%|████████▊ | 43310/49018 [00:19<00:02, 2267.33it/s]Loading:  89%|████████▉ | 43538/49018 [00:19<00:02, 2270.96it/s]Loading:  89%|████████▉ | 43766/49018 [00:20<00:02, 2270.48it/s]Loading:  90%|████████▉ | 43994/49018 [00:20<00:02, 2267.18it/s]Loading:  90%|█████████ | 44222/49018 [00:20<00:02, 2269.76it/s]Loading:  91%|█████████ | 44449/49018 [00:20<00:02, 2267.35it/s]Loading:  91%|█████████ | 44677/49018 [00:20<00:01, 2270.31it/s]Loading:  92%|█████████▏| 44906/49018 [00:20<00:01, 2274.04it/s]Loading:  92%|█████████▏| 45134/49018 [00:20<00:01, 2265.31it/s]Loading:  93%|█████████▎| 45362/49018 [00:20<00:01, 2267.80it/s]Loading:  93%|█████████▎| 45589/49018 [00:20<00:01, 2263.74it/s]Loading:  93%|█████████▎| 45817/49018 [00:20<00:01, 2267.52it/s]Loading:  94%|█████████▍| 46044/49018 [00:21<00:01, 2265.77it/s]Loading:  94%|█████████▍| 46272/49018 [00:21<00:01, 2269.78it/s]Loading:  95%|█████████▍| 46500/49018 [00:21<00:01, 2270.44it/s]Loading:  95%|█████████▌| 46728/49018 [00:21<00:01, 2266.78it/s]Loading:  96%|█████████▌| 46957/49018 [00:21<00:00, 2271.46it/s]Loading:  96%|█████████▋| 47185/49018 [00:21<00:00, 2265.66it/s]Loading:  97%|█████████▋| 47413/49018 [00:21<00:00, 2268.38it/s]Loading:  97%|█████████▋| 47640/49018 [00:21<00:00, 2265.46it/s]Loading:  98%|█████████▊| 47867/49018 [00:21<00:00, 2257.47it/s]Loading:  98%|█████████▊| 48095/49018 [00:21<00:00, 2261.64it/s]Loading:  99%|█████████▊| 48322/49018 [00:22<00:00, 2257.97it/s]Loading:  99%|█████████▉| 48549/49018 [00:22<00:00, 2261.15it/s]Loading: 100%|█████████▉| 48776/49018 [00:22<00:00, 2263.76it/s]Loading: 100%|█████████▉| 49003/49018 [00:22<00:00, 2261.36it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2190.60it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank24]:[W424 18:08:16.893407230 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 18:08:16.893405798 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 18:08:16.893402611 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 18:08:16.893414444 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 18:08:16.893408212 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 18:08:16.893392873 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 18:08:16.893410717 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 18:08:16.069915949 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 18:08:16.070639398 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 18:08:16.071947004 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 18:08:16.074138293 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 18:08:16.074249914 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 18:08:16.080956141 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 18:08:16.089610716 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 18:08:16.095777512 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 18:08:16.763773304 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 18:08:17.764069928 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 18:08:17.764082482 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 18:08:17.764706738 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 18:08:17.766453225 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 18:08:17.767724846 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 18:08:17.768938607 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 18:08:17.771431474 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 18:08:17.771936962 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 18:08:17.772268100 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 18:08:17.667897085 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 18:08:17.668170373 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 18:08:17.668838700 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 18:08:17.669013562 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 18:08:17.670040829 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 18:08:17.670672667 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 18:08:17.361612750 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 18:08:17.250488585 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 18:08:17.251777889 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 18:08:17.687488880 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 18:08:17.687482118 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 18:08:17.687485043 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 18:08:18.666532434 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 18:08:18.927679669 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 18:08:18.868881985 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:24<20:01, 24.52s/it]Train:   4%|▍         | 2/50 [00:24<08:17, 10.37s/it]Train:   6%|▌         | 3/50 [00:25<04:29,  5.73s/it]Train:   8%|▊         | 4/50 [00:25<02:43,  3.56s/it]Train:  10%|█         | 5/50 [00:25<01:45,  2.35s/it]Train:  12%|█▏        | 6/50 [00:25<01:11,  1.62s/it]Train:  14%|█▍        | 7/50 [00:26<00:49,  1.16s/it]Train:  16%|█▌        | 8/50 [00:26<00:36,  1.16it/s]Train:  18%|█▊        | 9/50 [00:26<00:26,  1.52it/s]Train:  20%|██        | 10/50 [00:26<00:20,  1.93it/s]Train:  22%|██▏       | 11/50 [00:26<00:16,  2.35it/s]Train:  24%|██▍       | 12/50 [00:27<00:13,  2.77it/s]Train:  26%|██▌       | 13/50 [00:27<00:11,  3.16it/s]Train:  28%|██▊       | 14/50 [00:27<00:10,  3.51it/s]Train:  30%|███       | 15/50 [00:27<00:09,  3.82it/s]Train:  32%|███▏      | 16/50 [00:27<00:08,  4.05it/s]Train:  34%|███▍      | 17/50 [00:28<00:07,  4.23it/s]Train:  36%|███▌      | 18/50 [00:28<00:07,  4.36it/s]Train:  38%|███▊      | 19/50 [00:28<00:06,  4.47it/s]Train:  40%|████      | 20/50 [00:28<00:06,  4.53it/s]Train:  42%|████▏     | 21/50 [00:29<00:06,  4.58it/s]Train:  44%|████▍     | 22/50 [00:29<00:06,  4.62it/s]Train:  46%|████▌     | 23/50 [00:29<00:05,  4.67it/s]Train:  48%|████▊     | 24/50 [00:29<00:05,  4.70it/s]Train:  50%|█████     | 25/50 [00:29<00:05,  4.72it/s]Train:  52%|█████▏    | 26/50 [00:30<00:05,  4.71it/s]Train:  54%|█████▍    | 27/50 [00:30<00:04,  4.70it/s]Train:  56%|█████▌    | 28/50 [00:30<00:04,  4.71it/s]Train:  58%|█████▊    | 29/50 [00:30<00:04,  4.72it/s]Train:  60%|██████    | 30/50 [00:30<00:04,  4.72it/s]Train:  62%|██████▏   | 31/50 [00:31<00:04,  4.72it/s]Train:  64%|██████▍   | 32/50 [00:31<00:03,  4.70it/s]Train:  66%|██████▌   | 33/50 [00:31<00:03,  4.70it/s]Train:  68%|██████▊   | 34/50 [00:31<00:03,  4.71it/s]Train:  70%|███████   | 35/50 [00:31<00:03,  4.72it/s]Train:  72%|███████▏  | 36/50 [00:32<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:32<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:32<00:02,  4.73it/s]Train:  78%|███████▊  | 39/50 [00:32<00:02,  4.73it/s]Train:  80%|████████  | 40/50 [00:33<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:33<00:01,  4.73it/s]Train:  84%|████████▍ | 42/50 [00:33<00:01,  4.73it/s]Train:  86%|████████▌ | 43/50 [00:33<00:01,  4.75it/s]Train:  88%|████████▊ | 44/50 [00:33<00:01,  4.74it/s]Train:  90%|█████████ | 45/50 [00:34<00:01,  4.73it/s]Train:  92%|█████████▏| 46/50 [00:34<00:00,  4.75it/s]Train:  94%|█████████▍| 47/50 [00:34<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:34<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:34<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  1.42it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.19it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.25it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.45it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.67it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.69it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.71it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.71it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.71it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.72it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.73it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.75it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.74it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.75it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.75it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.75it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.75it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.76it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.75it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.76it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.75it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.76it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.75it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.77it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.77it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.76it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.75it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.74it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.76it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.75it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.75it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.74it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.75it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.74it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.75it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.75it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.74it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.75it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.75it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.76it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.72it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.22it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.95it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.29it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.47it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.66it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.66it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.70it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.70it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.69it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.71it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.72it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.74it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.72it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.73it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.73it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.74it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.73it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.73it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.72it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.71it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.70it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.73it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.72it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.72it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.73it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.71it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.72it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.70it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.71it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.70it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.72it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.72it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.73it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.74it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.73it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.73it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.72it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.70it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.73it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.74it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.76it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.24it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.98it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.28it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.54it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.64it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.67it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.72it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.73it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.74it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.73it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.72it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.74it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.74it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.71it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.69it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.70it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.73it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.73it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.73it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.72it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.73it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.75it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.73it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.71it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.72it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.73it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.72it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.73it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.75it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.75it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.74it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.75it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.73it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.72it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.74it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.67it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.24it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.99it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.29it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.45it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.64it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.66it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.70it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.73it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.73it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.73it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.74it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.74it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.75it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.76it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.75it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.76it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.75it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.75it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.74it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.74it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.73it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.72it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.70it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.72it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.71it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.71it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.73it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.74it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.74it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.75it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.75it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.76it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.75it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.75it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.74it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.76it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.76it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.75it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.77it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]
0: Process 0 - Local timer:  load_data  :  96.81
0: Process 0 - Local timer:  train_validate_test  :  78.13
0: Process 0 - Local timer:  create_model  :  1.31
0: Minimum timers: 
0: load_data  :  96.81
0: train_validate_test  :  78.07
0: create_model  :  1.29
0: Maximum timers: 
0: load_data  :  97.44
0: train_validate_test  :  78.14
0: create_model  :  1.32
0: Average timers: 
0: load_data  :  97.17
0: train_validate_test  :  78.09
0: create_model  :  1.3
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 18:09:13.697980374 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 18:09:14.368392061 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 18:09:14.368692160 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 18:09:14.368842104 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 18:09:14.926615819 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 18:09:14.369578050 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 18:09:14.926618394 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 18:09:14.369620641 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 18:09:14.258305505 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 18:09:14.926631379 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 18:09:14.258294374 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 18:09:14.926611982 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 18:09:14.258280227 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 18:09:14.926600120 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 18:09:14.258301638 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 18:09:14.926628293 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 18:09:14.258351302 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 18:09:14.926603105 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 18:09:14.369834957 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 18:09:14.258444298 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 18:09:14.926621250 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 18:09:14.369950706 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 18:09:14.258438337 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 18:09:14.971214274 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 18:09:14.971234161 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 18:09:14.971235724 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 18:09:14.971249029 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 18:09:14.971289125 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 18:09:14.971526715 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 18:09:14.259664772 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 18:09:14.972251758 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 18:09:14.631736297 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 18:09:14.631712392 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 18:09:14.631709737 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 18:09:14.631724275 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 18:09:14.631724595 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 18:09:14.631720297 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 18:09:14.631734354 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 18:09:14.631749012 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 18:09:14.973737120 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 06:09:17 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 06:09:17 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_2 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 18:09:30.843434159 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.843622446 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.843669075 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.843689624 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.843726053 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.843749898 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.845302074 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:30.846880519 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 18:09:31.635675901 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.635736096 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.635741386 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.636026065 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.637462068 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.637698336 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.638944248 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.651474874 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.400921135 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.401235700 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.401242363 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.401290715 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.401300042 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.401310191 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.402990032 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.403142831 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.853554068 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854127306 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854186999 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854240641 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854244168 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854249999 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854240881 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:31.854255940 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.050982412 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.051287583 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.051432478 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.051527809 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.051536636 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.051551454 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.052785551 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:09:32.053038501 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_2 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Read attr time (sec):  0.0017347335815429688
0: read and bcast: trainset/x/variable_count 0.18288135528564453
0: read and bcast: trainset/x/variable_offset 0.3683328628540039
0: read and bcast: trainset/x/variable_dim 0.3687326908111572
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5950942039489746
0: read and bcast: trainset/edge_index/variable_offset 0.7801346778869629
0: read and bcast: trainset/edge_index/variable_dim 0.7903313636779785
0: read and bcast: trainset/edge_attr/variable_count 0.972949743270874
0: read and bcast: trainset/edge_attr/variable_offset 1.157362937927246
0: read and bcast: trainset/edge_attr/variable_dim 1.1677179336547852
0: read and bcast: trainset/pos/variable_count 1.3523266315460205
0: read and bcast: trainset/pos/variable_offset 1.538440227508545
0: read and bcast: trainset/pos/variable_dim 1.5481581687927246
0: read and bcast: trainset/energy/variable_count 1.7332561016082764
0: read and bcast: trainset/energy/variable_offset 1.9173152446746826
0: read and bcast: trainset/energy/variable_dim 1.9275844097137451
0: read and bcast: trainset/forces/variable_count 2.1099014282226562
0: read and bcast: trainset/forces/variable_offset 2.295602560043335
0: read and bcast: trainset/forces/variable_dim 2.3055288791656494
0: read and bcast: trainset/y/variable_count 2.4904592037200928
0: read and bcast: trainset/y/variable_offset 2.6749517917633057
0: read and bcast: trainset/y/variable_dim 2.6863820552825928
0: Overall time (sec):  2.6883442401885986
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.6927223205566406
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0008385181427001953
0: read and bcast: valset/x/variable_count 0.007936954498291016
0: read and bcast: valset/x/variable_offset 0.015620231628417969
0: read and bcast: valset/x/variable_dim 0.015810251235961914
0: read and bcast: valset/edge_index/variable_count 0.02289128303527832
0: read and bcast: valset/edge_index/variable_offset 0.030452728271484375
0: read and bcast: valset/edge_index/variable_dim 0.03064894676208496
0: read and bcast: valset/edge_attr/variable_count 0.03767132759094238
0: read and bcast: valset/edge_attr/variable_offset 0.04495382308959961
0: read and bcast: valset/edge_attr/variable_dim 0.04514360427856445
0: read and bcast: valset/pos/variable_count 0.052306175231933594
0: read and bcast: valset/pos/variable_offset 0.05973696708679199
0: read and bcast: valset/pos/variable_dim 0.05992531776428223
0: read and bcast: valset/energy/variable_count 0.06717085838317871
0: read and bcast: valset/energy/variable_offset 0.07470273971557617
0: read and bcast: valset/energy/variable_dim 0.07489776611328125
0: read and bcast: valset/forces/variable_count 0.081878662109375
0: read and bcast: valset/forces/variable_offset 0.08939981460571289
0: read and bcast: valset/forces/variable_dim 0.08958077430725098
0: read and bcast: valset/y/variable_count 0.09654521942138672
0: read and bcast: valset/y/variable_offset 0.10399866104125977
0: read and bcast: valset/y/variable_dim 0.10421085357666016
0: Overall time (sec):  0.10526418685913086
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.1084434986114502
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007932186126708984
0: read and bcast: testset/x/variable_count 0.007778167724609375
0: read and bcast: testset/x/variable_offset 0.015333890914916992
0: read and bcast: testset/x/variable_dim 0.01553487777709961
0: read and bcast: testset/edge_index/variable_count 0.0226287841796875
0: read and bcast: testset/edge_index/variable_offset 0.02957773208618164
0: read and bcast: testset/edge_index/variable_dim 0.029767990112304688
0: read and bcast: testset/edge_attr/variable_count 0.03723311424255371
0: read and bcast: testset/edge_attr/variable_offset 0.04465651512145996
0: read and bcast: testset/edge_attr/variable_dim 0.044855594635009766
0: read and bcast: testset/pos/variable_count 0.052234649658203125
0: read and bcast: testset/pos/variable_offset 0.05938839912414551
0: read and bcast: testset/pos/variable_dim 0.059589385986328125
0: read and bcast: testset/energy/variable_count 0.06666135787963867
0: read and bcast: testset/energy/variable_offset 0.07422590255737305
0: read and bcast: testset/energy/variable_dim 0.07442617416381836
0: read and bcast: testset/forces/variable_count 0.08178997039794922
0: read and bcast: testset/forces/variable_offset 0.08887028694152832
0: read and bcast: testset/forces/variable_dim 0.08905220031738281
0: read and bcast: testset/y/variable_count 0.09643435478210449
0: read and bcast: testset/y/variable_offset 0.10388517379760742
0: read and bcast: testset/y/variable_dim 0.1040806770324707
0: Overall time (sec):  0.10506582260131836
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10812878608703613
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
11 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
30 transition1x nsplit: 8680250 11 789114
31 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
37 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
34 transition1x nsplit: 8680250 11 789114
38 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
26 alexandria nsplit: 9705384 11 882307
19 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
22 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
24 alexandria nsplit: 9705384 11 882308
20 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
0: Adios reading time (sec):  0.47737884521484375
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.14030718803405762
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.13785266876220703
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 227/64000 [00:00<00:28, 2268.19it/s]Loading:   1%|          | 461/64000 [00:00<00:27, 2309.84it/s]Loading:   1%|          | 693/64000 [00:00<00:27, 2313.07it/s]Loading:   1%|▏         | 927/64000 [00:00<00:27, 2322.18it/s]Loading:   2%|▏         | 1160/64000 [00:00<00:27, 2322.47it/s]Loading:   2%|▏         | 1393/64000 [00:00<00:27, 2301.13it/s]Loading:   3%|▎         | 1624/64000 [00:00<00:27, 2274.97it/s]Loading:   3%|▎         | 1852/64000 [00:00<00:27, 2271.50it/s]Loading:   3%|▎         | 2080/64000 [00:00<00:27, 2272.86it/s]Loading:   4%|▎         | 2308/64000 [00:01<00:27, 2265.83it/s]Loading:   4%|▍         | 2535/64000 [00:01<00:27, 2261.62it/s]Loading:   4%|▍         | 2762/64000 [00:01<00:27, 2253.78it/s]Loading:   5%|▍         | 2988/64000 [00:01<00:27, 2254.68it/s]Loading:   5%|▌         | 3215/64000 [00:01<00:26, 2258.16it/s]Loading:   5%|▌         | 3441/64000 [00:01<00:26, 2258.05it/s]Loading:   6%|▌         | 3668/64000 [00:01<00:26, 2259.88it/s]Loading:   6%|▌         | 3894/64000 [00:01<00:40, 1490.85it/s]Loading:   6%|▋         | 4118/64000 [00:01<00:36, 1655.67it/s]Loading:   7%|▋         | 4344/64000 [00:02<00:33, 1798.77it/s]Loading:   7%|▋         | 4569/64000 [00:02<00:31, 1913.13it/s]Loading:   7%|▋         | 4795/64000 [00:02<00:29, 2004.77it/s]Loading:   8%|▊         | 5019/64000 [00:02<00:28, 2067.91it/s]Loading:   8%|▊         | 5245/64000 [00:02<00:27, 2119.68it/s]Loading:   9%|▊         | 5472/64000 [00:02<00:27, 2162.10it/s]Loading:   9%|▉         | 5697/64000 [00:02<00:26, 2185.34it/s]Loading:   9%|▉         | 5924/64000 [00:02<00:26, 2207.63it/s]Loading:  10%|▉         | 6149/64000 [00:02<00:26, 2219.57it/s]Loading:  10%|▉         | 6375/64000 [00:02<00:25, 2231.24it/s]Loading:  10%|█         | 6601/64000 [00:03<00:25, 2239.59it/s]Loading:  11%|█         | 6827/64000 [00:03<00:25, 2245.01it/s]Loading:  11%|█         | 7055/64000 [00:03<00:25, 2252.94it/s]Loading:  11%|█▏        | 7281/64000 [00:03<00:25, 2254.02it/s]Loading:  12%|█▏        | 7508/64000 [00:03<00:25, 2258.75it/s]Loading:  12%|█▏        | 7735/64000 [00:03<00:24, 2253.66it/s]Loading:  12%|█▏        | 7961/64000 [00:03<00:24, 2254.96it/s]Loading:  13%|█▎        | 8187/64000 [00:03<00:24, 2254.80it/s]Loading:  13%|█▎        | 8413/64000 [00:03<00:24, 2252.15it/s]Loading:  14%|█▎        | 8640/64000 [00:03<00:24, 2254.46it/s]Loading:  14%|█▍        | 8866/64000 [00:04<00:24, 2252.05it/s]Loading:  14%|█▍        | 9092/64000 [00:04<00:24, 2254.23it/s]Loading:  15%|█▍        | 9319/64000 [00:04<00:24, 2258.09it/s]Loading:  15%|█▍        | 9545/64000 [00:04<00:24, 2254.68it/s]Loading:  15%|█▌        | 9771/64000 [00:04<00:24, 2249.54it/s]Loading:  16%|█▌        | 9996/64000 [00:04<00:24, 2238.69it/s]Loading:  16%|█▌        | 10221/64000 [00:04<00:24, 2239.27it/s]Loading:  16%|█▋        | 10446/64000 [00:04<00:23, 2240.50it/s]Loading:  17%|█▋        | 10671/64000 [00:04<00:23, 2240.84it/s]Loading:  17%|█▋        | 10896/64000 [00:04<00:23, 2243.32it/s]Loading:  17%|█▋        | 11121/64000 [00:05<00:23, 2241.46it/s]Loading:  18%|█▊        | 11348/64000 [00:05<00:23, 2248.64it/s]Loading:  18%|█▊        | 11574/64000 [00:05<00:23, 2251.59it/s]Loading:  18%|█▊        | 11800/64000 [00:05<00:23, 2248.98it/s]Loading:  19%|█▉        | 12025/64000 [00:05<00:23, 2249.06it/s]Loading:  19%|█▉        | 12250/64000 [00:05<00:34, 1518.90it/s]Loading:  19%|█▉        | 12475/64000 [00:05<00:30, 1681.42it/s]Loading:  20%|█▉        | 12701/64000 [00:05<00:28, 1820.59it/s]Loading:  20%|██        | 12925/64000 [00:06<00:26, 1928.19it/s]Loading:  21%|██        | 13151/64000 [00:06<00:25, 2016.20it/s]Loading:  21%|██        | 13377/64000 [00:06<00:24, 2081.18it/s]Loading:  21%|██▏       | 13603/64000 [00:06<00:23, 2130.39it/s]Loading:  22%|██▏       | 13830/64000 [00:06<00:23, 2169.81it/s]Loading:  22%|██▏       | 14055/64000 [00:06<00:22, 2192.41it/s]Loading:  22%|██▏       | 14281/64000 [00:06<00:22, 2210.98it/s]Loading:  23%|██▎       | 14506/64000 [00:06<00:22, 2220.75it/s]Loading:  23%|██▎       | 14733/64000 [00:06<00:22, 2234.89it/s]Loading:  23%|██▎       | 14959/64000 [00:06<00:21, 2240.26it/s]Loading:  24%|██▎       | 15185/64000 [00:07<00:21, 2244.25it/s]Loading:  24%|██▍       | 15411/64000 [00:07<00:21, 2247.77it/s]Loading:  24%|██▍       | 15637/64000 [00:07<00:21, 2249.65it/s]Loading:  25%|██▍       | 15863/64000 [00:07<00:21, 2252.24it/s]Loading:  25%|██▌       | 16089/64000 [00:07<00:21, 2250.82it/s]Loading:  25%|██▌       | 16315/64000 [00:07<00:21, 2253.39it/s]Loading:  26%|██▌       | 16541/64000 [00:07<00:21, 2251.70it/s]Loading:  26%|██▌       | 16767/64000 [00:07<00:21, 2238.15it/s]Loading:  27%|██▋       | 16993/64000 [00:07<00:20, 2242.28it/s]Loading:  27%|██▋       | 17219/64000 [00:07<00:20, 2247.20it/s]Loading:  27%|██▋       | 17444/64000 [00:08<00:20, 2241.80it/s]Loading:  28%|██▊       | 17669/64000 [00:08<00:20, 2239.92it/s]Loading:  28%|██▊       | 17894/64000 [00:08<00:20, 2241.59it/s]Loading:  28%|██▊       | 18119/64000 [00:08<00:20, 2243.17it/s]Loading:  29%|██▊       | 18344/64000 [00:08<00:20, 2240.73it/s]Loading:  29%|██▉       | 18569/64000 [00:08<00:20, 2243.34it/s]Loading:  29%|██▉       | 18796/64000 [00:08<00:20, 2250.39it/s]Loading:  30%|██▉       | 19022/64000 [00:08<00:19, 2251.72it/s]Loading:  30%|███       | 19248/64000 [00:08<00:19, 2252.40it/s]Loading:  30%|███       | 19474/64000 [00:08<00:19, 2248.36it/s]Loading:  31%|███       | 19700/64000 [00:09<00:19, 2249.69it/s]Loading:  31%|███       | 19925/64000 [00:09<00:19, 2247.52it/s]Loading:  31%|███▏      | 20151/64000 [00:09<00:19, 2249.95it/s]Loading:  32%|███▏      | 20377/64000 [00:09<00:19, 2252.53it/s]Loading:  32%|███▏      | 20603/64000 [00:09<00:19, 2247.64it/s]Loading:  33%|███▎      | 20828/64000 [00:09<00:19, 2245.26it/s]Loading:  33%|███▎      | 21053/64000 [00:09<00:19, 2245.97it/s]Loading:  33%|███▎      | 21279/64000 [00:09<00:19, 2247.65it/s]Loading:  34%|███▎      | 21504/64000 [00:09<00:18, 2243.36it/s]Loading:  34%|███▍      | 21729/64000 [00:09<00:18, 2241.70it/s]Loading:  34%|███▍      | 21955/64000 [00:10<00:18, 2245.83it/s]Loading:  35%|███▍      | 22180/64000 [00:10<00:29, 1423.00it/s]Loading:  35%|███▌      | 22404/64000 [00:10<00:26, 1596.05it/s]Loading:  35%|███▌      | 22629/64000 [00:10<00:23, 1746.88it/s]Loading:  36%|███▌      | 22853/64000 [00:10<00:22, 1868.76it/s]Loading:  36%|███▌      | 23077/64000 [00:10<00:20, 1965.32it/s]Loading:  36%|███▋      | 23303/64000 [00:10<00:19, 2042.98it/s]Loading:  37%|███▋      | 23528/64000 [00:10<00:19, 2098.55it/s]Loading:  37%|███▋      | 23753/64000 [00:11<00:18, 2139.92it/s]Loading:  37%|███▋      | 23978/64000 [00:11<00:18, 2170.46it/s]Loading:  38%|███▊      | 24204/64000 [00:11<00:18, 2194.98it/s]Loading:  38%|███▊      | 24430/64000 [00:11<00:17, 2213.54it/s]Loading:  39%|███▊      | 24657/64000 [00:11<00:17, 2228.40it/s]Loading:  39%|███▉      | 24882/64000 [00:11<00:17, 2233.53it/s]Loading:  39%|███▉      | 25107/64000 [00:11<00:17, 2230.73it/s]Loading:  40%|███▉      | 25332/64000 [00:11<00:17, 2233.90it/s]Loading:  40%|███▉      | 25557/64000 [00:11<00:17, 2236.36it/s]Loading:  40%|████      | 25782/64000 [00:11<00:17, 2238.69it/s]Loading:  41%|████      | 26007/64000 [00:12<00:16, 2238.75it/s]Loading:  41%|████      | 26232/64000 [00:12<00:16, 2236.59it/s]Loading:  41%|████▏     | 26456/64000 [00:12<00:16, 2237.45it/s]Loading:  42%|████▏     | 26680/64000 [00:12<00:16, 2232.42it/s]Loading:  42%|████▏     | 26904/64000 [00:12<00:16, 2234.13it/s]Loading:  42%|████▏     | 27128/64000 [00:12<00:16, 2234.40it/s]Loading:  43%|████▎     | 27354/64000 [00:12<00:16, 2240.71it/s]Loading:  43%|████▎     | 27581/64000 [00:12<00:16, 2247.28it/s]Loading:  43%|████▎     | 27806/64000 [00:12<00:16, 2238.93it/s]Loading:  44%|████▍     | 28030/64000 [00:12<00:16, 2236.35it/s]Loading:  44%|████▍     | 28254/64000 [00:13<00:16, 2229.61it/s]Loading:  44%|████▍     | 28479/64000 [00:13<00:15, 2232.97it/s]Loading:  45%|████▍     | 28704/64000 [00:13<00:15, 2236.07it/s]Loading:  45%|████▌     | 28928/64000 [00:13<00:15, 2233.69it/s]Loading:  46%|████▌     | 29153/64000 [00:13<00:15, 2238.20it/s]Loading:  46%|████▌     | 29377/64000 [00:13<00:15, 2235.82it/s]Loading:  46%|████▋     | 29601/64000 [00:13<00:15, 2233.90it/s]Loading:  47%|████▋     | 29826/64000 [00:13<00:15, 2237.64it/s]Loading:  47%|████▋     | 30050/64000 [00:13<00:15, 2235.56it/s]Loading:  47%|████▋     | 30274/64000 [00:13<00:15, 2232.47it/s]Loading:  48%|████▊     | 30498/64000 [00:14<00:15, 2226.91it/s]Loading:  48%|████▊     | 30721/64000 [00:14<00:14, 2227.23it/s]Loading:  48%|████▊     | 30947/64000 [00:14<00:14, 2236.07it/s]Loading:  49%|████▊     | 31171/64000 [00:14<00:14, 2229.06it/s]Loading:  49%|████▉     | 31394/64000 [00:14<00:14, 2226.59it/s]Loading:  49%|████▉     | 31617/64000 [00:14<00:14, 2225.07it/s]Loading:  50%|████▉     | 31842/64000 [00:14<00:14, 2231.75it/s]Loading:  50%|█████     | 32066/64000 [00:14<00:14, 2232.89it/s]Loading:  50%|█████     | 32290/64000 [00:14<00:14, 2234.52it/s]Loading:  51%|█████     | 32516/64000 [00:14<00:14, 2240.99it/s]Loading:  51%|█████     | 32741/64000 [00:15<00:13, 2237.79it/s]Loading:  52%|█████▏    | 32966/64000 [00:15<00:13, 2240.70it/s]Loading:  52%|█████▏    | 33193/64000 [00:15<00:13, 2243.40it/s]Loading:  52%|█████▏    | 33418/64000 [00:15<00:13, 2244.44it/s]Loading:  53%|█████▎    | 33644/64000 [00:15<00:13, 2247.01it/s]Loading:  53%|█████▎    | 33869/64000 [00:15<00:13, 2242.17it/s]Loading:  53%|█████▎    | 34094/64000 [00:15<00:13, 2237.51it/s]Loading:  54%|█████▎    | 34318/64000 [00:15<00:13, 2237.14it/s]Loading:  54%|█████▍    | 34543/64000 [00:15<00:13, 2239.99it/s]Loading:  54%|█████▍    | 34769/64000 [00:15<00:13, 2244.92it/s]Loading:  55%|█████▍    | 34994/64000 [00:16<00:22, 1297.08it/s]Loading:  55%|█████▌    | 35218/64000 [00:16<00:19, 1483.14it/s]Loading:  55%|█████▌    | 35441/64000 [00:16<00:17, 1647.29it/s]Loading:  56%|█████▌    | 35666/64000 [00:16<00:15, 1789.87it/s]Loading:  56%|█████▌    | 35891/64000 [00:16<00:14, 1905.96it/s]Loading:  56%|█████▋    | 36114/64000 [00:16<00:14, 1990.85it/s]Loading:  57%|█████▋    | 36338/64000 [00:16<00:13, 2059.39it/s]Loading:  57%|█████▋    | 36562/64000 [00:17<00:13, 2108.04it/s]Loading:  57%|█████▋    | 36786/64000 [00:17<00:12, 2145.79it/s]Loading:  58%|█████▊    | 37013/64000 [00:17<00:12, 2180.52it/s]Loading:  58%|█████▊    | 37238/64000 [00:17<00:12, 2198.36it/s]Loading:  59%|█████▊    | 37463/64000 [00:17<00:11, 2211.72it/s]Loading:  59%|█████▉    | 37687/64000 [00:17<00:11, 2212.12it/s]Loading:  59%|█████▉    | 37912/64000 [00:17<00:11, 2222.19it/s]Loading:  60%|█████▉    | 38137/64000 [00:17<00:11, 2229.59it/s]Loading:  60%|█████▉    | 38363/64000 [00:17<00:11, 2236.64it/s]Loading:  60%|██████    | 38590/64000 [00:17<00:11, 2245.24it/s]Loading:  61%|██████    | 38815/64000 [00:18<00:11, 2240.96it/s]Loading:  61%|██████    | 39043/64000 [00:18<00:11, 2249.96it/s]Loading:  61%|██████▏   | 39270/64000 [00:18<00:10, 2254.95it/s]Loading:  62%|██████▏   | 39496/64000 [00:18<00:10, 2249.49it/s]Loading:  62%|██████▏   | 39723/64000 [00:18<00:10, 2252.91it/s]Loading:  62%|██████▏   | 39949/64000 [00:18<00:10, 2252.27it/s]Loading:  63%|██████▎   | 40175/64000 [00:18<00:10, 2251.39it/s]Loading:  63%|██████▎   | 40401/64000 [00:18<00:10, 2252.07it/s]Loading:  63%|██████▎   | 40627/64000 [00:18<00:10, 2249.05it/s]Loading:  64%|██████▍   | 40852/64000 [00:18<00:10, 2246.20it/s]Loading:  64%|██████▍   | 41077/64000 [00:19<00:10, 2241.35it/s]Loading:  65%|██████▍   | 41303/64000 [00:19<00:10, 2244.95it/s]Loading:  65%|██████▍   | 41528/64000 [00:19<00:10, 2241.62it/s]Loading:  65%|██████▌   | 41753/64000 [00:19<00:09, 2243.28it/s]Loading:  66%|██████▌   | 41979/64000 [00:19<00:09, 2248.00it/s]Loading:  66%|██████▌   | 42204/64000 [00:19<00:09, 2245.44it/s]Loading:  66%|██████▋   | 42429/64000 [00:19<00:09, 2242.61it/s]Loading:  67%|██████▋   | 42654/64000 [00:19<00:09, 2237.64it/s]Loading:  67%|██████▋   | 42878/64000 [00:19<00:09, 2236.97it/s]Loading:  67%|██████▋   | 43102/64000 [00:19<00:09, 2232.36it/s]Loading:  68%|██████▊   | 43326/64000 [00:20<00:09, 2219.55it/s]Loading:  68%|██████▊   | 43548/64000 [00:20<00:09, 2213.67it/s]Loading:  68%|██████▊   | 43770/64000 [00:20<00:09, 2210.44it/s]Loading:  69%|██████▊   | 43993/64000 [00:20<00:09, 2214.55it/s]Loading:  69%|██████▉   | 44217/64000 [00:20<00:08, 2221.01it/s]Loading:  69%|██████▉   | 44440/64000 [00:20<00:08, 2223.53it/s]Loading:  70%|██████▉   | 44664/64000 [00:20<00:08, 2226.75it/s]Loading:  70%|███████   | 44887/64000 [00:20<00:08, 2225.61it/s]Loading:  70%|███████   | 45112/64000 [00:20<00:08, 2231.06it/s]Loading:  71%|███████   | 45336/64000 [00:20<00:08, 2231.86it/s]Loading:  71%|███████   | 45560/64000 [00:21<00:08, 2231.52it/s]Loading:  72%|███████▏  | 45785/64000 [00:21<00:08, 2235.54it/s]Loading:  72%|███████▏  | 46009/64000 [00:21<00:08, 2234.09it/s]Loading:  72%|███████▏  | 46233/64000 [00:21<00:07, 2235.73it/s]Loading:  73%|███████▎  | 46458/64000 [00:21<00:07, 2237.65it/s]Loading:  73%|███████▎  | 46682/64000 [00:21<00:07, 2236.37it/s]Loading:  73%|███████▎  | 46907/64000 [00:21<00:07, 2237.87it/s]Loading:  74%|███████▎  | 47131/64000 [00:21<00:07, 2234.61it/s]Loading:  74%|███████▍  | 47356/64000 [00:21<00:07, 2237.18it/s]Loading:  74%|███████▍  | 47582/64000 [00:21<00:07, 2242.34it/s]Loading:  75%|███████▍  | 47807/64000 [00:22<00:07, 2235.82it/s]Loading:  75%|███████▌  | 48031/64000 [00:22<00:07, 2236.37it/s]Loading:  75%|███████▌  | 48255/64000 [00:22<00:07, 2234.13it/s]Loading:  76%|███████▌  | 48479/64000 [00:22<00:06, 2235.52it/s]Loading:  76%|███████▌  | 48703/64000 [00:22<00:06, 2234.83it/s]Loading:  76%|███████▋  | 48928/64000 [00:22<00:06, 2237.92it/s]Loading:  77%|███████▋  | 49153/64000 [00:22<00:06, 2240.03it/s]Loading:  77%|███████▋  | 49378/64000 [00:22<00:06, 2237.36it/s]Loading:  78%|███████▊  | 49605/64000 [00:22<00:06, 2246.04it/s]Loading:  78%|███████▊  | 49830/64000 [00:22<00:06, 2236.64it/s]Loading:  78%|███████▊  | 50054/64000 [00:23<00:06, 2230.47it/s]Loading:  79%|███████▊  | 50279/64000 [00:23<00:06, 2233.70it/s]Loading:  79%|███████▉  | 50503/64000 [00:23<00:06, 2232.37it/s]Loading:  79%|███████▉  | 50728/64000 [00:23<00:05, 2234.84it/s]Loading:  80%|███████▉  | 50952/64000 [00:23<00:10, 1202.34it/s]Loading:  80%|███████▉  | 51176/64000 [00:23<00:09, 1394.88it/s]Loading:  80%|████████  | 51400/64000 [00:23<00:08, 1571.91it/s]Loading:  81%|████████  | 51625/64000 [00:24<00:07, 1726.86it/s]Loading:  81%|████████  | 51849/64000 [00:24<00:06, 1853.61it/s]Loading:  81%|████████▏ | 52072/64000 [00:24<00:06, 1950.15it/s]Loading:  82%|████████▏ | 52297/64000 [00:24<00:05, 2030.12it/s]Loading:  82%|████████▏ | 52523/64000 [00:24<00:05, 2092.06it/s]Loading:  82%|████████▏ | 52746/64000 [00:24<00:05, 2129.21it/s]Loading:  83%|████████▎ | 52971/64000 [00:24<00:05, 2163.80it/s]Loading:  83%|████████▎ | 53194/64000 [00:24<00:04, 2180.84it/s]Loading:  83%|████████▎ | 53419/64000 [00:24<00:04, 2199.92it/s]Loading:  84%|████████▍ | 53644/64000 [00:24<00:04, 2212.21it/s]Loading:  84%|████████▍ | 53868/64000 [00:25<00:04, 2219.57it/s]Loading:  85%|████████▍ | 54093/64000 [00:25<00:04, 2227.07it/s]Loading:  85%|████████▍ | 54317/64000 [00:25<00:04, 2227.12it/s]Loading:  85%|████████▌ | 54541/64000 [00:25<00:04, 2228.26it/s]Loading:  86%|████████▌ | 54765/64000 [00:25<00:04, 2227.58it/s]Loading:  86%|████████▌ | 54989/64000 [00:25<00:04, 2222.46it/s]Loading:  86%|████████▋ | 55212/64000 [00:25<00:03, 2224.47it/s]Loading:  87%|████████▋ | 55435/64000 [00:25<00:03, 2217.53it/s]Loading:  87%|████████▋ | 55659/64000 [00:25<00:03, 2222.32it/s]Loading:  87%|████████▋ | 55883/64000 [00:25<00:03, 2226.45it/s]Loading:  88%|████████▊ | 56106/64000 [00:26<00:03, 2224.56it/s]Loading:  88%|████████▊ | 56329/64000 [00:26<00:03, 2223.42it/s]Loading:  88%|████████▊ | 56553/64000 [00:26<00:03, 2225.84it/s]Loading:  89%|████████▊ | 56779/64000 [00:26<00:03, 2233.63it/s]Loading:  89%|████████▉ | 57005/64000 [00:26<00:03, 2240.79it/s]Loading:  89%|████████▉ | 57230/64000 [00:26<00:03, 2233.62it/s]Loading:  90%|████████▉ | 57455/64000 [00:26<00:02, 2237.31it/s]Loading:  90%|█████████ | 57679/64000 [00:26<00:02, 2237.13it/s]Loading:  90%|█████████ | 57903/64000 [00:26<00:02, 2235.47it/s]Loading:  91%|█████████ | 58127/64000 [00:26<00:02, 2225.95it/s]Loading:  91%|█████████ | 58351/64000 [00:27<00:02, 2228.55it/s]Loading:  92%|█████████▏| 58574/64000 [00:27<00:02, 2226.76it/s]Loading:  92%|█████████▏| 58797/64000 [00:27<00:02, 2224.90it/s]Loading:  92%|█████████▏| 59022/64000 [00:27<00:02, 2231.31it/s]Loading:  93%|█████████▎| 59247/64000 [00:27<00:02, 2234.50it/s]Loading:  93%|█████████▎| 59472/64000 [00:27<00:02, 2236.76it/s]Loading:  93%|█████████▎| 59697/64000 [00:27<00:01, 2239.71it/s]Loading:  94%|█████████▎| 59921/64000 [00:27<00:01, 2235.06it/s]Loading:  94%|█████████▍| 60146/64000 [00:27<00:01, 2237.56it/s]Loading:  94%|█████████▍| 60370/64000 [00:27<00:01, 2235.78it/s]Loading:  95%|█████████▍| 60595/64000 [00:28<00:01, 2236.94it/s]Loading:  95%|█████████▌| 60819/64000 [00:28<00:01, 2236.38it/s]Loading:  95%|█████████▌| 61043/64000 [00:28<00:01, 2230.25it/s]Loading:  96%|█████████▌| 61267/64000 [00:28<00:01, 2229.17it/s]Loading:  96%|█████████▌| 61490/64000 [00:28<00:01, 2226.38it/s]Loading:  96%|█████████▋| 61714/64000 [00:28<00:01, 2229.29it/s]Loading:  97%|█████████▋| 61938/64000 [00:28<00:00, 2231.67it/s]Loading:  97%|█████████▋| 62162/64000 [00:28<00:00, 2227.17it/s]Loading:  97%|█████████▋| 62385/64000 [00:28<00:00, 2227.93it/s]Loading:  98%|█████████▊| 62609/64000 [00:28<00:00, 2230.35it/s]Loading:  98%|█████████▊| 62833/64000 [00:29<00:00, 2230.38it/s]Loading:  99%|█████████▊| 63057/64000 [00:29<00:00, 2230.04it/s]Loading:  99%|█████████▉| 63281/64000 [00:29<00:00, 2228.83it/s]Loading:  99%|█████████▉| 63505/64000 [00:29<00:00, 2231.21it/s]Loading: 100%|█████████▉| 63729/64000 [00:29<00:00, 2228.69it/s]Loading: 100%|█████████▉| 63953/64000 [00:29<00:00, 2230.69it/s]Loading: 100%|██████████| 64000/64000 [00:29<00:00, 2162.05it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 226/49018 [00:00<00:21, 2253.59it/s]Loading:   1%|          | 457/49018 [00:00<00:21, 2281.52it/s]Loading:   1%|▏         | 689/49018 [00:00<00:21, 2294.51it/s]Loading:   2%|▏         | 919/49018 [00:00<00:20, 2294.50it/s]Loading:   2%|▏         | 1149/49018 [00:00<00:20, 2292.74it/s]Loading:   3%|▎         | 1379/49018 [00:00<00:20, 2291.58it/s]Loading:   3%|▎         | 1610/49018 [00:00<00:20, 2295.37it/s]Loading:   4%|▍         | 1841/49018 [00:00<00:20, 2298.31it/s]Loading:   4%|▍         | 2071/49018 [00:00<00:20, 2294.89it/s]Loading:   5%|▍         | 2301/49018 [00:01<00:20, 2296.07it/s]Loading:   5%|▌         | 2531/49018 [00:01<00:20, 2293.41it/s]Loading:   6%|▌         | 2761/49018 [00:01<00:20, 2294.66it/s]Loading:   6%|▌         | 2992/49018 [00:01<00:20, 2296.29it/s]Loading:   7%|▋         | 3222/49018 [00:01<00:19, 2293.97it/s]Loading:   7%|▋         | 3452/49018 [00:01<00:19, 2294.34it/s]Loading:   8%|▊         | 3682/49018 [00:01<00:19, 2288.96it/s]Loading:   8%|▊         | 3912/49018 [00:01<00:19, 2289.92it/s]Loading:   8%|▊         | 4141/49018 [00:01<00:19, 2288.31it/s]Loading:   9%|▉         | 4371/49018 [00:01<00:19, 2289.96it/s]Loading:   9%|▉         | 4603/49018 [00:02<00:19, 2296.10it/s]Loading:  10%|▉         | 4833/49018 [00:02<00:19, 2291.11it/s]Loading:  10%|█         | 5063/49018 [00:02<00:19, 2292.15it/s]Loading:  11%|█         | 5293/49018 [00:02<00:19, 2289.80it/s]Loading:  11%|█▏        | 5524/49018 [00:02<00:18, 2294.89it/s]Loading:  12%|█▏        | 5754/49018 [00:02<00:18, 2293.94it/s]Loading:  12%|█▏        | 5984/49018 [00:02<00:18, 2289.35it/s]Loading:  13%|█▎        | 6214/49018 [00:02<00:18, 2290.94it/s]Loading:  13%|█▎        | 6444/49018 [00:02<00:18, 2287.13it/s]Loading:  14%|█▎        | 6675/49018 [00:02<00:18, 2291.50it/s]Loading:  14%|█▍        | 6905/49018 [00:03<00:18, 2289.41it/s]Loading:  15%|█▍        | 7135/49018 [00:03<00:18, 2289.88it/s]Loading:  15%|█▌        | 7365/49018 [00:03<00:18, 2291.45it/s]Loading:  15%|█▌        | 7595/49018 [00:03<00:18, 2285.17it/s]Loading:  16%|█▌        | 7824/49018 [00:03<00:18, 2282.38it/s]Loading:  16%|█▋        | 8053/49018 [00:03<00:17, 2275.86it/s]Loading:  17%|█▋        | 8281/49018 [00:03<00:17, 2275.55it/s]Loading:  17%|█▋        | 8509/49018 [00:03<00:17, 2275.11it/s]Loading:  18%|█▊        | 8737/49018 [00:03<00:17, 2275.99it/s]Loading:  18%|█▊        | 8966/49018 [00:03<00:17, 2278.68it/s]Loading:  19%|█▉        | 9194/49018 [00:04<00:17, 2274.39it/s]Loading:  19%|█▉        | 9423/49018 [00:04<00:17, 2278.06it/s]Loading:  20%|█▉        | 9651/49018 [00:04<00:17, 2276.50it/s]Loading:  20%|██        | 9879/49018 [00:04<00:17, 2271.10it/s]Loading:  21%|██        | 10107/49018 [00:04<00:17, 2273.57it/s]Loading:  21%|██        | 10335/49018 [00:04<00:17, 2271.21it/s]Loading:  22%|██▏       | 10564/49018 [00:04<00:16, 2274.58it/s]Loading:  22%|██▏       | 10792/49018 [00:04<00:16, 2273.09it/s]Loading:  22%|██▏       | 11020/49018 [00:04<00:16, 2273.21it/s]Loading:  23%|██▎       | 11248/49018 [00:04<00:16, 2272.56it/s]Loading:  23%|██▎       | 11476/49018 [00:05<00:16, 2270.73it/s]Loading:  24%|██▍       | 11704/49018 [00:05<00:16, 2272.76it/s]Loading:  24%|██▍       | 11932/49018 [00:05<00:16, 2269.83it/s]Loading:  25%|██▍       | 12160/49018 [00:05<00:16, 2272.56it/s]Loading:  25%|██▌       | 12388/49018 [00:05<00:16, 2274.23it/s]Loading:  26%|██▌       | 12616/49018 [00:05<00:16, 2274.26it/s]Loading:  26%|██▌       | 12845/49018 [00:05<00:15, 2276.53it/s]Loading:  27%|██▋       | 13073/49018 [00:05<00:15, 2271.88it/s]Loading:  27%|██▋       | 13301/49018 [00:05<00:15, 2256.95it/s]Loading:  28%|██▊       | 13527/49018 [00:05<00:15, 2240.23it/s]Loading:  28%|██▊       | 13752/49018 [00:06<00:15, 2233.88it/s]Loading:  29%|██▊       | 13976/49018 [00:06<00:15, 2228.30it/s]Loading:  29%|██▉       | 14199/49018 [00:06<00:15, 2221.68it/s]Loading:  29%|██▉       | 14422/49018 [00:06<00:15, 2221.71it/s]Loading:  30%|██▉       | 14645/49018 [00:06<00:15, 2217.81it/s]Loading:  30%|███       | 14869/49018 [00:06<00:15, 2222.26it/s]Loading:  31%|███       | 15092/49018 [00:06<00:15, 2223.36it/s]Loading:  31%|███       | 15315/49018 [00:06<00:15, 2221.83it/s]Loading:  32%|███▏      | 15539/49018 [00:06<00:15, 2225.28it/s]Loading:  32%|███▏      | 15762/49018 [00:06<00:14, 2217.78it/s]Loading:  33%|███▎      | 15985/49018 [00:07<00:14, 2220.55it/s]Loading:  33%|███▎      | 16209/49018 [00:07<00:14, 2224.22it/s]Loading:  34%|███▎      | 16432/49018 [00:07<00:14, 2223.13it/s]Loading:  34%|███▍      | 16655/49018 [00:07<00:14, 2223.11it/s]Loading:  34%|███▍      | 16878/49018 [00:07<00:14, 2220.63it/s]Loading:  35%|███▍      | 17101/49018 [00:07<00:14, 2221.55it/s]Loading:  35%|███▌      | 17324/49018 [00:07<00:14, 2218.78it/s]Loading:  36%|███▌      | 17546/49018 [00:07<00:14, 2213.41it/s]Loading:  36%|███▌      | 17769/49018 [00:07<00:14, 2215.32it/s]Loading:  37%|███▋      | 17991/49018 [00:08<00:30, 1029.59it/s]Loading:  37%|███▋      | 18210/49018 [00:08<00:25, 1221.41it/s]Loading:  38%|███▊      | 18431/49018 [00:08<00:21, 1409.57it/s]Loading:  38%|███▊      | 18651/49018 [00:08<00:19, 1578.70it/s]Loading:  39%|███▊      | 18873/49018 [00:08<00:17, 1728.46it/s]Loading:  39%|███▉      | 19093/49018 [00:08<00:16, 1846.30it/s]Loading:  39%|███▉      | 19316/49018 [00:08<00:15, 1946.23it/s]Loading:  40%|███▉      | 19538/49018 [00:09<00:14, 2020.53it/s]Loading:  40%|████      | 19758/49018 [00:09<00:14, 2070.00it/s]Loading:  41%|████      | 19979/49018 [00:09<00:13, 2109.46it/s]Loading:  41%|████      | 20200/49018 [00:09<00:13, 2136.13it/s]Loading:  42%|████▏     | 20422/49018 [00:09<00:13, 2158.59it/s]Loading:  42%|████▏     | 20645/49018 [00:09<00:13, 2177.56it/s]Loading:  43%|████▎     | 20866/49018 [00:09<00:12, 2184.55it/s]Loading:  43%|████▎     | 21090/49018 [00:09<00:12, 2198.54it/s]Loading:  43%|████▎     | 21312/49018 [00:09<00:12, 2200.74it/s]Loading:  44%|████▍     | 21534/49018 [00:09<00:12, 2204.71it/s]Loading:  44%|████▍     | 21756/49018 [00:10<00:12, 2206.75it/s]Loading:  45%|████▍     | 21978/49018 [00:10<00:12, 2203.36it/s]Loading:  45%|████▌     | 22199/49018 [00:10<00:12, 2203.39it/s]Loading:  46%|████▌     | 22420/49018 [00:10<00:12, 2202.54it/s]Loading:  46%|████▌     | 22642/49018 [00:10<00:11, 2207.37it/s]Loading:  47%|████▋     | 22864/49018 [00:10<00:11, 2209.22it/s]Loading:  47%|████▋     | 23086/49018 [00:10<00:11, 2208.12it/s]Loading:  48%|████▊     | 23309/49018 [00:10<00:11, 2212.09it/s]Loading:  48%|████▊     | 23531/49018 [00:10<00:11, 2202.45it/s]Loading:  48%|████▊     | 23752/49018 [00:10<00:11, 2203.06it/s]Loading:  49%|████▉     | 23974/49018 [00:11<00:11, 2207.04it/s]Loading:  49%|████▉     | 24195/49018 [00:11<00:11, 2204.82it/s]Loading:  50%|████▉     | 24419/49018 [00:11<00:11, 2212.21it/s]Loading:  50%|█████     | 24641/49018 [00:11<00:11, 2209.61it/s]Loading:  51%|█████     | 24862/49018 [00:11<00:10, 2209.39it/s]Loading:  51%|█████     | 25083/49018 [00:11<00:10, 2206.23it/s]Loading:  52%|█████▏    | 25304/49018 [00:11<00:10, 2201.10it/s]Loading:  52%|█████▏    | 25525/49018 [00:11<00:10, 2200.92it/s]Loading:  53%|█████▎    | 25746/49018 [00:11<00:10, 2200.56it/s]Loading:  53%|█████▎    | 25969/49018 [00:11<00:10, 2206.84it/s]Loading:  53%|█████▎    | 26190/49018 [00:12<00:10, 2206.98it/s]Loading:  54%|█████▍    | 26411/49018 [00:12<00:10, 2201.39it/s]Loading:  54%|█████▍    | 26633/49018 [00:12<00:10, 2204.98it/s]Loading:  55%|█████▍    | 26854/49018 [00:12<00:10, 2202.45it/s]Loading:  55%|█████▌    | 27076/49018 [00:12<00:09, 2206.42it/s]Loading:  56%|█████▌    | 27299/49018 [00:12<00:09, 2211.07it/s]Loading:  56%|█████▌    | 27521/49018 [00:12<00:09, 2211.10it/s]Loading:  57%|█████▋    | 27744/49018 [00:12<00:09, 2215.31it/s]Loading:  57%|█████▋    | 27966/49018 [00:12<00:09, 2210.77it/s]Loading:  58%|█████▊    | 28188/49018 [00:12<00:09, 2208.97it/s]Loading:  58%|█████▊    | 28410/49018 [00:13<00:09, 2209.46it/s]Loading:  58%|█████▊    | 28631/49018 [00:13<00:09, 2206.20it/s]Loading:  59%|█████▉    | 28853/49018 [00:13<00:09, 2208.43it/s]Loading:  59%|█████▉    | 29074/49018 [00:13<00:09, 2206.66it/s]Loading:  60%|█████▉    | 29297/49018 [00:13<00:08, 2212.08it/s]Loading:  60%|██████    | 29519/49018 [00:13<00:08, 2210.31it/s]Loading:  61%|██████    | 29741/49018 [00:13<00:08, 2205.84it/s]Loading:  61%|██████    | 29963/49018 [00:13<00:08, 2208.21it/s]Loading:  62%|██████▏   | 30185/49018 [00:13<00:08, 2209.07it/s]Loading:  62%|██████▏   | 30408/49018 [00:13<00:08, 2212.85it/s]Loading:  62%|██████▏   | 30631/49018 [00:14<00:08, 2217.04it/s]Loading:  63%|██████▎   | 30853/49018 [00:14<00:08, 2216.40it/s]Loading:  63%|██████▎   | 31075/49018 [00:14<00:08, 2216.16it/s]Loading:  64%|██████▍   | 31297/49018 [00:14<00:08, 2212.48it/s]Loading:  64%|██████▍   | 31519/49018 [00:14<00:07, 2212.41it/s]Loading:  65%|██████▍   | 31742/49018 [00:14<00:07, 2215.76it/s]Loading:  65%|██████▌   | 31964/49018 [00:14<00:07, 2212.31it/s]Loading:  66%|██████▌   | 32188/49018 [00:14<00:07, 2217.95it/s]Loading:  66%|██████▌   | 32410/49018 [00:14<00:07, 2217.29it/s]Loading:  67%|██████▋   | 32633/49018 [00:14<00:07, 2219.87it/s]Loading:  67%|██████▋   | 32856/49018 [00:15<00:07, 2220.94it/s]Loading:  67%|██████▋   | 33079/49018 [00:15<00:07, 2222.46it/s]Loading:  68%|██████▊   | 33303/49018 [00:15<00:07, 2225.40it/s]Loading:  68%|██████▊   | 33526/49018 [00:15<00:06, 2223.65it/s]Loading:  69%|██████▉   | 33749/49018 [00:15<00:06, 2224.98it/s]Loading:  69%|██████▉   | 33972/49018 [00:15<00:06, 2226.36it/s]Loading:  70%|██████▉   | 34195/49018 [00:15<00:06, 2222.08it/s]Loading:  70%|███████   | 34418/49018 [00:15<00:06, 2221.85it/s]Loading:  71%|███████   | 34641/49018 [00:15<00:06, 2218.91it/s]Loading:  71%|███████   | 34864/49018 [00:15<00:06, 2220.79it/s]Loading:  72%|███████▏  | 35087/49018 [00:16<00:06, 2221.60it/s]Loading:  72%|███████▏  | 35310/49018 [00:16<00:06, 2219.86it/s]Loading:  72%|███████▏  | 35533/49018 [00:16<00:06, 2220.58it/s]Loading:  73%|███████▎  | 35756/49018 [00:16<00:05, 2217.02it/s]Loading:  73%|███████▎  | 35979/49018 [00:16<00:05, 2218.40it/s]Loading:  74%|███████▍  | 36201/49018 [00:16<00:05, 2213.52it/s]Loading:  74%|███████▍  | 36424/49018 [00:16<00:05, 2215.57it/s]Loading:  75%|███████▍  | 36648/49018 [00:16<00:05, 2220.25it/s]Loading:  75%|███████▌  | 36871/49018 [00:16<00:05, 2219.25it/s]Loading:  76%|███████▌  | 37094/49018 [00:16<00:05, 2221.86it/s]Loading:  76%|███████▌  | 37317/49018 [00:17<00:05, 2220.04it/s]Loading:  77%|███████▋  | 37540/49018 [00:17<00:05, 2221.46it/s]Loading:  77%|███████▋  | 37763/49018 [00:17<00:05, 2223.56it/s]Loading:  77%|███████▋  | 37986/49018 [00:17<00:04, 2221.16it/s]Loading:  78%|███████▊  | 38209/49018 [00:17<00:04, 2220.71it/s]Loading:  78%|███████▊  | 38432/49018 [00:17<00:04, 2216.64it/s]Loading:  79%|███████▉  | 38655/49018 [00:17<00:04, 2218.54it/s]Loading:  79%|███████▉  | 38877/49018 [00:17<00:04, 2217.73it/s]Loading:  80%|███████▉  | 39100/49018 [00:17<00:04, 2218.44it/s]Loading:  80%|████████  | 39323/49018 [00:17<00:04, 2219.06it/s]Loading:  81%|████████  | 39545/49018 [00:18<00:04, 2217.68it/s]Loading:  81%|████████  | 39768/49018 [00:18<00:04, 2218.49it/s]Loading:  82%|████████▏ | 39991/49018 [00:18<00:04, 2221.45it/s]Loading:  82%|████████▏ | 40214/49018 [00:18<00:03, 2217.81it/s]Loading:  82%|████████▏ | 40437/49018 [00:18<00:03, 2218.70it/s]Loading:  83%|████████▎ | 40659/49018 [00:18<00:03, 2215.91it/s]Loading:  83%|████████▎ | 40883/49018 [00:18<00:03, 2221.07it/s]Loading:  84%|████████▍ | 41106/49018 [00:18<00:03, 2221.07it/s]Loading:  84%|████████▍ | 41329/49018 [00:18<00:03, 2220.81it/s]Loading:  85%|████████▍ | 41552/49018 [00:18<00:03, 2222.90it/s]Loading:  85%|████████▌ | 41775/49018 [00:19<00:03, 2221.17it/s]Loading:  86%|████████▌ | 41999/49018 [00:19<00:03, 2224.37it/s]Loading:  86%|████████▌ | 42223/49018 [00:19<00:03, 2226.22it/s]Loading:  87%|████████▋ | 42446/49018 [00:19<00:02, 2225.22it/s]Loading:  87%|████████▋ | 42669/49018 [00:19<00:02, 2226.34it/s]Loading:  88%|████████▊ | 42892/49018 [00:19<00:02, 2222.93it/s]Loading:  88%|████████▊ | 43115/49018 [00:19<00:02, 2221.78it/s]Loading:  88%|████████▊ | 43338/49018 [00:19<00:02, 2222.68it/s]Loading:  89%|████████▉ | 43561/49018 [00:19<00:02, 2219.51it/s]Loading:  89%|████████▉ | 43784/49018 [00:19<00:02, 2219.99it/s]Loading:  90%|████████▉ | 44007/49018 [00:20<00:02, 2220.52it/s]Loading:  90%|█████████ | 44230/49018 [00:20<00:02, 2220.74it/s]Loading:  91%|█████████ | 44453/49018 [00:20<00:02, 2223.49it/s]Loading:  91%|█████████ | 44676/49018 [00:20<00:01, 2220.82it/s]Loading:  92%|█████████▏| 44899/49018 [00:20<00:01, 2223.29it/s]Loading:  92%|█████████▏| 45122/49018 [00:20<00:01, 2222.96it/s]Loading:  93%|█████████▎| 45345/49018 [00:20<00:01, 2224.16it/s]Loading:  93%|█████████▎| 45568/49018 [00:20<00:01, 2224.95it/s]Loading:  93%|█████████▎| 45791/49018 [00:21<00:03, 938.06it/s] Loading:  94%|█████████▍| 46014/49018 [00:21<00:02, 1135.06it/s]Loading:  94%|█████████▍| 46237/49018 [00:21<00:02, 1330.33it/s]Loading:  95%|█████████▍| 46460/49018 [00:21<00:01, 1513.44it/s]Loading:  95%|█████████▌| 46683/49018 [00:21<00:01, 1673.77it/s]Loading:  96%|█████████▌| 46906/49018 [00:21<00:01, 1808.54it/s]Loading:  96%|█████████▌| 47130/49018 [00:21<00:00, 1918.07it/s]Loading:  97%|█████████▋| 47352/49018 [00:22<00:00, 1997.36it/s]Loading:  97%|█████████▋| 47575/49018 [00:22<00:00, 2060.45it/s]Loading:  98%|█████████▊| 47798/49018 [00:22<00:00, 2106.91it/s]Loading:  98%|█████████▊| 48021/49018 [00:22<00:00, 2139.66it/s]Loading:  98%|█████████▊| 48245/49018 [00:22<00:00, 2167.86it/s]Loading:  99%|█████████▉| 48467/49018 [00:22<00:00, 2182.89it/s]Loading:  99%|█████████▉| 48690/49018 [00:22<00:00, 2195.92it/s]Loading: 100%|█████████▉| 48913/49018 [00:22<00:00, 2203.78it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2151.01it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 228/49018 [00:00<00:21, 2275.91it/s]Loading:   1%|          | 458/49018 [00:00<00:21, 2287.95it/s]Loading:   1%|▏         | 690/49018 [00:00<00:21, 2298.14it/s]Loading:   2%|▏         | 920/49018 [00:00<00:20, 2296.84it/s]Loading:   2%|▏         | 1151/49018 [00:00<00:20, 2300.64it/s]Loading:   3%|▎         | 1382/49018 [00:00<00:20, 2297.73it/s]Loading:   3%|▎         | 1614/49018 [00:00<00:20, 2303.11it/s]Loading:   4%|▍         | 1845/49018 [00:01<00:51, 908.89it/s] Loading:   4%|▍         | 2073/49018 [00:01<00:42, 1116.67it/s]Loading:   5%|▍         | 2304/49018 [00:01<00:35, 1328.65it/s]Loading:   5%|▌         | 2535/49018 [00:01<00:30, 1526.49it/s]Loading:   6%|▌         | 2765/49018 [00:01<00:27, 1699.98it/s]Loading:   6%|▌         | 2995/49018 [00:01<00:24, 1844.24it/s]Loading:   7%|▋         | 3228/49018 [00:01<00:23, 1968.13it/s]Loading:   7%|▋         | 3461/49018 [00:01<00:22, 2064.27it/s]Loading:   8%|▊         | 3691/49018 [00:02<00:21, 2128.67it/s]Loading:   8%|▊         | 3923/49018 [00:02<00:20, 2181.27it/s]Loading:   8%|▊         | 4153/49018 [00:02<00:20, 2215.45it/s]Loading:   9%|▉         | 4385/49018 [00:02<00:19, 2243.12it/s]Loading:   9%|▉         | 4615/49018 [00:02<00:19, 2259.44it/s]Loading:  10%|▉         | 4846/49018 [00:02<00:19, 2271.66it/s]Loading:  10%|█         | 5077/49018 [00:02<00:19, 2281.92it/s]Loading:  11%|█         | 5308/49018 [00:02<00:19, 2284.32it/s]Loading:  11%|█▏        | 5538/49018 [00:02<00:19, 2286.19it/s]Loading:  12%|█▏        | 5768/49018 [00:02<00:18, 2282.93it/s]Loading:  12%|█▏        | 5998/49018 [00:03<00:18, 2287.17it/s]Loading:  13%|█▎        | 6228/49018 [00:03<00:18, 2287.56it/s]Loading:  13%|█▎        | 6458/49018 [00:03<00:18, 2281.32it/s]Loading:  14%|█▎        | 6687/49018 [00:03<00:18, 2283.89it/s]Loading:  14%|█▍        | 6916/49018 [00:03<00:18, 2281.13it/s]Loading:  15%|█▍        | 7145/49018 [00:03<00:18, 2282.61it/s]Loading:  15%|█▌        | 7374/49018 [00:03<00:18, 2276.36it/s]Loading:  16%|█▌        | 7603/49018 [00:03<00:18, 2278.15it/s]Loading:  16%|█▌        | 7832/49018 [00:03<00:18, 2281.34it/s]Loading:  16%|█▋        | 8061/49018 [00:03<00:17, 2278.01it/s]Loading:  17%|█▋        | 8289/49018 [00:04<00:17, 2278.19it/s]Loading:  17%|█▋        | 8517/49018 [00:04<00:17, 2277.71it/s]Loading:  18%|█▊        | 8745/49018 [00:04<00:17, 2277.58it/s]Loading:  18%|█▊        | 8975/49018 [00:04<00:17, 2281.51it/s]Loading:  19%|█▉        | 9204/49018 [00:04<00:17, 2282.25it/s]Loading:  19%|█▉        | 9434/49018 [00:04<00:17, 2287.01it/s]Loading:  20%|█▉        | 9663/49018 [00:04<00:17, 2287.60it/s]Loading:  20%|██        | 9892/49018 [00:04<00:17, 2287.36it/s]Loading:  21%|██        | 10121/49018 [00:04<00:17, 2286.01it/s]Loading:  21%|██        | 10351/49018 [00:04<00:16, 2287.89it/s]Loading:  22%|██▏       | 10581/49018 [00:05<00:16, 2289.81it/s]Loading:  22%|██▏       | 10810/49018 [00:05<00:16, 2284.21it/s]Loading:  23%|██▎       | 11039/49018 [00:05<00:16, 2285.19it/s]Loading:  23%|██▎       | 11268/49018 [00:05<00:16, 2284.42it/s]Loading:  23%|██▎       | 11497/49018 [00:05<00:16, 2282.69it/s]Loading:  24%|██▍       | 11727/49018 [00:05<00:16, 2284.71it/s]Loading:  24%|██▍       | 11956/49018 [00:05<00:16, 2282.71it/s]Loading:  25%|██▍       | 12186/49018 [00:05<00:16, 2285.28it/s]Loading:  25%|██▌       | 12415/49018 [00:05<00:16, 2281.40it/s]Loading:  26%|██▌       | 12645/49018 [00:06<00:15, 2283.98it/s]Loading:  26%|██▋       | 12875/49018 [00:06<00:15, 2285.88it/s]Loading:  27%|██▋       | 13104/49018 [00:06<00:15, 2283.60it/s]Loading:  27%|██▋       | 13334/49018 [00:06<00:15, 2286.21it/s]Loading:  28%|██▊       | 13563/49018 [00:06<00:15, 2283.81it/s]Loading:  28%|██▊       | 13792/49018 [00:06<00:15, 2285.49it/s]Loading:  29%|██▊       | 14021/49018 [00:06<00:15, 2285.26it/s]Loading:  29%|██▉       | 14251/49018 [00:06<00:15, 2288.37it/s]Loading:  30%|██▉       | 14480/49018 [00:06<00:15, 2287.16it/s]Loading:  30%|███       | 14709/49018 [00:06<00:15, 2283.09it/s]Loading:  30%|███       | 14938/49018 [00:07<00:14, 2282.02it/s]Loading:  31%|███       | 15167/49018 [00:07<00:14, 2280.58it/s]Loading:  31%|███▏      | 15396/49018 [00:07<00:14, 2281.12it/s]Loading:  32%|███▏      | 15626/49018 [00:07<00:14, 2283.87it/s]Loading:  32%|███▏      | 15855/49018 [00:07<00:14, 2284.48it/s]Loading:  33%|███▎      | 16086/49018 [00:07<00:14, 2290.47it/s]Loading:  33%|███▎      | 16316/49018 [00:07<00:14, 2285.88it/s]Loading:  34%|███▍      | 16545/49018 [00:07<00:14, 2285.21it/s]Loading:  34%|███▍      | 16774/49018 [00:07<00:14, 2278.22it/s]Loading:  35%|███▍      | 17002/49018 [00:07<00:14, 2278.43it/s]Loading:  35%|███▌      | 17230/49018 [00:08<00:13, 2278.62it/s]Loading:  36%|███▌      | 17458/49018 [00:08<00:13, 2272.05it/s]Loading:  36%|███▌      | 17686/49018 [00:08<00:13, 2270.93it/s]Loading:  37%|███▋      | 17914/49018 [00:08<00:13, 2263.55it/s]Loading:  37%|███▋      | 18141/49018 [00:08<00:13, 2248.05it/s]Loading:  37%|███▋      | 18366/49018 [00:08<00:13, 2237.34it/s]Loading:  38%|███▊      | 18590/49018 [00:08<00:13, 2223.00it/s]Loading:  38%|███▊      | 18813/49018 [00:08<00:13, 2220.37it/s]Loading:  39%|███▉      | 19036/49018 [00:08<00:13, 2216.12it/s]Loading:  39%|███▉      | 19258/49018 [00:08<00:13, 2214.88it/s]Loading:  40%|███▉      | 19480/49018 [00:09<00:13, 2214.81it/s]Loading:  40%|████      | 19702/49018 [00:09<00:13, 2211.62it/s]Loading:  41%|████      | 19924/49018 [00:09<00:13, 2211.82it/s]Loading:  41%|████      | 20146/49018 [00:09<00:13, 2207.75it/s]Loading:  42%|████▏     | 20367/49018 [00:09<00:12, 2205.35it/s]Loading:  42%|████▏     | 20588/49018 [00:09<00:12, 2205.05it/s]Loading:  42%|████▏     | 20809/49018 [00:09<00:12, 2201.77it/s]Loading:  43%|████▎     | 21030/49018 [00:09<00:12, 2201.93it/s]Loading:  43%|████▎     | 21251/49018 [00:09<00:12, 2198.53it/s]Loading:  44%|████▍     | 21471/49018 [00:09<00:12, 2196.57it/s]Loading:  44%|████▍     | 21692/49018 [00:10<00:12, 2200.47it/s]Loading:  45%|████▍     | 21913/49018 [00:10<00:12, 2202.99it/s]Loading:  45%|████▌     | 22134/49018 [00:10<00:12, 2203.88it/s]Loading:  46%|████▌     | 22355/49018 [00:10<00:12, 2201.46it/s]Loading:  46%|████▌     | 22577/49018 [00:10<00:11, 2204.96it/s]Loading:  47%|████▋     | 22800/49018 [00:10<00:11, 2209.58it/s]Loading:  47%|████▋     | 23021/49018 [00:10<00:11, 2206.10it/s]Loading:  47%|████▋     | 23243/49018 [00:10<00:11, 2207.50it/s]Loading:  48%|████▊     | 23464/49018 [00:10<00:11, 2205.84it/s]Loading:  48%|████▊     | 23685/49018 [00:10<00:11, 2206.41it/s]Loading:  49%|████▉     | 23907/49018 [00:11<00:11, 2210.16it/s]Loading:  49%|████▉     | 24129/49018 [00:11<00:11, 2206.87it/s]Loading:  50%|████▉     | 24352/49018 [00:11<00:11, 2211.84it/s]Loading:  50%|█████     | 24574/49018 [00:11<00:11, 2207.95it/s]Loading:  51%|█████     | 24796/49018 [00:11<00:10, 2208.63it/s]Loading:  51%|█████     | 25018/49018 [00:11<00:10, 2211.00it/s]Loading:  51%|█████▏    | 25240/49018 [00:11<00:10, 2206.49it/s]Loading:  52%|█████▏    | 25462/49018 [00:11<00:10, 2208.25it/s]Loading:  52%|█████▏    | 25683/49018 [00:11<00:10, 2204.41it/s]Loading:  53%|█████▎    | 25905/49018 [00:11<00:10, 2208.95it/s]Loading:  53%|█████▎    | 26126/49018 [00:12<00:10, 2205.99it/s]Loading:  54%|█████▍    | 26351/49018 [00:12<00:10, 2216.46it/s]Loading:  54%|█████▍    | 26573/49018 [00:12<00:10, 2216.74it/s]Loading:  55%|█████▍    | 26795/49018 [00:12<00:10, 2209.54it/s]Loading:  55%|█████▌    | 27016/49018 [00:12<00:09, 2209.24it/s]Loading:  56%|█████▌    | 27238/49018 [00:12<00:09, 2210.78it/s]Loading:  56%|█████▌    | 27461/49018 [00:12<00:09, 2215.89it/s]Loading:  56%|█████▋    | 27683/49018 [00:12<00:09, 2215.32it/s]Loading:  57%|█████▋    | 27905/49018 [00:12<00:09, 2212.55it/s]Loading:  57%|█████▋    | 28127/49018 [00:12<00:09, 2214.24it/s]Loading:  58%|█████▊    | 28349/49018 [00:13<00:09, 2211.59it/s]Loading:  58%|█████▊    | 28571/49018 [00:13<00:09, 2212.33it/s]Loading:  59%|█████▊    | 28793/49018 [00:13<00:09, 2211.15it/s]Loading:  59%|█████▉    | 29015/49018 [00:13<00:09, 2206.98it/s]Loading:  60%|█████▉    | 29236/49018 [00:13<00:08, 2205.89it/s]Loading:  60%|██████    | 29457/49018 [00:13<00:08, 2205.37it/s]Loading:  61%|██████    | 29678/49018 [00:13<00:08, 2204.82it/s]Loading:  61%|██████    | 29899/49018 [00:13<00:08, 2205.32it/s]Loading:  61%|██████▏   | 30120/49018 [00:13<00:08, 2204.49it/s]Loading:  62%|██████▏   | 30342/49018 [00:13<00:08, 2206.38it/s]Loading:  62%|██████▏   | 30563/49018 [00:14<00:08, 2201.74it/s]Loading:  63%|██████▎   | 30784/49018 [00:14<00:08, 2201.19it/s]Loading:  63%|██████▎   | 31006/49018 [00:14<00:08, 2204.01it/s]Loading:  64%|██████▎   | 31227/49018 [00:14<00:08, 2202.07it/s]Loading:  64%|██████▍   | 31448/49018 [00:14<00:07, 2201.97it/s]Loading:  65%|██████▍   | 31669/49018 [00:14<00:07, 2203.64it/s]Loading:  65%|██████▌   | 31892/49018 [00:14<00:07, 2210.89it/s]Loading:  66%|██████▌   | 32114/49018 [00:14<00:07, 2211.34it/s]Loading:  66%|██████▌   | 32336/49018 [00:14<00:07, 2209.16it/s]Loading:  66%|██████▋   | 32559/49018 [00:14<00:07, 2213.92it/s]Loading:  67%|██████▋   | 32781/49018 [00:15<00:07, 2209.24it/s]Loading:  67%|██████▋   | 33002/49018 [00:15<00:07, 2209.16it/s]Loading:  68%|██████▊   | 33223/49018 [00:15<00:07, 2209.19it/s]Loading:  68%|██████▊   | 33444/49018 [00:15<00:07, 2208.00it/s]Loading:  69%|██████▊   | 33667/49018 [00:15<00:06, 2212.89it/s]Loading:  69%|██████▉   | 33889/49018 [00:15<00:06, 2206.69it/s]Loading:  70%|██████▉   | 34110/49018 [00:15<00:06, 2207.58it/s]Loading:  70%|███████   | 34331/49018 [00:15<00:06, 2206.61it/s]Loading:  70%|███████   | 34552/49018 [00:15<00:06, 2204.45it/s]Loading:  71%|███████   | 34774/49018 [00:15<00:06, 2207.96it/s]Loading:  71%|███████▏  | 34995/49018 [00:16<00:06, 2204.05it/s]Loading:  72%|███████▏  | 35217/49018 [00:16<00:06, 2206.79it/s]Loading:  72%|███████▏  | 35438/49018 [00:16<00:06, 2207.57it/s]Loading:  73%|███████▎  | 35659/49018 [00:16<00:06, 2205.06it/s]Loading:  73%|███████▎  | 35882/49018 [00:16<00:05, 2210.54it/s]Loading:  74%|███████▎  | 36104/49018 [00:16<00:05, 2209.14it/s]Loading:  74%|███████▍  | 36325/49018 [00:16<00:05, 2208.69it/s]Loading:  75%|███████▍  | 36546/49018 [00:16<00:05, 2203.81it/s]Loading:  75%|███████▌  | 36767/49018 [00:16<00:05, 2197.83it/s]Loading:  75%|███████▌  | 36988/49018 [00:16<00:05, 2198.65it/s]Loading:  76%|███████▌  | 37208/49018 [00:17<00:05, 2197.57it/s]Loading:  76%|███████▋  | 37430/49018 [00:17<00:05, 2202.68it/s]Loading:  77%|███████▋  | 37651/49018 [00:17<00:05, 2203.18it/s]Loading:  77%|███████▋  | 37872/49018 [00:17<00:14, 795.55it/s] Loading:  78%|███████▊  | 38093/49018 [00:18<00:11, 983.97it/s]Loading:  78%|███████▊  | 38312/49018 [00:18<00:09, 1177.11it/s]Loading:  79%|███████▊  | 38532/49018 [00:18<00:07, 1366.96it/s]Loading:  79%|███████▉  | 38752/49018 [00:18<00:06, 1541.67it/s]Loading:  80%|███████▉  | 38971/49018 [00:18<00:05, 1691.11it/s]Loading:  80%|███████▉  | 39193/49018 [00:18<00:05, 1822.19it/s]Loading:  80%|████████  | 39413/49018 [00:18<00:05, 1917.88it/s]Loading:  81%|████████  | 39634/49018 [00:18<00:04, 1997.21it/s]Loading:  81%|████████▏ | 39855/49018 [00:18<00:04, 2055.96it/s]Loading:  82%|████████▏ | 40076/49018 [00:18<00:04, 2097.60it/s]Loading:  82%|████████▏ | 40298/49018 [00:19<00:04, 2131.62it/s]Loading:  83%|████████▎ | 40518/49018 [00:19<00:03, 2146.99it/s]Loading:  83%|████████▎ | 40739/49018 [00:19<00:03, 2164.45it/s]Loading:  84%|████████▎ | 40960/49018 [00:19<00:03, 2176.57it/s]Loading:  84%|████████▍ | 41180/49018 [00:19<00:03, 2181.82it/s]Loading:  84%|████████▍ | 41402/49018 [00:19<00:03, 2190.93it/s]Loading:  85%|████████▍ | 41623/49018 [00:19<00:03, 2189.64it/s]Loading:  85%|████████▌ | 41844/49018 [00:19<00:03, 2195.07it/s]Loading:  86%|████████▌ | 42067/49018 [00:19<00:03, 2203.04it/s]Loading:  86%|████████▋ | 42288/49018 [00:19<00:03, 2204.73it/s]Loading:  87%|████████▋ | 42510/49018 [00:20<00:02, 2206.38it/s]Loading:  87%|████████▋ | 42731/49018 [00:20<00:02, 2200.85it/s]Loading:  88%|████████▊ | 42952/49018 [00:20<00:02, 2198.67it/s]Loading:  88%|████████▊ | 43173/49018 [00:20<00:02, 2201.29it/s]Loading:  89%|████████▊ | 43394/49018 [00:20<00:02, 2197.57it/s]Loading:  89%|████████▉ | 43614/49018 [00:20<00:02, 2196.34it/s]Loading:  89%|████████▉ | 43834/49018 [00:20<00:02, 2191.25it/s]Loading:  90%|████████▉ | 44056/49018 [00:20<00:02, 2196.88it/s]Loading:  90%|█████████ | 44276/49018 [00:20<00:02, 2196.99it/s]Loading:  91%|█████████ | 44496/49018 [00:20<00:02, 2192.43it/s]Loading:  91%|█████████ | 44716/49018 [00:21<00:01, 2190.17it/s]Loading:  92%|█████████▏| 44936/49018 [00:21<00:01, 2186.20it/s]Loading:  92%|█████████▏| 45156/49018 [00:21<00:01, 2187.64it/s]Loading:  93%|█████████▎| 45376/49018 [00:21<00:01, 2189.64it/s]Loading:  93%|█████████▎| 45595/49018 [00:21<00:01, 2183.51it/s]Loading:  93%|█████████▎| 45814/49018 [00:21<00:01, 2184.16it/s]Loading:  94%|█████████▍| 46034/49018 [00:21<00:01, 2186.15it/s]Loading:  94%|█████████▍| 46254/49018 [00:21<00:01, 2188.05it/s]Loading:  95%|█████████▍| 46474/49018 [00:21<00:01, 2190.10it/s]Loading:  95%|█████████▌| 46694/49018 [00:21<00:01, 2186.29it/s]Loading:  96%|█████████▌| 46915/49018 [00:22<00:00, 2191.33it/s]Loading:  96%|█████████▌| 47136/49018 [00:22<00:00, 2195.23it/s]Loading:  97%|█████████▋| 47356/49018 [00:22<00:00, 2192.74it/s]Loading:  97%|█████████▋| 47578/49018 [00:22<00:00, 2198.16it/s]Loading:  98%|█████████▊| 47798/49018 [00:22<00:00, 2194.27it/s]Loading:  98%|█████████▊| 48018/49018 [00:22<00:00, 2192.03it/s]Loading:  98%|█████████▊| 48238/49018 [00:22<00:00, 2192.95it/s]Loading:  99%|█████████▉| 48458/49018 [00:22<00:00, 2186.86it/s]Loading:  99%|█████████▉| 48678/49018 [00:22<00:00, 2189.37it/s]Loading: 100%|█████████▉| 48897/49018 [00:22<00:00, 2185.78it/s]Loading: 100%|██████████| 49018/49018 [00:23<00:00, 2128.64it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank16]:[W424 18:11:36.473054904 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 18:11:36.514774079 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 18:11:36.517153100 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 18:11:36.519334828 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 18:11:36.536164654 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 18:11:36.541177109 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 18:11:36.715796956 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 18:11:36.719055554 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 18:11:36.719612282 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 18:11:36.720261775 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 18:11:36.721730996 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 18:11:36.721840544 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 18:11:36.722249971 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 18:11:37.638240536 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 18:11:37.660963336 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 18:11:37.665771231 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 18:11:37.666670787 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 18:11:37.666865135 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 18:11:37.668649839 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 18:11:37.438761902 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 18:11:37.447635643 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 18:11:37.449392900 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 18:11:37.450081855 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 18:11:37.450451024 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 18:11:37.451214229 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 18:11:37.451726889 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 18:11:37.452065711 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 18:11:37.464985355 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 18:11:37.470608711 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 18:11:37.473588213 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 18:11:37.474084788 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 18:11:38.560305400 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 18:11:38.449019310 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 18:11:38.820508905 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 18:11:38.821469433 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 18:11:38.821647202 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 18:11:38.450514165 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 18:11:38.776369817 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 18:11:38.037346106 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 18:11:38.970562079 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:25<20:37, 25.25s/it]Train:   4%|▍         | 2/50 [00:25<08:25, 10.53s/it]Train:   6%|▌         | 3/50 [00:25<04:33,  5.82s/it]Train:   8%|▊         | 4/50 [00:25<02:45,  3.61s/it]Train:  10%|█         | 5/50 [00:26<01:47,  2.38s/it]Train:  12%|█▏        | 6/50 [00:26<01:12,  1.64s/it]Train:  14%|█▍        | 7/50 [00:26<00:50,  1.18s/it]Train:  16%|█▌        | 8/50 [00:26<00:36,  1.15it/s]Train:  18%|█▊        | 9/50 [00:26<00:27,  1.51it/s]Train:  20%|██        | 10/50 [00:27<00:20,  1.91it/s]Train:  22%|██▏       | 11/50 [00:27<00:16,  2.32it/s]Train:  24%|██▍       | 12/50 [00:27<00:13,  2.74it/s]Train:  26%|██▌       | 13/50 [00:27<00:11,  3.14it/s]Train:  28%|██▊       | 14/50 [00:28<00:10,  3.49it/s]Train:  30%|███       | 15/50 [00:28<00:09,  3.79it/s]Train:  32%|███▏      | 16/50 [00:28<00:08,  4.02it/s]Train:  34%|███▍      | 17/50 [00:28<00:07,  4.18it/s]Train:  36%|███▌      | 18/50 [00:28<00:07,  4.34it/s]Train:  38%|███▊      | 19/50 [00:29<00:06,  4.46it/s]Train:  40%|████      | 20/50 [00:29<00:06,  4.52it/s]Train:  42%|████▏     | 21/50 [00:29<00:06,  4.58it/s]Train:  44%|████▍     | 22/50 [00:29<00:06,  4.63it/s]Train:  46%|████▌     | 23/50 [00:29<00:05,  4.65it/s]Train:  48%|████▊     | 24/50 [00:30<00:05,  4.66it/s]Train:  50%|█████     | 25/50 [00:30<00:05,  4.69it/s]Train:  52%|█████▏    | 26/50 [00:30<00:05,  4.69it/s]Train:  54%|█████▍    | 27/50 [00:30<00:04,  4.70it/s]Train:  56%|█████▌    | 28/50 [00:31<00:04,  4.69it/s]Train:  58%|█████▊    | 29/50 [00:31<00:04,  4.72it/s]Train:  60%|██████    | 30/50 [00:31<00:04,  4.71it/s]Train:  62%|██████▏   | 31/50 [00:31<00:04,  4.72it/s]Train:  64%|██████▍   | 32/50 [00:31<00:03,  4.72it/s]Train:  66%|██████▌   | 33/50 [00:32<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:32<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:32<00:03,  4.75it/s]Train:  72%|███████▏  | 36/50 [00:32<00:02,  4.74it/s]Train:  74%|███████▍  | 37/50 [00:32<00:02,  4.73it/s]Train:  76%|███████▌  | 38/50 [00:33<00:02,  4.71it/s]Train:  78%|███████▊  | 39/50 [00:33<00:02,  4.72it/s]Train:  80%|████████  | 40/50 [00:33<00:02,  4.71it/s]Train:  82%|████████▏ | 41/50 [00:33<00:01,  4.72it/s]Train:  84%|████████▍ | 42/50 [00:33<00:01,  4.72it/s]Train:  86%|████████▌ | 43/50 [00:34<00:01,  4.72it/s]Train:  88%|████████▊ | 44/50 [00:34<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:34<00:01,  4.72it/s]Train:  92%|█████████▏| 46/50 [00:34<00:00,  4.71it/s]Train:  94%|█████████▍| 47/50 [00:35<00:00,  4.72it/s]Train:  96%|█████████▌| 48/50 [00:35<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:35<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:35<00:00,  1.40it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.21it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.25it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.45it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.67it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.68it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.70it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.71it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.72it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.72it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.72it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.72it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.72it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.72it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.70it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.70it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.69it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.70it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.71it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.72it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.72it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.71it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.72it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.71it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.72it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.73it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.74it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.74it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.76it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.74it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.74it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.73it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.74it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.75it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.75it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.76it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.74it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.73it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.73it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.72it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.74it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.73it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.19it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.91it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.23it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.51it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.57it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.63it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.65it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.67it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.70it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.70it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.71it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.71it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.71it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.72it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.71it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.71it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.70it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.69it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.69it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.69it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.69it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.68it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.65it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.67it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.66it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.69it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.70it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.71it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.70it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.70it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.72it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.72it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.71it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.70it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.70it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.71it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.70it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.70it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.70it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.71it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.72it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.74it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.74it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.65it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.22it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.30it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.52it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.58it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.62it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.64it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.67it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.71it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.70it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.70it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.71it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.70it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.70it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.70it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.72it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.71it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.71it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.70it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.69it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.69it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.69it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.68it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.69it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.70it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.69it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.69it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.69it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.70it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.71it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.72it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.71it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.71it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.71it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.71it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.69it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.69it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.70it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.70it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.70it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.70it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.70it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.69it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.69it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.65it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.21it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.96it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.28it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.44it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.60it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.63it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.65it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.67it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.67it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.69it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.70it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.70it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.71it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.71it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.73it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.74it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.75it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.73it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.74it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.73it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.73it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.73it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.53it/s]Train:  54%|█████▍    | 27/50 [00:05<00:05,  4.53it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.58it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.61it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.65it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.67it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.70it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.72it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.75it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.75it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.70it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.72it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.74it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.73it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.74it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.75it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.76it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.65it/s]
0: Process 0 - Local timer:  load_data  :  97.35
0: Process 0 - Local timer:  train_validate_test  :  78.85
0: Process 0 - Local timer:  create_model  :  1.44
0: Minimum timers: 
0: load_data  :  97.35
0: train_validate_test  :  78.79
0: create_model  :  1.32
0: Maximum timers: 
0: load_data  :  98.46
0: train_validate_test  :  78.86
0: create_model  :  1.44
0: Average timers: 
0: load_data  :  97.95
0: train_validate_test  :  78.81
0: create_model  :  1.38
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 18:12:33.134905261 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 18:12:35.804040096 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 18:12:35.804319105 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 18:12:35.804376083 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 18:12:35.804400810 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 18:12:35.805117709 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 18:12:35.362657881 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 18:12:35.362622814 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 18:12:35.362647742 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 18:12:35.362625409 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 18:12:35.362641159 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 18:12:35.694550290 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 18:12:35.362658472 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 18:12:35.694528809 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 18:12:35.805974584 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 18:12:35.362685163 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 18:12:35.694517398 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 18:12:35.362690312 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 18:12:35.694567042 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 18:12:35.694519712 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 18:12:35.694589164 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 18:12:35.694664447 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 18:12:35.407057456 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 18:12:35.806259043 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 18:12:35.407076993 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 18:12:35.407051534 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 18:12:35.407072895 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 18:12:35.407149280 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 18:12:35.407128089 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 18:12:35.695581346 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 18:12:35.407948824 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 18:12:35.067133254 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 18:12:35.067140067 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 18:12:35.067135589 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 18:12:35.067113597 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 18:12:35.067124027 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 18:12:35.067275585 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 18:12:35.067127714 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 18:12:35.067352171 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 18:12:35.408742296 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 06:12:37 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 06:12:37 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_3 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 18:12:49.216560960 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.216713099 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.216891637 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.216975957 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.216997318 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.216996216 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.217004181 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.218206080 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.897514035 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.897785701 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.897798966 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.897867426 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.897877325 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.899391961 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.899429212 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.900684545 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 18:12:49.381008501 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.381172542 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.381170358 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.381216435 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.381358555 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982191594 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982493256 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982613644 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982632509 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982657647 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.982654741 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.382437711 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.382649272 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.983854613 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.984213152 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:49.407419138 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.888026999 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.888449503 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.888472547 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.888461385 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.888482145 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.889457790 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.889864644 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:12:50.890259685 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_3 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Read attr time (sec):  0.0016434192657470703
0: read and bcast: trainset/x/variable_count 0.18321490287780762
0: read and bcast: trainset/x/variable_offset 0.36980390548706055
0: read and bcast: trainset/x/variable_dim 0.3702061176300049
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5887084007263184
0: read and bcast: trainset/edge_index/variable_offset 0.7758073806762695
0: read and bcast: trainset/edge_index/variable_dim 0.7871353626251221
0: read and bcast: trainset/edge_attr/variable_count 0.9705727100372314
0: read and bcast: trainset/edge_attr/variable_offset 1.156313180923462
0: read and bcast: trainset/edge_attr/variable_dim 1.1661889553070068
0: read and bcast: trainset/pos/variable_count 1.3518390655517578
0: read and bcast: trainset/pos/variable_offset 1.535872459411621
0: read and bcast: trainset/pos/variable_dim 1.546309232711792
0: read and bcast: trainset/energy/variable_count 1.729727029800415
0: read and bcast: trainset/energy/variable_offset 1.916429042816162
0: read and bcast: trainset/energy/variable_dim 1.9264805316925049
0: read and bcast: trainset/forces/variable_count 2.110677719116211
0: read and bcast: trainset/forces/variable_offset 2.3013577461242676
0: read and bcast: trainset/forces/variable_dim 2.3122105598449707
0: read and bcast: trainset/y/variable_count 2.4959523677825928
0: read and bcast: trainset/y/variable_offset 2.679927349090576
0: read and bcast: trainset/y/variable_dim 2.6907801628112793
0: Overall time (sec):  2.6926581859588623
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.6971096992492676
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0008175373077392578
0: read and bcast: valset/x/variable_count 0.008191108703613281
0: read and bcast: valset/x/variable_offset 0.015408039093017578
0: read and bcast: valset/x/variable_dim 0.015573740005493164
0: read and bcast: valset/edge_index/variable_count 0.022881269454956055
0: read and bcast: valset/edge_index/variable_offset 0.030402183532714844
0: read and bcast: valset/edge_index/variable_dim 0.0305783748626709
0: read and bcast: valset/edge_attr/variable_count 0.03762221336364746
0: read and bcast: valset/edge_attr/variable_offset 0.04485964775085449
0: read and bcast: valset/edge_attr/variable_dim 0.04500889778137207
0: read and bcast: valset/pos/variable_count 0.05247235298156738
0: read and bcast: valset/pos/variable_offset 0.05987715721130371
0: read and bcast: valset/pos/variable_dim 0.06005525588989258
0: read and bcast: valset/energy/variable_count 0.06709551811218262
0: read and bcast: valset/energy/variable_offset 0.07447052001953125
0: read and bcast: valset/energy/variable_dim 0.07465672492980957
0: read and bcast: valset/forces/variable_count 0.08145737648010254
0: read and bcast: valset/forces/variable_offset 0.0883948802947998
0: read and bcast: valset/forces/variable_dim 0.08855652809143066
0: read and bcast: valset/y/variable_count 0.09590458869934082
0: read and bcast: valset/y/variable_offset 0.10342240333557129
0: read and bcast: valset/y/variable_dim 0.10360574722290039
0: Overall time (sec):  0.10461783409118652
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10758161544799805
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007889270782470703
0: read and bcast: testset/x/variable_count 0.007641315460205078
0: read and bcast: testset/x/variable_offset 0.014684438705444336
0: read and bcast: testset/x/variable_dim 0.014843225479125977
0: read and bcast: testset/edge_index/variable_count 0.022203445434570312
0: read and bcast: testset/edge_index/variable_offset 0.029560089111328125
0: read and bcast: testset/edge_index/variable_dim 0.029764175415039062
0: read and bcast: testset/edge_attr/variable_count 0.036856651306152344
0: read and bcast: testset/edge_attr/variable_offset 0.044329166412353516
0: read and bcast: testset/edge_attr/variable_dim 0.04452228546142578
0: read and bcast: testset/pos/variable_count 0.051532745361328125
0: read and bcast: testset/pos/variable_offset 0.05894970893859863
0: read and bcast: testset/pos/variable_dim 0.0591280460357666
0: read and bcast: testset/energy/variable_count 0.06647205352783203
0: read and bcast: testset/energy/variable_offset 0.0732879638671875
0: read and bcast: testset/energy/variable_dim 0.07346081733703613
0: read and bcast: testset/forces/variable_count 0.08086371421813965
0: read and bcast: testset/forces/variable_offset 0.08811569213867188
0: read and bcast: testset/forces/variable_dim 0.08829998970031738
0: read and bcast: testset/y/variable_count 0.09508109092712402
0: read and bcast: testset/y/variable_offset 0.10261273384094238
0: read and bcast: testset/y/variable_dim 0.10279250144958496
0: Overall time (sec):  0.10377359390258789
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.1068117618560791
2 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
1 ani1x nsplit: 4460384 6 743398
3 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
9 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
31 transition1x nsplit: 8680250 11 789114
30 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
33 transition1x nsplit: 8680250 11 789114
39 transition1x nsplit: 8680250 11 789113
37 transition1x nsplit: 8680250 11 789113
34 transition1x nsplit: 8680250 11 789114
38 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
19 alexandria nsplit: 9705384 11 882308
21 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
20 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
26 alexandria nsplit: 9705384 11 882307
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
0: Adios reading time (sec):  0.4764561653137207
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
0: Adios reading time (sec):  0.14228463172912598
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.14046120643615723
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 229/64000 [00:00<00:27, 2286.09it/s]Loading:   1%|          | 465/64000 [00:00<00:27, 2325.24it/s]Loading:   1%|          | 698/64000 [00:00<00:27, 2323.82it/s]Loading:   1%|▏         | 932/64000 [00:00<00:27, 2328.46it/s]Loading:   2%|▏         | 1165/64000 [00:00<00:26, 2327.81it/s]Loading:   2%|▏         | 1398/64000 [00:00<00:27, 2304.76it/s]Loading:   3%|▎         | 1629/64000 [00:00<00:27, 2282.31it/s]Loading:   3%|▎         | 1858/64000 [00:00<00:27, 2276.03it/s]Loading:   3%|▎         | 2086/64000 [00:00<00:27, 2272.07it/s]Loading:   4%|▎         | 2314/64000 [00:01<00:27, 2263.33it/s]Loading:   4%|▍         | 2541/64000 [00:01<00:27, 2264.31it/s]Loading:   4%|▍         | 2768/64000 [00:01<00:27, 2257.92it/s]Loading:   5%|▍         | 2994/64000 [00:01<00:27, 2256.27it/s]Loading:   5%|▌         | 3220/64000 [00:01<00:26, 2257.08it/s]Loading:   5%|▌         | 3446/64000 [00:01<00:26, 2257.12it/s]Loading:   6%|▌         | 3672/64000 [00:01<00:26, 2255.17it/s]Loading:   6%|▌         | 3898/64000 [00:01<00:40, 1495.33it/s]Loading:   6%|▋         | 4122/64000 [00:01<00:36, 1658.34it/s]Loading:   7%|▋         | 4346/64000 [00:02<00:33, 1797.80it/s]Loading:   7%|▋         | 4570/64000 [00:02<00:31, 1910.37it/s]Loading:   7%|▋         | 4796/64000 [00:02<00:29, 2003.09it/s]Loading:   8%|▊         | 5020/64000 [00:02<00:28, 2066.46it/s]Loading:   8%|▊         | 5245/64000 [00:02<00:27, 2117.73it/s]Loading:   9%|▊         | 5471/64000 [00:02<00:27, 2158.37it/s]Loading:   9%|▉         | 5695/64000 [00:02<00:26, 2180.45it/s]Loading:   9%|▉         | 5921/64000 [00:02<00:26, 2201.96it/s]Loading:  10%|▉         | 6147/64000 [00:02<00:26, 2217.07it/s]Loading:  10%|▉         | 6373/64000 [00:02<00:25, 2227.30it/s]Loading:  10%|█         | 6600/64000 [00:03<00:25, 2238.54it/s]Loading:  11%|█         | 6826/64000 [00:03<00:25, 2242.46it/s]Loading:  11%|█         | 7053/64000 [00:03<00:25, 2250.36it/s]Loading:  11%|█▏        | 7279/64000 [00:03<00:25, 2248.59it/s]Loading:  12%|█▏        | 7505/64000 [00:03<00:25, 2250.38it/s]Loading:  12%|█▏        | 7731/64000 [00:03<00:25, 2249.52it/s]Loading:  12%|█▏        | 7957/64000 [00:03<00:24, 2250.92it/s]Loading:  13%|█▎        | 8183/64000 [00:03<00:24, 2252.10it/s]Loading:  13%|█▎        | 8409/64000 [00:03<00:24, 2249.39it/s]Loading:  13%|█▎        | 8636/64000 [00:03<00:24, 2254.63it/s]Loading:  14%|█▍        | 8862/64000 [00:04<00:24, 2252.79it/s]Loading:  14%|█▍        | 9088/64000 [00:04<00:24, 2253.64it/s]Loading:  15%|█▍        | 9315/64000 [00:04<00:24, 2257.46it/s]Loading:  15%|█▍        | 9541/64000 [00:04<00:24, 2252.54it/s]Loading:  15%|█▌        | 9768/64000 [00:04<00:24, 2254.86it/s]Loading:  16%|█▌        | 9994/64000 [00:04<00:23, 2251.70it/s]Loading:  16%|█▌        | 10220/64000 [00:04<00:23, 2251.97it/s]Loading:  16%|█▋        | 10446/64000 [00:04<00:23, 2253.11it/s]Loading:  17%|█▋        | 10672/64000 [00:04<00:23, 2250.55it/s]Loading:  17%|█▋        | 10898/64000 [00:04<00:23, 2252.69it/s]Loading:  17%|█▋        | 11124/64000 [00:05<00:23, 2253.82it/s]Loading:  18%|█▊        | 11353/64000 [00:05<00:23, 2261.95it/s]Loading:  18%|█▊        | 11580/64000 [00:05<00:23, 2262.96it/s]Loading:  18%|█▊        | 11807/64000 [00:05<00:23, 2260.70it/s]Loading:  19%|█▉        | 12034/64000 [00:05<00:23, 2256.24it/s]Loading:  19%|█▉        | 12260/64000 [00:05<00:33, 1528.53it/s]Loading:  20%|█▉        | 12487/64000 [00:05<00:30, 1694.64it/s]Loading:  20%|█▉        | 12715/64000 [00:05<00:27, 1836.40it/s]Loading:  20%|██        | 12942/64000 [00:06<00:26, 1946.50it/s]Loading:  21%|██        | 13170/64000 [00:06<00:24, 2034.75it/s]Loading:  21%|██        | 13396/64000 [00:06<00:24, 2095.90it/s]Loading:  21%|██▏       | 13622/64000 [00:06<00:23, 2142.28it/s]Loading:  22%|██▏       | 13849/64000 [00:06<00:23, 2178.16it/s]Loading:  22%|██▏       | 14078/64000 [00:06<00:22, 2208.34it/s]Loading:  22%|██▏       | 14307/64000 [00:06<00:22, 2231.27it/s]Loading:  23%|██▎       | 14533/64000 [00:06<00:22, 2237.06it/s]Loading:  23%|██▎       | 14760/64000 [00:06<00:21, 2245.64it/s]Loading:  23%|██▎       | 14986/64000 [00:06<00:21, 2248.69it/s]Loading:  24%|██▍       | 15214/64000 [00:07<00:21, 2256.12it/s]Loading:  24%|██▍       | 15443/64000 [00:07<00:21, 2263.35it/s]Loading:  24%|██▍       | 15671/64000 [00:07<00:21, 2266.78it/s]Loading:  25%|██▍       | 15900/64000 [00:07<00:21, 2273.49it/s]Loading:  25%|██▌       | 16128/64000 [00:07<00:21, 2273.44it/s]Loading:  26%|██▌       | 16356/64000 [00:07<00:20, 2272.50it/s]Loading:  26%|██▌       | 16584/64000 [00:07<00:20, 2273.85it/s]Loading:  26%|██▋       | 16812/64000 [00:07<00:20, 2268.87it/s]Loading:  27%|██▋       | 17040/64000 [00:07<00:20, 2270.51it/s]Loading:  27%|██▋       | 17268/64000 [00:07<00:20, 2270.79it/s]Loading:  27%|██▋       | 17496/64000 [00:08<00:20, 2272.46it/s]Loading:  28%|██▊       | 17724/64000 [00:08<00:20, 2271.46it/s]Loading:  28%|██▊       | 17952/64000 [00:08<00:20, 2272.57it/s]Loading:  28%|██▊       | 18180/64000 [00:08<00:20, 2273.01it/s]Loading:  29%|██▉       | 18408/64000 [00:08<00:20, 2271.85it/s]Loading:  29%|██▉       | 18636/64000 [00:08<00:19, 2274.16it/s]Loading:  29%|██▉       | 18864/64000 [00:08<00:19, 2272.70it/s]Loading:  30%|██▉       | 19092/64000 [00:08<00:19, 2274.45it/s]Loading:  30%|███       | 19321/64000 [00:08<00:19, 2276.36it/s]Loading:  31%|███       | 19549/64000 [00:08<00:19, 2272.73it/s]Loading:  31%|███       | 19778/64000 [00:09<00:19, 2275.50it/s]Loading:  31%|███▏      | 20006/64000 [00:09<00:19, 2270.01it/s]Loading:  32%|███▏      | 20234/64000 [00:09<00:19, 2268.16it/s]Loading:  32%|███▏      | 20461/64000 [00:09<00:19, 2264.37it/s]Loading:  32%|███▏      | 20689/64000 [00:09<00:19, 2268.18it/s]Loading:  33%|███▎      | 20917/64000 [00:09<00:18, 2270.12it/s]Loading:  33%|███▎      | 21145/64000 [00:09<00:18, 2267.90it/s]Loading:  33%|███▎      | 21374/64000 [00:09<00:18, 2272.02it/s]Loading:  34%|███▍      | 21602/64000 [00:09<00:18, 2266.33it/s]Loading:  34%|███▍      | 21831/64000 [00:09<00:18, 2271.11it/s]Loading:  34%|███▍      | 22059/64000 [00:10<00:18, 2269.82it/s]Loading:  35%|███▍      | 22286/64000 [00:10<00:28, 1439.41it/s]Loading:  35%|███▌      | 22513/64000 [00:10<00:25, 1615.41it/s]Loading:  36%|███▌      | 22740/64000 [00:10<00:23, 1767.25it/s]Loading:  36%|███▌      | 22967/64000 [00:10<00:21, 1891.26it/s]Loading:  36%|███▌      | 23193/64000 [00:10<00:20, 1988.03it/s]Loading:  37%|███▋      | 23420/64000 [00:10<00:19, 2063.75it/s]Loading:  37%|███▋      | 23647/64000 [00:10<00:19, 2121.18it/s]Loading:  37%|███▋      | 23873/64000 [00:11<00:18, 2160.82it/s]Loading:  38%|███▊      | 24101/64000 [00:11<00:18, 2195.32it/s]Loading:  38%|███▊      | 24330/64000 [00:11<00:17, 2220.97it/s]Loading:  38%|███▊      | 24557/64000 [00:11<00:17, 2235.05it/s]Loading:  39%|███▊      | 24785/64000 [00:11<00:17, 2245.84it/s]Loading:  39%|███▉      | 25012/64000 [00:11<00:17, 2251.90it/s]Loading:  39%|███▉      | 25241/64000 [00:11<00:17, 2260.70it/s]Loading:  40%|███▉      | 25468/64000 [00:11<00:17, 2263.35it/s]Loading:  40%|████      | 25696/64000 [00:11<00:16, 2266.90it/s]Loading:  41%|████      | 25924/64000 [00:11<00:16, 2269.69it/s]Loading:  41%|████      | 26152/64000 [00:12<00:16, 2266.12it/s]Loading:  41%|████      | 26379/64000 [00:12<00:16, 2266.89it/s]Loading:  42%|████▏     | 26606/64000 [00:12<00:16, 2263.41it/s]Loading:  42%|████▏     | 26833/64000 [00:12<00:16, 2261.79it/s]Loading:  42%|████▏     | 27060/64000 [00:12<00:16, 2259.42it/s]Loading:  43%|████▎     | 27286/64000 [00:12<00:16, 2255.29it/s]Loading:  43%|████▎     | 27514/64000 [00:12<00:16, 2261.11it/s]Loading:  43%|████▎     | 27741/64000 [00:12<00:16, 2258.64it/s]Loading:  44%|████▎     | 27968/64000 [00:12<00:15, 2261.66it/s]Loading:  44%|████▍     | 28195/64000 [00:12<00:15, 2261.78it/s]Loading:  44%|████▍     | 28422/64000 [00:13<00:15, 2256.15it/s]Loading:  45%|████▍     | 28648/64000 [00:13<00:15, 2255.71it/s]Loading:  45%|████▌     | 28874/64000 [00:13<00:15, 2252.01it/s]Loading:  45%|████▌     | 29102/64000 [00:13<00:15, 2257.87it/s]Loading:  46%|████▌     | 29329/64000 [00:13<00:15, 2259.09it/s]Loading:  46%|████▌     | 29557/64000 [00:13<00:15, 2262.77it/s]Loading:  47%|████▋     | 29784/64000 [00:13<00:15, 2264.87it/s]Loading:  47%|████▋     | 30011/64000 [00:13<00:15, 2262.38it/s]Loading:  47%|████▋     | 30239/64000 [00:13<00:14, 2266.75it/s]Loading:  48%|████▊     | 30466/64000 [00:13<00:14, 2265.91it/s]Loading:  48%|████▊     | 30694/64000 [00:14<00:14, 2267.68it/s]Loading:  48%|████▊     | 30922/64000 [00:14<00:14, 2269.40it/s]Loading:  49%|████▊     | 31149/64000 [00:14<00:14, 2267.64it/s]Loading:  49%|████▉     | 31377/64000 [00:14<00:14, 2270.28it/s]Loading:  49%|████▉     | 31605/64000 [00:14<00:14, 2268.51it/s]Loading:  50%|████▉     | 31833/64000 [00:14<00:14, 2270.33it/s]Loading:  50%|█████     | 32061/64000 [00:14<00:14, 2265.87it/s]Loading:  50%|█████     | 32288/64000 [00:14<00:14, 2258.91it/s]Loading:  51%|█████     | 32514/64000 [00:14<00:13, 2258.82it/s]Loading:  51%|█████     | 32740/64000 [00:14<00:13, 2257.31it/s]Loading:  52%|█████▏    | 32967/64000 [00:15<00:13, 2258.89it/s]Loading:  52%|█████▏    | 33193/64000 [00:15<00:13, 2255.79it/s]Loading:  52%|█████▏    | 33419/64000 [00:15<00:13, 2254.77it/s]Loading:  53%|█████▎    | 33646/64000 [00:15<00:13, 2257.02it/s]Loading:  53%|█████▎    | 33872/64000 [00:15<00:13, 2254.58it/s]Loading:  53%|█████▎    | 34099/64000 [00:15<00:13, 2257.61it/s]Loading:  54%|█████▎    | 34325/64000 [00:15<00:13, 2258.11it/s]Loading:  54%|█████▍    | 34553/64000 [00:15<00:13, 2262.18it/s]Loading:  54%|█████▍    | 34781/64000 [00:15<00:12, 2266.58it/s]Loading:  55%|█████▍    | 35008/64000 [00:16<00:21, 1330.32it/s]Loading:  55%|█████▌    | 35234/64000 [00:16<00:18, 1516.46it/s]Loading:  55%|█████▌    | 35458/64000 [00:16<00:17, 1677.15it/s]Loading:  56%|█████▌    | 35685/64000 [00:16<00:15, 1818.60it/s]Loading:  56%|█████▌    | 35911/64000 [00:16<00:14, 1931.26it/s]Loading:  56%|█████▋    | 36136/64000 [00:16<00:13, 2016.08it/s]Loading:  57%|█████▋    | 36362/64000 [00:16<00:13, 2081.64it/s]Loading:  57%|█████▋    | 36587/64000 [00:16<00:12, 2127.93it/s]Loading:  58%|█████▊    | 36813/64000 [00:17<00:12, 2165.24it/s]Loading:  58%|█████▊    | 37039/64000 [00:17<00:12, 2191.90it/s]Loading:  58%|█████▊    | 37265/64000 [00:17<00:12, 2210.16it/s]Loading:  59%|█████▊    | 37492/64000 [00:17<00:11, 2225.99it/s]Loading:  59%|█████▉    | 37718/64000 [00:17<00:11, 2234.66it/s]Loading:  59%|█████▉    | 37945/64000 [00:17<00:11, 2245.01it/s]Loading:  60%|█████▉    | 38171/64000 [00:17<00:11, 2248.52it/s]Loading:  60%|█████▉    | 38397/64000 [00:17<00:11, 2249.50it/s]Loading:  60%|██████    | 38623/64000 [00:17<00:11, 2250.74it/s]Loading:  61%|██████    | 38849/64000 [00:17<00:11, 2245.69it/s]Loading:  61%|██████    | 39075/64000 [00:18<00:11, 2249.28it/s]Loading:  61%|██████▏   | 39301/64000 [00:18<00:10, 2249.97it/s]Loading:  62%|██████▏   | 39527/64000 [00:18<00:10, 2249.28it/s]Loading:  62%|██████▏   | 39754/64000 [00:18<00:10, 2253.17it/s]Loading:  62%|██████▏   | 39980/64000 [00:18<00:10, 2252.06it/s]Loading:  63%|██████▎   | 40206/64000 [00:18<00:10, 2253.89it/s]Loading:  63%|██████▎   | 40432/64000 [00:18<00:10, 2253.29it/s]Loading:  64%|██████▎   | 40659/64000 [00:18<00:10, 2256.53it/s]Loading:  64%|██████▍   | 40886/64000 [00:18<00:10, 2258.87it/s]Loading:  64%|██████▍   | 41112/64000 [00:18<00:10, 2257.07it/s]Loading:  65%|██████▍   | 41340/64000 [00:19<00:10, 2260.95it/s]Loading:  65%|██████▍   | 41567/64000 [00:19<00:09, 2257.93it/s]Loading:  65%|██████▌   | 41794/64000 [00:19<00:09, 2260.63it/s]Loading:  66%|██████▌   | 42021/64000 [00:19<00:09, 2261.00it/s]Loading:  66%|██████▌   | 42248/64000 [00:19<00:09, 2258.99it/s]Loading:  66%|██████▋   | 42474/64000 [00:19<00:09, 2258.63it/s]Loading:  67%|██████▋   | 42700/64000 [00:19<00:09, 2252.53it/s]Loading:  67%|██████▋   | 42926/64000 [00:19<00:09, 2248.75it/s]Loading:  67%|██████▋   | 43151/64000 [00:19<00:09, 2247.77it/s]Loading:  68%|██████▊   | 43376/64000 [00:19<00:09, 2242.59it/s]Loading:  68%|██████▊   | 43601/64000 [00:20<00:09, 2243.18it/s]Loading:  68%|██████▊   | 43826/64000 [00:20<00:09, 2241.03it/s]Loading:  69%|██████▉   | 44052/64000 [00:20<00:08, 2245.10it/s]Loading:  69%|██████▉   | 44277/64000 [00:20<00:08, 2245.25it/s]Loading:  70%|██████▉   | 44503/64000 [00:20<00:08, 2247.99it/s]Loading:  70%|██████▉   | 44729/64000 [00:20<00:08, 2250.73it/s]Loading:  70%|███████   | 44955/64000 [00:20<00:08, 2247.45it/s]Loading:  71%|███████   | 45181/64000 [00:20<00:08, 2249.65it/s]Loading:  71%|███████   | 45406/64000 [00:20<00:08, 2246.15it/s]Loading:  71%|███████▏  | 45632/64000 [00:20<00:08, 2249.13it/s]Loading:  72%|███████▏  | 45858/64000 [00:21<00:08, 2251.41it/s]Loading:  72%|███████▏  | 46084/64000 [00:21<00:07, 2249.51it/s]Loading:  72%|███████▏  | 46309/64000 [00:21<00:07, 2248.19it/s]Loading:  73%|███████▎  | 46534/64000 [00:21<00:07, 2248.71it/s]Loading:  73%|███████▎  | 46760/64000 [00:21<00:07, 2251.07it/s]Loading:  73%|███████▎  | 46986/64000 [00:21<00:07, 2251.45it/s]Loading:  74%|███████▍  | 47212/64000 [00:21<00:07, 2246.61it/s]Loading:  74%|███████▍  | 47437/64000 [00:21<00:07, 2247.15it/s]Loading:  74%|███████▍  | 47662/64000 [00:21<00:07, 2242.48it/s]Loading:  75%|███████▍  | 47888/64000 [00:21<00:07, 2245.20it/s]Loading:  75%|███████▌  | 48114/64000 [00:22<00:07, 2247.83it/s]Loading:  76%|███████▌  | 48339/64000 [00:22<00:06, 2247.09it/s]Loading:  76%|███████▌  | 48565/64000 [00:22<00:06, 2248.86it/s]Loading:  76%|███████▌  | 48790/64000 [00:22<00:06, 2246.24it/s]Loading:  77%|███████▋  | 49016/64000 [00:22<00:06, 2248.77it/s]Loading:  77%|███████▋  | 49241/64000 [00:22<00:06, 2248.23it/s]Loading:  77%|███████▋  | 49468/64000 [00:22<00:06, 2253.99it/s]Loading:  78%|███████▊  | 49694/64000 [00:22<00:06, 2254.43it/s]Loading:  78%|███████▊  | 49920/64000 [00:22<00:06, 2248.56it/s]Loading:  78%|███████▊  | 50145/64000 [00:22<00:06, 2248.97it/s]Loading:  79%|███████▊  | 50370/64000 [00:23<00:06, 2245.08it/s]Loading:  79%|███████▉  | 50596/64000 [00:23<00:05, 2248.42it/s]Loading:  79%|███████▉  | 50821/64000 [00:23<00:05, 2247.30it/s]Loading:  80%|███████▉  | 51046/64000 [00:23<00:10, 1209.41it/s]Loading:  80%|████████  | 51270/64000 [00:23<00:09, 1401.75it/s]Loading:  80%|████████  | 51493/64000 [00:23<00:07, 1575.27it/s]Loading:  81%|████████  | 51717/64000 [00:23<00:07, 1727.40it/s]Loading:  81%|████████  | 51941/64000 [00:24<00:06, 1853.91it/s]Loading:  82%|████████▏ | 52164/64000 [00:24<00:06, 1950.66it/s]Loading:  82%|████████▏ | 52388/64000 [00:24<00:05, 2028.81it/s]Loading:  82%|████████▏ | 52611/64000 [00:24<00:05, 2083.24it/s]Loading:  83%|████████▎ | 52836/64000 [00:24<00:05, 2128.03it/s]Loading:  83%|████████▎ | 53061/64000 [00:24<00:05, 2161.96it/s]Loading:  83%|████████▎ | 53284/64000 [00:24<00:04, 2181.40it/s]Loading:  84%|████████▎ | 53509/64000 [00:24<00:04, 2198.83it/s]Loading:  84%|████████▍ | 53733/64000 [00:24<00:04, 2208.30it/s]Loading:  84%|████████▍ | 53958/64000 [00:24<00:04, 2217.92it/s]Loading:  85%|████████▍ | 54183/64000 [00:25<00:04, 2224.76it/s]Loading:  85%|████████▌ | 54407/64000 [00:25<00:04, 2226.30it/s]Loading:  85%|████████▌ | 54632/64000 [00:25<00:04, 2232.14it/s]Loading:  86%|████████▌ | 54856/64000 [00:25<00:04, 2234.23it/s]Loading:  86%|████████▌ | 55082/64000 [00:25<00:03, 2239.67it/s]Loading:  86%|████████▋ | 55308/64000 [00:25<00:03, 2243.35it/s]Loading:  87%|████████▋ | 55533/64000 [00:25<00:03, 2244.02it/s]Loading:  87%|████████▋ | 55761/64000 [00:25<00:03, 2253.60it/s]Loading:  87%|████████▋ | 55987/64000 [00:25<00:03, 2249.10it/s]Loading:  88%|████████▊ | 56212/64000 [00:25<00:03, 2249.28it/s]Loading:  88%|████████▊ | 56438/64000 [00:26<00:03, 2251.57it/s]Loading:  89%|████████▊ | 56664/64000 [00:26<00:03, 2245.82it/s]Loading:  89%|████████▉ | 56890/64000 [00:26<00:03, 2247.62it/s]Loading:  89%|████████▉ | 57115/64000 [00:26<00:03, 2245.52it/s]Loading:  90%|████████▉ | 57341/64000 [00:26<00:02, 2247.38it/s]Loading:  90%|████████▉ | 57566/64000 [00:26<00:02, 2244.51it/s]Loading:  90%|█████████ | 57792/64000 [00:26<00:02, 2247.00it/s]Loading:  91%|█████████ | 58018/64000 [00:26<00:02, 2249.92it/s]Loading:  91%|█████████ | 58243/64000 [00:26<00:02, 2248.23it/s]Loading:  91%|█████████▏| 58469/64000 [00:26<00:02, 2249.58it/s]Loading:  92%|█████████▏| 58694/64000 [00:27<00:02, 2245.26it/s]Loading:  92%|█████████▏| 58919/64000 [00:27<00:02, 2246.04it/s]Loading:  92%|█████████▏| 59145/64000 [00:27<00:02, 2248.82it/s]Loading:  93%|█████████▎| 59370/64000 [00:27<00:02, 2247.18it/s]Loading:  93%|█████████▎| 59596/64000 [00:27<00:01, 2249.69it/s]Loading:  93%|█████████▎| 59821/64000 [00:27<00:01, 2248.56it/s]Loading:  94%|█████████▍| 60047/64000 [00:27<00:01, 2250.07it/s]Loading:  94%|█████████▍| 60273/64000 [00:27<00:01, 2249.70it/s]Loading:  95%|█████████▍| 60498/64000 [00:27<00:01, 2246.54it/s]Loading:  95%|█████████▍| 60723/64000 [00:27<00:01, 2246.22it/s]Loading:  95%|█████████▌| 60948/64000 [00:28<00:01, 2244.01it/s]Loading:  96%|█████████▌| 61174/64000 [00:28<00:01, 2247.95it/s]Loading:  96%|█████████▌| 61400/64000 [00:28<00:01, 2248.93it/s]Loading:  96%|█████████▋| 61625/64000 [00:28<00:01, 2247.06it/s]Loading:  97%|█████████▋| 61852/64000 [00:28<00:00, 2251.14it/s]Loading:  97%|█████████▋| 62078/64000 [00:28<00:00, 2247.31it/s]Loading:  97%|█████████▋| 62305/64000 [00:28<00:00, 2252.13it/s]Loading:  98%|█████████▊| 62531/64000 [00:28<00:00, 2253.72it/s]Loading:  98%|█████████▊| 62757/64000 [00:28<00:00, 2254.25it/s]Loading:  98%|█████████▊| 62983/64000 [00:28<00:00, 2255.95it/s]Loading:  99%|█████████▉| 63209/64000 [00:29<00:00, 2250.38it/s]Loading:  99%|█████████▉| 63435/64000 [00:29<00:00, 2249.82it/s]Loading:  99%|█████████▉| 63660/64000 [00:29<00:00, 2247.03it/s]Loading: 100%|█████████▉| 63886/64000 [00:29<00:00, 2250.17it/s]Loading: 100%|██████████| 64000/64000 [00:29<00:00, 2176.83it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 228/49018 [00:00<00:21, 2271.72it/s]Loading:   1%|          | 461/49018 [00:00<00:21, 2304.78it/s]Loading:   1%|▏         | 694/49018 [00:00<00:20, 2316.22it/s]Loading:   2%|▏         | 927/49018 [00:00<00:20, 2320.54it/s]Loading:   2%|▏         | 1161/49018 [00:00<00:20, 2326.27it/s]Loading:   3%|▎         | 1394/49018 [00:00<00:20, 2323.77it/s]Loading:   3%|▎         | 1628/49018 [00:00<00:20, 2327.76it/s]Loading:   4%|▍         | 1863/49018 [00:00<00:20, 2331.97it/s]Loading:   4%|▍         | 2097/49018 [00:00<00:20, 2328.63it/s]Loading:   5%|▍         | 2331/49018 [00:01<00:20, 2331.56it/s]Loading:   5%|▌         | 2565/49018 [00:01<00:19, 2328.61it/s]Loading:   6%|▌         | 2799/49018 [00:01<00:19, 2329.99it/s]Loading:   6%|▌         | 3033/49018 [00:01<00:19, 2331.95it/s]Loading:   7%|▋         | 3267/49018 [00:01<00:19, 2329.30it/s]Loading:   7%|▋         | 3501/49018 [00:01<00:19, 2330.53it/s]Loading:   8%|▊         | 3735/49018 [00:01<00:19, 2326.66it/s]Loading:   8%|▊         | 3968/49018 [00:01<00:19, 2326.51it/s]Loading:   9%|▊         | 4201/49018 [00:01<00:19, 2323.50it/s]Loading:   9%|▉         | 4434/49018 [00:01<00:19, 2324.68it/s]Loading:  10%|▉         | 4668/49018 [00:02<00:19, 2327.89it/s]Loading:  10%|▉         | 4901/49018 [00:02<00:18, 2326.67it/s]Loading:  10%|█         | 5135/49018 [00:02<00:18, 2328.72it/s]Loading:  11%|█         | 5368/49018 [00:02<00:18, 2328.45it/s]Loading:  11%|█▏        | 5602/49018 [00:02<00:18, 2331.03it/s]Loading:  12%|█▏        | 5836/49018 [00:02<00:18, 2327.77it/s]Loading:  12%|█▏        | 6070/49018 [00:02<00:18, 2328.59it/s]Loading:  13%|█▎        | 6304/49018 [00:02<00:18, 2329.87it/s]Loading:  13%|█▎        | 6537/49018 [00:02<00:18, 2323.94it/s]Loading:  14%|█▍        | 6770/49018 [00:02<00:18, 2325.74it/s]Loading:  14%|█▍        | 7003/49018 [00:03<00:18, 2325.72it/s]Loading:  15%|█▍        | 7236/49018 [00:03<00:17, 2322.28it/s]Loading:  15%|█▌        | 7469/49018 [00:03<00:17, 2318.55it/s]Loading:  16%|█▌        | 7702/49018 [00:03<00:17, 2319.03it/s]Loading:  16%|█▌        | 7935/49018 [00:03<00:17, 2320.31it/s]Loading:  17%|█▋        | 8168/49018 [00:03<00:17, 2318.37it/s]Loading:  17%|█▋        | 8401/49018 [00:03<00:17, 2320.20it/s]Loading:  18%|█▊        | 8634/49018 [00:03<00:17, 2320.25it/s]Loading:  18%|█▊        | 8868/49018 [00:03<00:17, 2324.01it/s]Loading:  19%|█▊        | 9101/49018 [00:03<00:17, 2323.75it/s]Loading:  19%|█▉        | 9334/49018 [00:04<00:17, 2320.50it/s]Loading:  20%|█▉        | 9567/49018 [00:04<00:17, 2320.41it/s]Loading:  20%|█▉        | 9800/49018 [00:04<00:16, 2318.86it/s]Loading:  20%|██        | 10033/49018 [00:04<00:16, 2321.20it/s]Loading:  21%|██        | 10266/49018 [00:04<00:16, 2319.12it/s]Loading:  21%|██▏       | 10499/49018 [00:04<00:16, 2320.22it/s]Loading:  22%|██▏       | 10732/49018 [00:04<00:16, 2322.02it/s]Loading:  22%|██▏       | 10965/49018 [00:04<00:16, 2319.01it/s]Loading:  23%|██▎       | 11198/49018 [00:04<00:16, 2320.54it/s]Loading:  23%|██▎       | 11431/49018 [00:04<00:16, 2316.37it/s]Loading:  24%|██▍       | 11663/49018 [00:05<00:16, 2316.26it/s]Loading:  24%|██▍       | 11895/49018 [00:05<00:16, 2314.16it/s]Loading:  25%|██▍       | 12128/49018 [00:05<00:15, 2316.81it/s]Loading:  25%|██▌       | 12360/49018 [00:05<00:15, 2315.57it/s]Loading:  26%|██▌       | 12592/49018 [00:05<00:15, 2311.35it/s]Loading:  26%|██▌       | 12824/49018 [00:05<00:15, 2311.87it/s]Loading:  27%|██▋       | 13056/49018 [00:05<00:15, 2307.12it/s]Loading:  27%|██▋       | 13287/49018 [00:05<00:15, 2289.49it/s]Loading:  28%|██▊       | 13516/49018 [00:05<00:15, 2279.17it/s]Loading:  28%|██▊       | 13744/49018 [00:05<00:15, 2270.77it/s]Loading:  29%|██▊       | 13972/49018 [00:06<00:15, 2268.40it/s]Loading:  29%|██▉       | 14199/49018 [00:06<00:15, 2259.70it/s]Loading:  29%|██▉       | 14426/49018 [00:06<00:15, 2259.86it/s]Loading:  30%|██▉       | 14652/49018 [00:06<00:15, 2257.50it/s]Loading:  30%|███       | 14879/49018 [00:06<00:15, 2260.28it/s]Loading:  31%|███       | 15106/49018 [00:06<00:14, 2262.07it/s]Loading:  31%|███▏      | 15333/49018 [00:06<00:14, 2259.64it/s]Loading:  32%|███▏      | 15560/49018 [00:06<00:14, 2260.10it/s]Loading:  32%|███▏      | 15787/49018 [00:06<00:14, 2256.36it/s]Loading:  33%|███▎      | 16013/49018 [00:06<00:14, 2256.53it/s]Loading:  33%|███▎      | 16239/49018 [00:07<00:14, 2256.56it/s]Loading:  34%|███▎      | 16465/49018 [00:07<00:14, 2252.44it/s]Loading:  34%|███▍      | 16691/49018 [00:07<00:14, 2253.40it/s]Loading:  35%|███▍      | 16917/49018 [00:07<00:14, 2251.57it/s]Loading:  35%|███▍      | 17144/49018 [00:07<00:14, 2254.67it/s]Loading:  35%|███▌      | 17370/49018 [00:07<00:14, 2255.67it/s]Loading:  36%|███▌      | 17596/49018 [00:07<00:13, 2253.15it/s]Loading:  36%|███▋      | 17822/49018 [00:07<00:13, 2253.76it/s]Loading:  37%|███▋      | 18048/49018 [00:08<00:29, 1060.42it/s]Loading:  37%|███▋      | 18273/49018 [00:08<00:24, 1258.81it/s]Loading:  38%|███▊      | 18498/49018 [00:08<00:21, 1449.23it/s]Loading:  38%|███▊      | 18723/49018 [00:08<00:18, 1620.48it/s]Loading:  39%|███▊      | 18948/49018 [00:08<00:17, 1768.22it/s]Loading:  39%|███▉      | 19173/49018 [00:08<00:15, 1888.58it/s]Loading:  40%|███▉      | 19399/49018 [00:08<00:14, 1986.04it/s]Loading:  40%|████      | 19625/49018 [00:08<00:14, 2060.34it/s]Loading:  40%|████      | 19850/49018 [00:09<00:13, 2111.16it/s]Loading:  41%|████      | 20075/49018 [00:09<00:13, 2148.61it/s]Loading:  41%|████▏     | 20300/49018 [00:09<00:13, 2176.29it/s]Loading:  42%|████▏     | 20525/49018 [00:09<00:12, 2196.81it/s]Loading:  42%|████▏     | 20750/49018 [00:09<00:12, 2211.69it/s]Loading:  43%|████▎     | 20976/49018 [00:09<00:12, 2224.26it/s]Loading:  43%|████▎     | 21202/49018 [00:09<00:12, 2234.13it/s]Loading:  44%|████▎     | 21427/49018 [00:09<00:12, 2236.59it/s]Loading:  44%|████▍     | 21652/49018 [00:09<00:12, 2240.15it/s]Loading:  45%|████▍     | 21877/49018 [00:09<00:12, 2242.65it/s]Loading:  45%|████▌     | 22104/49018 [00:10<00:11, 2249.50it/s]Loading:  46%|████▌     | 22331/49018 [00:10<00:11, 2252.95it/s]Loading:  46%|████▌     | 22557/49018 [00:10<00:11, 2251.33it/s]Loading:  46%|████▋     | 22783/49018 [00:10<00:11, 2247.59it/s]Loading:  47%|████▋     | 23008/49018 [00:10<00:11, 2246.00it/s]Loading:  47%|████▋     | 23234/49018 [00:10<00:11, 2248.13it/s]Loading:  48%|████▊     | 23459/49018 [00:10<00:11, 2247.53it/s]Loading:  48%|████▊     | 23684/49018 [00:10<00:11, 2246.59it/s]Loading:  49%|████▉     | 23910/49018 [00:10<00:11, 2247.99it/s]Loading:  49%|████▉     | 24135/49018 [00:10<00:11, 2247.93it/s]Loading:  50%|████▉     | 24361/49018 [00:11<00:10, 2251.23it/s]Loading:  50%|█████     | 24587/49018 [00:11<00:10, 2250.98it/s]Loading:  51%|█████     | 24813/49018 [00:11<00:10, 2248.48it/s]Loading:  51%|█████     | 25039/49018 [00:11<00:10, 2250.12it/s]Loading:  52%|█████▏    | 25265/49018 [00:11<00:10, 2245.11it/s]Loading:  52%|█████▏    | 25490/49018 [00:11<00:10, 2246.39it/s]Loading:  52%|█████▏    | 25715/49018 [00:11<00:10, 2244.06it/s]Loading:  53%|█████▎    | 25941/49018 [00:11<00:10, 2246.03it/s]Loading:  53%|█████▎    | 26166/49018 [00:11<00:10, 2246.15it/s]Loading:  54%|█████▍    | 26391/49018 [00:11<00:10, 2243.88it/s]Loading:  54%|█████▍    | 26617/49018 [00:12<00:09, 2246.24it/s]Loading:  55%|█████▍    | 26842/49018 [00:12<00:09, 2243.82it/s]Loading:  55%|█████▌    | 27068/49018 [00:12<00:09, 2246.55it/s]Loading:  56%|█████▌    | 27294/49018 [00:12<00:09, 2249.66it/s]Loading:  56%|█████▌    | 27520/49018 [00:12<00:09, 2249.88it/s]Loading:  57%|█████▋    | 27747/49018 [00:12<00:09, 2253.52it/s]Loading:  57%|█████▋    | 27973/49018 [00:12<00:09, 2247.88it/s]Loading:  58%|█████▊    | 28198/49018 [00:12<00:09, 2244.79it/s]Loading:  58%|█████▊    | 28423/49018 [00:12<00:09, 2245.66it/s]Loading:  58%|█████▊    | 28648/49018 [00:12<00:09, 2243.20it/s]Loading:  59%|█████▉    | 28874/49018 [00:13<00:08, 2247.54it/s]Loading:  59%|█████▉    | 29099/49018 [00:13<00:08, 2246.49it/s]Loading:  60%|█████▉    | 29325/49018 [00:13<00:08, 2249.34it/s]Loading:  60%|██████    | 29550/49018 [00:13<00:08, 2247.87it/s]Loading:  61%|██████    | 29775/49018 [00:13<00:08, 2243.07it/s]Loading:  61%|██████    | 30000/49018 [00:13<00:08, 2244.92it/s]Loading:  62%|██████▏   | 30225/49018 [00:13<00:08, 2240.10it/s]Loading:  62%|██████▏   | 30451/49018 [00:13<00:08, 2244.24it/s]Loading:  63%|██████▎   | 30676/49018 [00:13<00:08, 2242.09it/s]Loading:  63%|██████▎   | 30902/49018 [00:13<00:08, 2245.96it/s]Loading:  64%|██████▎   | 31127/49018 [00:14<00:07, 2245.65it/s]Loading:  64%|██████▍   | 31352/49018 [00:14<00:07, 2240.94it/s]Loading:  64%|██████▍   | 31577/49018 [00:14<00:07, 2241.94it/s]Loading:  65%|██████▍   | 31802/49018 [00:14<00:07, 2238.54it/s]Loading:  65%|██████▌   | 32026/49018 [00:14<00:07, 2238.90it/s]Loading:  66%|██████▌   | 32251/49018 [00:14<00:07, 2241.77it/s]Loading:  66%|██████▋   | 32476/49018 [00:14<00:07, 2241.86it/s]Loading:  67%|██████▋   | 32702/49018 [00:14<00:07, 2246.25it/s]Loading:  67%|██████▋   | 32927/49018 [00:14<00:07, 2243.66it/s]Loading:  68%|██████▊   | 33153/49018 [00:14<00:07, 2246.81it/s]Loading:  68%|██████▊   | 33380/49018 [00:15<00:06, 2252.72it/s]Loading:  69%|██████▊   | 33606/49018 [00:15<00:06, 2249.77it/s]Loading:  69%|██████▉   | 33833/49018 [00:15<00:06, 2253.10it/s]Loading:  69%|██████▉   | 34059/49018 [00:15<00:06, 2245.15it/s]Loading:  70%|██████▉   | 34285/49018 [00:15<00:06, 2247.36it/s]Loading:  70%|███████   | 34511/49018 [00:15<00:06, 2248.91it/s]Loading:  71%|███████   | 34736/49018 [00:15<00:06, 2247.91it/s]Loading:  71%|███████▏  | 34963/49018 [00:15<00:06, 2251.60it/s]Loading:  72%|███████▏  | 35189/49018 [00:15<00:06, 2248.73it/s]Loading:  72%|███████▏  | 35415/49018 [00:15<00:06, 2252.03it/s]Loading:  73%|███████▎  | 35641/49018 [00:16<00:05, 2252.83it/s]Loading:  73%|███████▎  | 35867/49018 [00:16<00:05, 2246.39it/s]Loading:  74%|███████▎  | 36092/49018 [00:16<00:05, 2245.15it/s]Loading:  74%|███████▍  | 36317/49018 [00:16<00:05, 2240.65it/s]Loading:  75%|███████▍  | 36542/49018 [00:16<00:05, 2242.12it/s]Loading:  75%|███████▌  | 36767/49018 [00:16<00:05, 2242.23it/s]Loading:  75%|███████▌  | 36993/49018 [00:16<00:05, 2247.26it/s]Loading:  76%|███████▌  | 37219/49018 [00:16<00:05, 2250.05it/s]Loading:  76%|███████▋  | 37445/49018 [00:16<00:05, 2251.64it/s]Loading:  77%|███████▋  | 37671/49018 [00:16<00:05, 2250.07it/s]Loading:  77%|███████▋  | 37897/49018 [00:17<00:04, 2242.93it/s]Loading:  78%|███████▊  | 38122/49018 [00:17<00:04, 2244.77it/s]Loading:  78%|███████▊  | 38347/49018 [00:17<00:04, 2244.41it/s]Loading:  79%|███████▊  | 38572/49018 [00:17<00:04, 2239.12it/s]Loading:  79%|███████▉  | 38798/49018 [00:17<00:04, 2242.96it/s]Loading:  80%|███████▉  | 39023/49018 [00:17<00:04, 2243.40it/s]Loading:  80%|████████  | 39249/49018 [00:17<00:04, 2247.45it/s]Loading:  81%|████████  | 39476/49018 [00:17<00:04, 2251.29it/s]Loading:  81%|████████  | 39702/49018 [00:17<00:04, 2249.15it/s]Loading:  81%|████████▏ | 39928/49018 [00:17<00:04, 2249.96it/s]Loading:  82%|████████▏ | 40153/49018 [00:18<00:03, 2247.76it/s]Loading:  82%|████████▏ | 40379/49018 [00:18<00:03, 2249.09it/s]Loading:  83%|████████▎ | 40604/49018 [00:18<00:03, 2249.21it/s]Loading:  83%|████████▎ | 40829/49018 [00:18<00:03, 2242.57it/s]Loading:  84%|████████▍ | 41054/49018 [00:18<00:03, 2244.04it/s]Loading:  84%|████████▍ | 41279/49018 [00:18<00:03, 2236.34it/s]Loading:  85%|████████▍ | 41504/49018 [00:18<00:03, 2238.82it/s]Loading:  85%|████████▌ | 41729/49018 [00:18<00:03, 2240.00it/s]Loading:  86%|████████▌ | 41956/49018 [00:18<00:03, 2248.33it/s]Loading:  86%|████████▌ | 42182/49018 [00:18<00:03, 2249.89it/s]Loading:  87%|████████▋ | 42407/49018 [00:19<00:02, 2249.35it/s]Loading:  87%|████████▋ | 42633/49018 [00:19<00:02, 2250.85it/s]Loading:  87%|████████▋ | 42859/49018 [00:19<00:02, 2245.97it/s]Loading:  88%|████████▊ | 43085/49018 [00:19<00:02, 2248.35it/s]Loading:  88%|████████▊ | 43311/49018 [00:19<00:02, 2249.70it/s]Loading:  89%|████████▉ | 43537/49018 [00:19<00:02, 2250.67it/s]Loading:  89%|████████▉ | 43763/49018 [00:19<00:02, 2252.66it/s]Loading:  90%|████████▉ | 43989/49018 [00:19<00:02, 2251.34it/s]Loading:  90%|█████████ | 44215/49018 [00:19<00:02, 2251.09it/s]Loading:  91%|█████████ | 44441/49018 [00:19<00:02, 2251.72it/s]Loading:  91%|█████████ | 44667/49018 [00:20<00:01, 2246.41it/s]Loading:  92%|█████████▏| 44894/49018 [00:20<00:01, 2251.72it/s]Loading:  92%|█████████▏| 45121/49018 [00:20<00:01, 2254.86it/s]Loading:  93%|█████████▎| 45347/49018 [00:20<00:01, 2256.07it/s]Loading:  93%|█████████▎| 45574/49018 [00:20<00:01, 2258.12it/s]Loading:  93%|█████████▎| 45800/49018 [00:21<00:03, 941.14it/s] Loading:  94%|█████████▍| 46025/49018 [00:21<00:02, 1138.54it/s]Loading:  94%|█████████▍| 46249/49018 [00:21<00:02, 1334.02it/s]Loading:  95%|█████████▍| 46475/49018 [00:21<00:01, 1520.45it/s]Loading:  95%|█████████▌| 46702/49018 [00:21<00:01, 1687.15it/s]Loading:  96%|█████████▌| 46927/49018 [00:21<00:01, 1822.24it/s]Loading:  96%|█████████▌| 47152/49018 [00:21<00:00, 1931.50it/s]Loading:  97%|█████████▋| 47376/49018 [00:21<00:00, 2013.75it/s]Loading:  97%|█████████▋| 47602/49018 [00:21<00:00, 2081.76it/s]Loading:  98%|█████████▊| 47829/49018 [00:21<00:00, 2132.64it/s]Loading:  98%|█████████▊| 48056/49018 [00:22<00:00, 2170.70it/s]Loading:  98%|█████████▊| 48282/49018 [00:22<00:00, 2194.22it/s]Loading:  99%|█████████▉| 48507/49018 [00:22<00:00, 2207.42it/s]Loading:  99%|█████████▉| 48733/49018 [00:22<00:00, 2222.72it/s]Loading: 100%|█████████▉| 48958/49018 [00:22<00:00, 2226.68it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2182.48it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 231/49018 [00:00<00:21, 2303.81it/s]Loading:   1%|          | 464/49018 [00:00<00:20, 2314.86it/s]Loading:   1%|▏         | 696/49018 [00:00<00:20, 2312.94it/s]Loading:   2%|▏         | 930/49018 [00:00<00:20, 2321.28it/s]Loading:   2%|▏         | 1164/49018 [00:00<00:20, 2326.09it/s]Loading:   3%|▎         | 1397/49018 [00:00<00:20, 2323.40it/s]Loading:   3%|▎         | 1632/49018 [00:00<00:20, 2330.00it/s]Loading:   4%|▍         | 1866/49018 [00:01<00:51, 922.74it/s] Loading:   4%|▍         | 2098/49018 [00:01<00:41, 1134.96it/s]Loading:   5%|▍         | 2332/49018 [00:01<00:34, 1349.38it/s]Loading:   5%|▌         | 2566/49018 [00:01<00:29, 1549.36it/s]Loading:   6%|▌         | 2801/49018 [00:01<00:26, 1727.91it/s]Loading:   6%|▌         | 3034/49018 [00:01<00:24, 1873.51it/s]Loading:   7%|▋         | 3268/49018 [00:01<00:22, 1991.82it/s]Loading:   7%|▋         | 3501/49018 [00:01<00:21, 2081.61it/s]Loading:   8%|▊         | 3735/49018 [00:02<00:21, 2152.13it/s]Loading:   8%|▊         | 3970/49018 [00:02<00:20, 2206.24it/s]Loading:   9%|▊         | 4203/49018 [00:02<00:20, 2240.72it/s]Loading:   9%|▉         | 4438/49018 [00:02<00:19, 2270.41it/s]Loading:  10%|▉         | 4671/49018 [00:02<00:19, 2287.05it/s]Loading:  10%|█         | 4907/49018 [00:02<00:19, 2305.88it/s]Loading:  10%|█         | 5144/49018 [00:02<00:18, 2322.55it/s]Loading:  11%|█         | 5379/49018 [00:02<00:18, 2325.65it/s]Loading:  11%|█▏        | 5613/49018 [00:02<00:18, 2323.67it/s]Loading:  12%|█▏        | 5847/49018 [00:02<00:18, 2317.86it/s]Loading:  12%|█▏        | 6080/49018 [00:03<00:18, 2318.54it/s]Loading:  13%|█▎        | 6313/49018 [00:03<00:18, 2315.40it/s]Loading:  13%|█▎        | 6546/49018 [00:03<00:18, 2317.82it/s]Loading:  14%|█▍        | 6779/49018 [00:03<00:18, 2320.19it/s]Loading:  14%|█▍        | 7012/49018 [00:03<00:18, 2319.25it/s]Loading:  15%|█▍        | 7245/49018 [00:03<00:18, 2317.83it/s]Loading:  15%|█▌        | 7477/49018 [00:03<00:17, 2309.91it/s]Loading:  16%|█▌        | 7710/49018 [00:03<00:17, 2314.20it/s]Loading:  16%|█▌        | 7942/49018 [00:03<00:17, 2311.28it/s]Loading:  17%|█▋        | 8174/49018 [00:03<00:17, 2313.23it/s]Loading:  17%|█▋        | 8407/49018 [00:04<00:17, 2317.05it/s]Loading:  18%|█▊        | 8639/49018 [00:04<00:17, 2315.80it/s]Loading:  18%|█▊        | 8871/49018 [00:04<00:17, 2313.26it/s]Loading:  19%|█▊        | 9103/49018 [00:04<00:17, 2310.24it/s]Loading:  19%|█▉        | 9335/49018 [00:04<00:17, 2312.14it/s]Loading:  20%|█▉        | 9567/49018 [00:04<00:17, 2310.11it/s]Loading:  20%|█▉        | 9800/49018 [00:04<00:16, 2314.86it/s]Loading:  20%|██        | 10032/49018 [00:04<00:16, 2314.78it/s]Loading:  21%|██        | 10264/49018 [00:04<00:16, 2309.93it/s]Loading:  21%|██▏       | 10497/49018 [00:05<00:16, 2313.82it/s]Loading:  22%|██▏       | 10729/49018 [00:05<00:16, 2311.59it/s]Loading:  22%|██▏       | 10961/49018 [00:05<00:16, 2313.83it/s]Loading:  23%|██▎       | 11193/49018 [00:05<00:16, 2314.68it/s]Loading:  23%|██▎       | 11425/49018 [00:05<00:16, 2309.28it/s]Loading:  24%|██▍       | 11657/49018 [00:05<00:16, 2312.39it/s]Loading:  24%|██▍       | 11889/49018 [00:05<00:16, 2308.51it/s]Loading:  25%|██▍       | 12121/49018 [00:05<00:15, 2311.40it/s]Loading:  25%|██▌       | 12353/49018 [00:05<00:15, 2309.22it/s]Loading:  26%|██▌       | 12585/49018 [00:05<00:15, 2309.65it/s]Loading:  26%|██▌       | 12817/49018 [00:06<00:15, 2311.14it/s]Loading:  27%|██▋       | 13049/49018 [00:06<00:15, 2308.45it/s]Loading:  27%|██▋       | 13282/49018 [00:06<00:15, 2311.98it/s]Loading:  28%|██▊       | 13514/49018 [00:06<00:15, 2306.12it/s]Loading:  28%|██▊       | 13746/49018 [00:06<00:15, 2307.76it/s]Loading:  29%|██▊       | 13978/49018 [00:06<00:15, 2310.41it/s]Loading:  29%|██▉       | 14210/49018 [00:06<00:15, 2307.39it/s]Loading:  29%|██▉       | 14441/49018 [00:06<00:14, 2307.24it/s]Loading:  30%|██▉       | 14672/49018 [00:06<00:14, 2306.32it/s]Loading:  30%|███       | 14903/49018 [00:06<00:14, 2304.47it/s]Loading:  31%|███       | 15134/49018 [00:07<00:14, 2304.01it/s]Loading:  31%|███▏      | 15367/49018 [00:07<00:14, 2309.89it/s]Loading:  32%|███▏      | 15598/49018 [00:07<00:14, 2309.59it/s]Loading:  32%|███▏      | 15829/49018 [00:07<00:14, 2305.91it/s]Loading:  33%|███▎      | 16061/49018 [00:07<00:14, 2307.61it/s]Loading:  33%|███▎      | 16292/49018 [00:07<00:14, 2300.07it/s]Loading:  34%|███▎      | 16523/49018 [00:07<00:14, 2302.24it/s]Loading:  34%|███▍      | 16754/49018 [00:07<00:14, 2300.55it/s]Loading:  35%|███▍      | 16985/49018 [00:07<00:13, 2301.21it/s]Loading:  35%|███▌      | 17216/49018 [00:07<00:13, 2302.01it/s]Loading:  36%|███▌      | 17447/49018 [00:08<00:13, 2296.63it/s]Loading:  36%|███▌      | 17678/49018 [00:08<00:13, 2299.01it/s]Loading:  37%|███▋      | 17908/49018 [00:08<00:13, 2295.57it/s]Loading:  37%|███▋      | 18138/49018 [00:08<00:13, 2283.25it/s]Loading:  37%|███▋      | 18367/49018 [00:08<00:13, 2270.92it/s]Loading:  38%|███▊      | 18595/49018 [00:08<00:13, 2255.20it/s]Loading:  38%|███▊      | 18821/49018 [00:08<00:13, 2249.29it/s]Loading:  39%|███▉      | 19046/49018 [00:08<00:13, 2242.48it/s]Loading:  39%|███▉      | 19271/49018 [00:08<00:13, 2244.11it/s]Loading:  40%|███▉      | 19496/49018 [00:08<00:13, 2240.95it/s]Loading:  40%|████      | 19721/49018 [00:09<00:13, 2234.40it/s]Loading:  41%|████      | 19945/49018 [00:09<00:13, 2233.54it/s]Loading:  41%|████      | 20169/49018 [00:09<00:12, 2231.93it/s]Loading:  42%|████▏     | 20394/49018 [00:09<00:12, 2237.25it/s]Loading:  42%|████▏     | 20619/49018 [00:09<00:12, 2238.20it/s]Loading:  43%|████▎     | 20844/49018 [00:09<00:12, 2241.62it/s]Loading:  43%|████▎     | 21069/49018 [00:09<00:12, 2243.34it/s]Loading:  43%|████▎     | 21294/49018 [00:09<00:12, 2242.29it/s]Loading:  44%|████▍     | 21519/49018 [00:09<00:12, 2242.56it/s]Loading:  44%|████▍     | 21744/49018 [00:09<00:12, 2241.10it/s]Loading:  45%|████▍     | 21970/49018 [00:10<00:12, 2243.68it/s]Loading:  45%|████▌     | 22195/49018 [00:10<00:11, 2242.01it/s]Loading:  46%|████▌     | 22420/49018 [00:10<00:11, 2238.27it/s]Loading:  46%|████▌     | 22644/49018 [00:10<00:11, 2238.19it/s]Loading:  47%|████▋     | 22868/49018 [00:10<00:11, 2233.04it/s]Loading:  47%|████▋     | 23093/49018 [00:10<00:11, 2237.36it/s]Loading:  48%|████▊     | 23319/49018 [00:10<00:11, 2242.97it/s]Loading:  48%|████▊     | 23544/49018 [00:10<00:11, 2241.41it/s]Loading:  48%|████▊     | 23769/49018 [00:10<00:11, 2243.34it/s]Loading:  49%|████▉     | 23994/49018 [00:10<00:11, 2240.60it/s]Loading:  49%|████▉     | 24219/49018 [00:11<00:11, 2237.07it/s]Loading:  50%|████▉     | 24444/49018 [00:11<00:10, 2238.69it/s]Loading:  50%|█████     | 24668/49018 [00:11<00:10, 2235.08it/s]Loading:  51%|█████     | 24893/49018 [00:11<00:10, 2237.68it/s]Loading:  51%|█████     | 25117/49018 [00:11<00:10, 2234.37it/s]Loading:  52%|█████▏    | 25342/49018 [00:11<00:10, 2238.30it/s]Loading:  52%|█████▏    | 25566/49018 [00:11<00:10, 2238.51it/s]Loading:  53%|█████▎    | 25790/49018 [00:11<00:10, 2231.48it/s]Loading:  53%|█████▎    | 26014/49018 [00:11<00:10, 2233.66it/s]Loading:  54%|█████▎    | 26238/49018 [00:11<00:10, 2229.26it/s]Loading:  54%|█████▍    | 26462/49018 [00:12<00:10, 2230.87it/s]Loading:  54%|█████▍    | 26686/49018 [00:12<00:10, 2227.89it/s]Loading:  55%|█████▍    | 26910/49018 [00:12<00:09, 2230.16it/s]Loading:  55%|█████▌    | 27135/49018 [00:12<00:09, 2235.54it/s]Loading:  56%|█████▌    | 27359/49018 [00:12<00:09, 2234.98it/s]Loading:  56%|█████▋    | 27584/49018 [00:12<00:09, 2238.27it/s]Loading:  57%|█████▋    | 27808/49018 [00:12<00:09, 2236.68it/s]Loading:  57%|█████▋    | 28032/49018 [00:12<00:09, 2236.76it/s]Loading:  58%|█████▊    | 28256/49018 [00:12<00:09, 2236.26it/s]Loading:  58%|█████▊    | 28480/49018 [00:12<00:09, 2230.78it/s]Loading:  59%|█████▊    | 28705/49018 [00:13<00:09, 2234.50it/s]Loading:  59%|█████▉    | 28929/49018 [00:13<00:08, 2234.90it/s]Loading:  59%|█████▉    | 29155/49018 [00:13<00:08, 2240.11it/s]Loading:  60%|█████▉    | 29380/49018 [00:13<00:08, 2242.78it/s]Loading:  60%|██████    | 29605/49018 [00:13<00:08, 2237.88it/s]Loading:  61%|██████    | 29829/49018 [00:13<00:08, 2237.38it/s]Loading:  61%|██████▏   | 30053/49018 [00:13<00:08, 2236.91it/s]Loading:  62%|██████▏   | 30278/49018 [00:13<00:08, 2239.08it/s]Loading:  62%|██████▏   | 30503/49018 [00:13<00:08, 2239.69it/s]Loading:  63%|██████▎   | 30727/49018 [00:13<00:08, 2238.05it/s]Loading:  63%|██████▎   | 30953/49018 [00:14<00:08, 2242.89it/s]Loading:  64%|██████▎   | 31178/49018 [00:14<00:07, 2241.17it/s]Loading:  64%|██████▍   | 31403/49018 [00:14<00:07, 2241.89it/s]Loading:  65%|██████▍   | 31628/49018 [00:14<00:07, 2241.43it/s]Loading:  65%|██████▍   | 31853/49018 [00:14<00:07, 2239.70it/s]Loading:  65%|██████▌   | 32078/49018 [00:14<00:07, 2242.07it/s]Loading:  66%|██████▌   | 32303/49018 [00:14<00:07, 2239.80it/s]Loading:  66%|██████▋   | 32528/49018 [00:14<00:07, 2241.99it/s]Loading:  67%|██████▋   | 32753/49018 [00:14<00:07, 2237.83it/s]Loading:  67%|██████▋   | 32977/49018 [00:14<00:07, 2238.34it/s]Loading:  68%|██████▊   | 33201/49018 [00:15<00:07, 2238.38it/s]Loading:  68%|██████▊   | 33425/49018 [00:15<00:06, 2234.86it/s]Loading:  69%|██████▊   | 33651/49018 [00:15<00:06, 2241.62it/s]Loading:  69%|██████▉   | 33876/49018 [00:15<00:06, 2242.36it/s]Loading:  70%|██████▉   | 34102/49018 [00:15<00:06, 2244.99it/s]Loading:  70%|███████   | 34327/49018 [00:15<00:06, 2245.08it/s]Loading:  70%|███████   | 34552/49018 [00:15<00:06, 2242.45it/s]Loading:  71%|███████   | 34777/49018 [00:15<00:06, 2243.52it/s]Loading:  71%|███████▏  | 35002/49018 [00:15<00:06, 2240.05it/s]Loading:  72%|███████▏  | 35227/49018 [00:15<00:06, 2241.99it/s]Loading:  72%|███████▏  | 35452/49018 [00:16<00:06, 2244.08it/s]Loading:  73%|███████▎  | 35677/49018 [00:16<00:05, 2240.19it/s]Loading:  73%|███████▎  | 35903/49018 [00:16<00:05, 2245.90it/s]Loading:  74%|███████▎  | 36128/49018 [00:16<00:05, 2244.13it/s]Loading:  74%|███████▍  | 36353/49018 [00:16<00:05, 2245.36it/s]Loading:  75%|███████▍  | 36578/49018 [00:16<00:05, 2245.90it/s]Loading:  75%|███████▌  | 36803/49018 [00:16<00:05, 2243.13it/s]Loading:  76%|███████▌  | 37028/49018 [00:16<00:05, 2244.13it/s]Loading:  76%|███████▌  | 37253/49018 [00:16<00:05, 2245.53it/s]Loading:  76%|███████▋  | 37479/49018 [00:16<00:05, 2247.87it/s]Loading:  77%|███████▋  | 37704/49018 [00:17<00:05, 2247.36it/s]Loading:  77%|███████▋  | 37929/49018 [00:17<00:13, 815.97it/s] Loading:  78%|███████▊  | 38154/49018 [00:17<00:10, 1008.35it/s]Loading:  78%|███████▊  | 38377/49018 [00:17<00:08, 1205.08it/s]Loading:  79%|███████▉  | 38602/49018 [00:18<00:07, 1399.70it/s]Loading:  79%|███████▉  | 38827/49018 [00:18<00:06, 1578.68it/s]Loading:  80%|███████▉  | 39051/49018 [00:18<00:05, 1730.63it/s]Loading:  80%|████████  | 39276/49018 [00:18<00:05, 1859.24it/s]Loading:  81%|████████  | 39500/49018 [00:18<00:04, 1957.69it/s]Loading:  81%|████████  | 39724/49018 [00:18<00:04, 2032.99it/s]Loading:  81%|████████▏ | 39948/49018 [00:18<00:04, 2089.79it/s]Loading:  82%|████████▏ | 40171/49018 [00:18<00:04, 2128.52it/s]Loading:  82%|████████▏ | 40395/49018 [00:18<00:03, 2159.10it/s]Loading:  83%|████████▎ | 40619/49018 [00:18<00:03, 2180.19it/s]Loading:  83%|████████▎ | 40843/49018 [00:19<00:03, 2197.09it/s]Loading:  84%|████████▍ | 41068/49018 [00:19<00:03, 2212.05it/s]Loading:  84%|████████▍ | 41292/49018 [00:19<00:03, 2217.72it/s]Loading:  85%|████████▍ | 41517/49018 [00:19<00:03, 2224.38it/s]Loading:  85%|████████▌ | 41741/49018 [00:19<00:03, 2227.10it/s]Loading:  86%|████████▌ | 41966/49018 [00:19<00:03, 2232.86it/s]Loading:  86%|████████▌ | 42190/49018 [00:19<00:03, 2233.59it/s]Loading:  87%|████████▋ | 42415/49018 [00:19<00:02, 2237.44it/s]Loading:  87%|████████▋ | 42640/49018 [00:19<00:02, 2240.10it/s]Loading:  87%|████████▋ | 42865/49018 [00:19<00:02, 2239.92it/s]Loading:  88%|████████▊ | 43090/49018 [00:20<00:02, 2240.84it/s]Loading:  88%|████████▊ | 43315/49018 [00:20<00:02, 2235.10it/s]Loading:  89%|████████▉ | 43539/49018 [00:20<00:02, 2234.03it/s]Loading:  89%|████████▉ | 43763/49018 [00:20<00:02, 2234.57it/s]Loading:  90%|████████▉ | 43987/49018 [00:20<00:02, 2234.68it/s]Loading:  90%|█████████ | 44212/49018 [00:20<00:02, 2237.21it/s]Loading:  91%|█████████ | 44436/49018 [00:20<00:02, 2235.05it/s]Loading:  91%|█████████ | 44661/49018 [00:20<00:01, 2237.26it/s]Loading:  92%|█████████▏| 44886/49018 [00:20<00:01, 2239.62it/s]Loading:  92%|█████████▏| 45110/49018 [00:20<00:01, 2237.04it/s]Loading:  92%|█████████▏| 45334/49018 [00:21<00:01, 2237.86it/s]Loading:  93%|█████████▎| 45558/49018 [00:21<00:01, 2233.56it/s]Loading:  93%|█████████▎| 45783/49018 [00:21<00:01, 2235.59it/s]Loading:  94%|█████████▍| 46007/49018 [00:21<00:01, 2235.76it/s]Loading:  94%|█████████▍| 46231/49018 [00:21<00:01, 2235.52it/s]Loading:  95%|█████████▍| 46455/49018 [00:21<00:01, 2236.56it/s]Loading:  95%|█████████▌| 46679/49018 [00:21<00:01, 2233.14it/s]Loading:  96%|█████████▌| 46903/49018 [00:21<00:00, 2233.94it/s]Loading:  96%|█████████▌| 47127/49018 [00:21<00:00, 2234.74it/s]Loading:  97%|█████████▋| 47351/49018 [00:21<00:00, 2234.56it/s]Loading:  97%|█████████▋| 47578/49018 [00:22<00:00, 2242.90it/s]Loading:  98%|█████████▊| 47803/49018 [00:22<00:00, 2242.73it/s]Loading:  98%|█████████▊| 48028/49018 [00:22<00:00, 2242.70it/s]Loading:  98%|█████████▊| 48253/49018 [00:22<00:00, 2239.18it/s]Loading:  99%|█████████▉| 48478/49018 [00:22<00:00, 2241.14it/s]Loading:  99%|█████████▉| 48703/49018 [00:22<00:00, 2242.44it/s]Loading: 100%|█████████▉| 48928/49018 [00:22<00:00, 2241.35it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2159.71it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank24]:[W424 18:14:53.858585411 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 18:14:53.880739642 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 18:14:53.880862395 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 18:14:53.883674853 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 18:14:53.884469712 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 18:14:53.886976209 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 18:14:53.942579705 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 18:14:53.944210854 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 18:14:53.945736724 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 18:14:53.951120452 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 18:14:53.952737755 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 18:14:53.954598680 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 18:14:53.913713950 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 18:14:53.965695037 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 18:14:53.977008967 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 18:14:54.487633079 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 18:14:54.534838654 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 18:14:54.535752507 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 18:14:54.538549230 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 18:14:54.539007188 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 18:14:54.545389099 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 18:14:54.937294264 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 18:14:54.942381798 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 18:14:54.947156437 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 18:14:54.690233632 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 18:14:54.959938090 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 18:14:54.712493127 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 18:14:54.714311185 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 18:14:54.714466830 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 18:14:54.715615248 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 18:14:54.727879397 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 18:14:54.067828584 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 18:14:54.957032841 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 18:14:54.328412816 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 18:14:54.329383201 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 18:14:54.329405523 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 18:14:54.958397669 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 18:14:54.282037332 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 18:14:54.543554466 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 18:14:55.480170882 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:24<19:44, 24.17s/it]Train:   4%|▍         | 2/50 [00:24<08:03, 10.08s/it]Train:   6%|▌         | 3/50 [00:24<04:22,  5.58s/it]Train:   8%|▊         | 4/50 [00:24<02:39,  3.46s/it]Train:  10%|█         | 5/50 [00:25<01:42,  2.29s/it]Train:  12%|█▏        | 6/50 [00:25<01:09,  1.58s/it]Train:  14%|█▍        | 7/50 [00:25<00:48,  1.13s/it]Train:  16%|█▌        | 8/50 [00:25<00:35,  1.19it/s]Train:  18%|█▊        | 9/50 [00:25<00:26,  1.55it/s]Train:  20%|██        | 10/50 [00:26<00:20,  1.96it/s]Train:  22%|██▏       | 11/50 [00:26<00:16,  2.39it/s]Train:  24%|██▍       | 12/50 [00:26<00:13,  2.81it/s]Train:  26%|██▌       | 13/50 [00:26<00:11,  3.20it/s]Train:  28%|██▊       | 14/50 [00:26<00:10,  3.54it/s]Train:  30%|███       | 15/50 [00:27<00:09,  3.83it/s]Train:  32%|███▏      | 16/50 [00:27<00:08,  4.06it/s]Train:  34%|███▍      | 17/50 [00:27<00:07,  4.23it/s]Train:  36%|███▌      | 18/50 [00:27<00:07,  4.37it/s]Train:  38%|███▊      | 19/50 [00:27<00:06,  4.46it/s]Train:  40%|████      | 20/50 [00:28<00:06,  4.53it/s]Train:  42%|████▏     | 21/50 [00:28<00:06,  4.59it/s]Train:  44%|████▍     | 22/50 [00:28<00:06,  4.62it/s]Train:  46%|████▌     | 23/50 [00:28<00:05,  4.66it/s]Train:  48%|████▊     | 24/50 [00:29<00:05,  4.66it/s]Train:  50%|█████     | 25/50 [00:29<00:05,  4.67it/s]Train:  52%|█████▏    | 26/50 [00:29<00:05,  4.68it/s]Train:  54%|█████▍    | 27/50 [00:29<00:04,  4.71it/s]Train:  56%|█████▌    | 28/50 [00:29<00:04,  4.70it/s]Train:  58%|█████▊    | 29/50 [00:30<00:04,  4.73it/s]Train:  60%|██████    | 30/50 [00:30<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:30<00:04,  4.74it/s]Train:  64%|██████▍   | 32/50 [00:30<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:30<00:03,  4.74it/s]Train:  68%|██████▊   | 34/50 [00:31<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:31<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:31<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:31<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:32<00:02,  4.73it/s]Train:  78%|███████▊  | 39/50 [00:32<00:02,  4.73it/s]Train:  80%|████████  | 40/50 [00:32<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:32<00:01,  4.72it/s]Train:  84%|████████▍ | 42/50 [00:32<00:01,  4.72it/s]Train:  86%|████████▌ | 43/50 [00:33<00:01,  4.72it/s]Train:  88%|████████▊ | 44/50 [00:33<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:33<00:01,  4.71it/s]Train:  92%|█████████▏| 46/50 [00:33<00:00,  4.72it/s]Train:  94%|█████████▍| 47/50 [00:33<00:00,  4.70it/s]Train:  96%|█████████▌| 48/50 [00:34<00:00,  4.72it/s]Train:  98%|█████████▊| 49/50 [00:34<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:34<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:34<00:00,  1.44it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.22it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.26it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.44it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.56it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.60it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.62it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.66it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.71it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.71it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.71it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.72it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.73it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.73it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.72it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.72it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.71it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.72it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.73it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.74it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.75it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.75it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.76it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.75it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.74it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.75it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.76it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.75it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.75it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.77it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.75it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.76it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.77it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.76it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.76it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.77it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.75it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.76it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.76it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.76it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.77it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.75it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.23it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.98it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.31it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.47it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.62it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.66it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.66it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.67it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.72it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.72it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.71it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.72it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.74it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.73it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.74it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.72it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.73it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.73it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.73it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.73it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.71it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.70it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.69it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.71it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.72it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.73it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.75it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.74it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.75it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.74it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.73it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.74it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.73it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.73it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.73it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.71it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.73it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.73it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.67it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.17it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.94it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.27it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.44it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.54it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.62it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.67it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.71it/s]Train:  18%|█▊        | 9/50 [00:01<00:08,  4.73it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.73it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.73it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.74it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.74it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.73it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.74it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.73it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.75it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.77it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.78it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.79it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.78it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.76it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.72it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.70it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.71it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.71it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.65it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.62it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.63it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.64it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.66it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.69it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.70it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.71it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.72it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.73it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.75it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.74it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.75it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.74it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.75it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.74it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.74it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.75it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.67it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.18it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.93it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.25it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.43it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.65it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.66it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.70it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.72it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.73it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.74it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.77it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.75it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.74it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.74it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.74it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.75it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.75it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.75it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.75it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.76it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.77it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.78it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.77it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.76it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.75it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.75it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.75it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.74it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.76it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.77it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.77it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.77it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.78it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.76it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.77it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.74it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.73it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.69it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.71it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.74it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.74it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.76it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]
0: Process 0 - Local timer:  load_data  :  96.95
0: Process 0 - Local timer:  train_validate_test  :  77.53
0: Process 0 - Local timer:  create_model  :  1.29
0: Minimum timers: 
0: load_data  :  96.95
0: train_validate_test  :  77.48
0: create_model  :  1.29
0: Maximum timers: 
0: load_data  :  97.35
0: train_validate_test  :  77.54
0: create_model  :  1.32
0: Average timers: 
0: load_data  :  97.22
0: train_validate_test  :  77.49
0: create_model  :  1.3
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 18:15:49.374126993 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 18:15:51.047421390 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 18:15:51.047588897 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 18:15:51.047604898 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 18:15:51.047686001 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 18:15:51.048238149 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 18:15:51.937566039 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 18:15:51.937606185 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 18:15:51.606117689 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 18:15:51.937613279 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 18:15:51.606135413 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 18:15:51.937610103 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 18:15:51.606158848 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 18:15:51.937684454 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 18:15:51.606170600 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 18:15:51.937694914 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 18:15:51.049301986 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 18:15:51.606182613 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 18:15:51.937788080 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 18:15:51.606155732 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 18:15:51.606198282 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 18:15:51.606310525 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 18:15:51.049814438 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 18:15:51.938580433 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 18:15:51.650818072 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 18:15:51.650795690 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 18:15:51.650812432 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 18:15:51.650797644 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 18:15:51.650850985 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 18:15:51.650873688 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 18:15:51.650926908 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 18:15:51.650962296 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 18:15:51.310876605 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 18:15:51.310874511 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 18:15:51.310875584 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 18:15:51.310852179 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 18:15:51.310920820 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 18:15:51.311077057 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 18:15:51.311083639 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 18:15:51.311373390 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 06:15:54 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 06:15:54 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_4 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 18:16:06.803533807 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.803798017 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.803841510 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.803875454 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.803916832 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.805247162 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.805390814 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.093462522 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.093583762 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.093581848 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.093572280 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.806865568 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.095335704 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.095339000 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.095365480 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.095413050 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.764501618 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.764543357 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.764557534 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.764696588 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.764713500 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467579787 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467601949 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467669097 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467709043 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467722208 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.467735513 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.766146119 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.766147301 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.469339433 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.469354432 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.767435576 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 18:16:06.249654238 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.249629461 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.249669868 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.251411340 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.251638812 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.251727630 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.252940811 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:16:06.275772211 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_4 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
0: Read attr time (sec):  0.0016911029815673828
0: read and bcast: trainset/x/variable_count 0.1837170124053955
0: read and bcast: trainset/x/variable_offset 0.371126651763916
0: read and bcast: trainset/x/variable_dim 0.37154293060302734
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5941519737243652
0: read and bcast: trainset/edge_index/variable_offset 0.783686637878418
0: read and bcast: trainset/edge_index/variable_dim 0.7936561107635498
0: read and bcast: trainset/edge_attr/variable_count 0.979860782623291
0: read and bcast: trainset/edge_attr/variable_offset 1.1663343906402588
0: read and bcast: trainset/edge_attr/variable_dim 1.17726469039917
0: read and bcast: trainset/pos/variable_count 1.3636188507080078
0: read and bcast: trainset/pos/variable_offset 1.5532550811767578
0: read and bcast: trainset/pos/variable_dim 1.5627574920654297
0: read and bcast: trainset/energy/variable_count 1.750554084777832
0: read and bcast: trainset/energy/variable_offset 1.9376521110534668
0: read and bcast: trainset/energy/variable_dim 1.9478189945220947
0: read and bcast: trainset/forces/variable_count 2.131495475769043
0: read and bcast: trainset/forces/variable_offset 2.317376136779785
0: read and bcast: trainset/forces/variable_dim 2.327099323272705
0: read and bcast: trainset/y/variable_count 2.5115537643432617
0: read and bcast: trainset/y/variable_offset 2.697559356689453
0: read and bcast: trainset/y/variable_dim 2.708308219909668
0: Overall time (sec):  2.710235118865967
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.7147369384765625
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0008573532104492188
0: read and bcast: valset/x/variable_count 0.008150577545166016
0: read and bcast: valset/x/variable_offset 0.015929698944091797
0: read and bcast: valset/x/variable_dim 0.016127586364746094
0: read and bcast: valset/edge_index/variable_count 0.023104190826416016
0: read and bcast: valset/edge_index/variable_offset 0.030034780502319336
0: read and bcast: valset/edge_index/variable_dim 0.030207157135009766
0: read and bcast: valset/edge_attr/variable_count 0.037485599517822266
0: read and bcast: valset/edge_attr/variable_offset 0.044940948486328125
0: read and bcast: valset/edge_attr/variable_dim 0.04513239860534668
0: read and bcast: valset/pos/variable_count 0.05261850357055664
0: read and bcast: valset/pos/variable_offset 0.05995607376098633
0: read and bcast: valset/pos/variable_dim 0.06014084815979004
0: read and bcast: valset/energy/variable_count 0.06763887405395508
0: read and bcast: valset/energy/variable_offset 0.07483792304992676
0: read and bcast: valset/energy/variable_dim 0.07500028610229492
0: read and bcast: valset/forces/variable_count 0.08191347122192383
0: read and bcast: valset/forces/variable_offset 0.08958864212036133
0: read and bcast: valset/forces/variable_dim 0.08981084823608398
0: read and bcast: valset/y/variable_count 0.0973513126373291
0: read and bcast: valset/y/variable_offset 0.10506868362426758
0: read and bcast: valset/y/variable_dim 0.10526132583618164
0: Overall time (sec):  0.10631513595581055
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10953807830810547
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007708072662353516
0: read and bcast: testset/x/variable_count 0.007770061492919922
0: read and bcast: testset/x/variable_offset 0.014991521835327148
0: read and bcast: testset/x/variable_dim 0.01519632339477539
0: read and bcast: testset/edge_index/variable_count 0.02217555046081543
0: read and bcast: testset/edge_index/variable_offset 0.029207944869995117
0: read and bcast: testset/edge_index/variable_dim 0.029410123825073242
0: read and bcast: testset/edge_attr/variable_count 0.03673887252807617
0: read and bcast: testset/edge_attr/variable_offset 0.044269561767578125
0: read and bcast: testset/edge_attr/variable_dim 0.044492483139038086
0: read and bcast: testset/pos/variable_count 0.051416873931884766
0: read and bcast: testset/pos/variable_offset 0.05891728401184082
0: read and bcast: testset/pos/variable_dim 0.05915260314941406
0: read and bcast: testset/energy/variable_count 0.06656479835510254
0: read and bcast: testset/energy/variable_offset 0.07393002510070801
0: read and bcast: testset/energy/variable_dim 0.07413864135742188
0: read and bcast: testset/forces/variable_count 0.08133888244628906
0: read and bcast: testset/forces/variable_offset 0.0883946418762207
0: read and bcast: testset/forces/variable_dim 0.08858299255371094
0: read and bcast: testset/y/variable_count 0.09606385231018066
0: read and bcast: testset/y/variable_offset 0.10374069213867188
0: read and bcast: testset/y/variable_dim 0.10394787788391113
0: Overall time (sec):  0.10490965843200684
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10785770416259766
1 ani1x nsplit: 4460384 6 743398
3 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
2 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
10 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
9 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
31 transition1x nsplit: 8680250 11 789114
30 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
34 transition1x nsplit: 8680250 11 789114
37 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
32 transition1x nsplit: 8680250 11 789114
20 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
23 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
19 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
26 alexandria nsplit: 9705384 11 882307
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
0: Adios reading time (sec):  0.4806187152862549
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.14129328727722168
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.13604426383972168
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 227/64000 [00:00<00:28, 2264.61it/s]Loading:   1%|          | 464/64000 [00:00<00:27, 2324.13it/s]Loading:   1%|          | 699/64000 [00:00<00:27, 2333.38it/s]Loading:   1%|▏         | 934/64000 [00:00<00:26, 2336.72it/s]Loading:   2%|▏         | 1168/64000 [00:00<00:26, 2337.67it/s]Loading:   2%|▏         | 1402/64000 [00:00<00:26, 2320.91it/s]Loading:   3%|▎         | 1635/64000 [00:00<00:27, 2302.25it/s]Loading:   3%|▎         | 1866/64000 [00:00<00:27, 2294.63it/s]Loading:   3%|▎         | 2096/64000 [00:00<00:27, 2277.93it/s]Loading:   4%|▎         | 2324/64000 [00:01<00:27, 2268.28it/s]Loading:   4%|▍         | 2551/64000 [00:01<00:27, 2268.41it/s]Loading:   4%|▍         | 2778/64000 [00:01<00:27, 2264.61it/s]Loading:   5%|▍         | 3005/64000 [00:01<00:26, 2266.04it/s]Loading:   5%|▌         | 3232/64000 [00:01<00:26, 2266.96it/s]Loading:   5%|▌         | 3459/64000 [00:01<00:26, 2263.12it/s]Loading:   6%|▌         | 3686/64000 [00:01<00:26, 2263.22it/s]Loading:   6%|▌         | 3913/64000 [00:01<00:40, 1468.16it/s]Loading:   6%|▋         | 4138/64000 [00:01<00:36, 1637.67it/s]Loading:   7%|▋         | 4365/64000 [00:02<00:33, 1786.74it/s]Loading:   7%|▋         | 4592/64000 [00:02<00:31, 1908.50it/s]Loading:   8%|▊         | 4819/64000 [00:02<00:29, 2003.56it/s]Loading:   8%|▊         | 5044/64000 [00:02<00:28, 2069.60it/s]Loading:   8%|▊         | 5270/64000 [00:02<00:27, 2120.74it/s]Loading:   9%|▊         | 5496/64000 [00:02<00:27, 2158.92it/s]Loading:   9%|▉         | 5722/64000 [00:02<00:26, 2186.94it/s]Loading:   9%|▉         | 5949/64000 [00:02<00:26, 2208.94it/s]Loading:  10%|▉         | 6174/64000 [00:02<00:26, 2219.58it/s]Loading:  10%|█         | 6401/64000 [00:02<00:25, 2232.10it/s]Loading:  10%|█         | 6627/64000 [00:03<00:25, 2240.26it/s]Loading:  11%|█         | 6854/64000 [00:03<00:25, 2247.91it/s]Loading:  11%|█         | 7082/64000 [00:03<00:25, 2255.45it/s]Loading:  11%|█▏        | 7309/64000 [00:03<00:25, 2257.44it/s]Loading:  12%|█▏        | 7537/64000 [00:03<00:24, 2263.17it/s]Loading:  12%|█▏        | 7764/64000 [00:03<00:24, 2262.89it/s]Loading:  12%|█▏        | 7991/64000 [00:03<00:24, 2264.62it/s]Loading:  13%|█▎        | 8219/64000 [00:03<00:24, 2267.72it/s]Loading:  13%|█▎        | 8446/64000 [00:03<00:24, 2267.59it/s]Loading:  14%|█▎        | 8674/64000 [00:03<00:24, 2268.63it/s]Loading:  14%|█▍        | 8901/64000 [00:04<00:24, 2264.86it/s]Loading:  14%|█▍        | 9129/64000 [00:04<00:24, 2267.93it/s]Loading:  15%|█▍        | 9357/64000 [00:04<00:24, 2270.25it/s]Loading:  15%|█▍        | 9585/64000 [00:04<00:24, 2263.65it/s]Loading:  15%|█▌        | 9812/64000 [00:04<00:23, 2261.58it/s]Loading:  16%|█▌        | 10039/64000 [00:04<00:23, 2254.82it/s]Loading:  16%|█▌        | 10265/64000 [00:04<00:23, 2255.36it/s]Loading:  16%|█▋        | 10491/64000 [00:04<00:23, 2251.66it/s]Loading:  17%|█▋        | 10717/64000 [00:04<00:23, 2253.04it/s]Loading:  17%|█▋        | 10943/64000 [00:05<00:23, 2254.23it/s]Loading:  17%|█▋        | 11169/64000 [00:05<00:23, 2254.26it/s]Loading:  18%|█▊        | 11396/64000 [00:05<00:23, 2257.31it/s]Loading:  18%|█▊        | 11622/64000 [00:05<00:23, 2254.97it/s]Loading:  19%|█▊        | 11849/64000 [00:05<00:23, 2257.30it/s]Loading:  19%|█▉        | 12076/64000 [00:05<00:22, 2259.51it/s]Loading:  19%|█▉        | 12302/64000 [00:05<00:34, 1513.35it/s]Loading:  20%|█▉        | 12530/64000 [00:05<00:30, 1682.41it/s]Loading:  20%|█▉        | 12756/64000 [00:05<00:28, 1821.00it/s]Loading:  20%|██        | 12983/64000 [00:06<00:26, 1935.47it/s]Loading:  21%|██        | 13210/64000 [00:06<00:25, 2023.24it/s]Loading:  21%|██        | 13436/64000 [00:06<00:24, 2086.34it/s]Loading:  21%|██▏       | 13662/64000 [00:06<00:23, 2133.91it/s]Loading:  22%|██▏       | 13887/64000 [00:06<00:23, 2165.79it/s]Loading:  22%|██▏       | 14113/64000 [00:06<00:22, 2191.00it/s]Loading:  22%|██▏       | 14339/64000 [00:06<00:22, 2210.88it/s]Loading:  23%|██▎       | 14565/64000 [00:06<00:22, 2222.69it/s]Loading:  23%|██▎       | 14792/64000 [00:06<00:22, 2235.35it/s]Loading:  23%|██▎       | 15017/64000 [00:06<00:21, 2239.36it/s]Loading:  24%|██▍       | 15244/64000 [00:07<00:21, 2246.80it/s]Loading:  24%|██▍       | 15471/64000 [00:07<00:21, 2251.52it/s]Loading:  25%|██▍       | 15697/64000 [00:07<00:21, 2248.00it/s]Loading:  25%|██▍       | 15923/64000 [00:07<00:21, 2249.95it/s]Loading:  25%|██▌       | 16149/64000 [00:07<00:21, 2247.51it/s]Loading:  26%|██▌       | 16375/64000 [00:07<00:21, 2251.12it/s]Loading:  26%|██▌       | 16601/64000 [00:07<00:21, 2251.96it/s]Loading:  26%|██▋       | 16827/64000 [00:07<00:20, 2252.01it/s]Loading:  27%|██▋       | 17053/64000 [00:07<00:20, 2251.90it/s]Loading:  27%|██▋       | 17279/64000 [00:07<00:20, 2253.72it/s]Loading:  27%|██▋       | 17505/64000 [00:08<00:20, 2254.75it/s]Loading:  28%|██▊       | 17731/64000 [00:08<00:20, 2254.09it/s]Loading:  28%|██▊       | 17957/64000 [00:08<00:20, 2253.97it/s]Loading:  28%|██▊       | 18183/64000 [00:08<00:20, 2252.99it/s]Loading:  29%|██▉       | 18409/64000 [00:08<00:20, 2247.83it/s]Loading:  29%|██▉       | 18635/64000 [00:08<00:20, 2250.81it/s]Loading:  29%|██▉       | 18861/64000 [00:08<00:20, 2250.49it/s]Loading:  30%|██▉       | 19087/64000 [00:08<00:19, 2249.57it/s]Loading:  30%|███       | 19314/64000 [00:08<00:19, 2254.15it/s]Loading:  31%|███       | 19540/64000 [00:08<00:19, 2251.21it/s]Loading:  31%|███       | 19766/64000 [00:09<00:19, 2252.08it/s]Loading:  31%|███       | 19992/64000 [00:09<00:19, 2252.18it/s]Loading:  32%|███▏      | 20218/64000 [00:09<00:19, 2254.09it/s]Loading:  32%|███▏      | 20444/64000 [00:09<00:19, 2254.35it/s]Loading:  32%|███▏      | 20670/64000 [00:09<00:19, 2248.14it/s]Loading:  33%|███▎      | 20896/64000 [00:09<00:19, 2249.23it/s]Loading:  33%|███▎      | 21121/64000 [00:09<00:19, 2245.64it/s]Loading:  33%|███▎      | 21347/64000 [00:09<00:18, 2248.93it/s]Loading:  34%|███▎      | 21573/64000 [00:09<00:18, 2250.21it/s]Loading:  34%|███▍      | 21800/64000 [00:09<00:18, 2253.68it/s]Loading:  34%|███▍      | 22026/64000 [00:10<00:18, 2255.09it/s]Loading:  35%|███▍      | 22252/64000 [00:10<00:29, 1411.51it/s]Loading:  35%|███▌      | 22478/64000 [00:10<00:26, 1590.54it/s]Loading:  35%|███▌      | 22704/64000 [00:10<00:23, 1744.00it/s]Loading:  36%|███▌      | 22928/64000 [00:10<00:22, 1866.57it/s]Loading:  36%|███▌      | 23153/64000 [00:10<00:20, 1965.43it/s]Loading:  37%|███▋      | 23378/64000 [00:10<00:19, 2041.66it/s]Loading:  37%|███▋      | 23604/64000 [00:10<00:19, 2102.35it/s]Loading:  37%|███▋      | 23829/64000 [00:11<00:18, 2143.24it/s]Loading:  38%|███▊      | 24055/64000 [00:11<00:18, 2177.03it/s]Loading:  38%|███▊      | 24281/64000 [00:11<00:18, 2200.10it/s]Loading:  38%|███▊      | 24505/64000 [00:11<00:17, 2210.85it/s]Loading:  39%|███▊      | 24730/64000 [00:11<00:17, 2221.31it/s]Loading:  39%|███▉      | 24954/64000 [00:11<00:17, 2223.82it/s]Loading:  39%|███▉      | 25179/64000 [00:11<00:17, 2231.40it/s]Loading:  40%|███▉      | 25405/64000 [00:11<00:17, 2237.38it/s]Loading:  40%|████      | 25630/64000 [00:11<00:17, 2235.39it/s]Loading:  40%|████      | 25855/64000 [00:11<00:17, 2238.40it/s]Loading:  41%|████      | 26080/64000 [00:12<00:16, 2240.59it/s]Loading:  41%|████      | 26306/64000 [00:12<00:16, 2245.17it/s]Loading:  41%|████▏     | 26532/64000 [00:12<00:16, 2248.55it/s]Loading:  42%|████▏     | 26757/64000 [00:12<00:16, 2245.47it/s]Loading:  42%|████▏     | 26983/64000 [00:12<00:16, 2249.17it/s]Loading:  43%|████▎     | 27208/64000 [00:12<00:16, 2248.70it/s]Loading:  43%|████▎     | 27435/64000 [00:12<00:16, 2253.60it/s]Loading:  43%|████▎     | 27661/64000 [00:12<00:16, 2254.98it/s]Loading:  44%|████▎     | 27887/64000 [00:12<00:16, 2253.42it/s]Loading:  44%|████▍     | 28113/64000 [00:12<00:15, 2253.20it/s]Loading:  44%|████▍     | 28339/64000 [00:13<00:15, 2251.11it/s]Loading:  45%|████▍     | 28566/64000 [00:13<00:15, 2254.04it/s]Loading:  45%|████▍     | 28792/64000 [00:13<00:15, 2251.20it/s]Loading:  45%|████▌     | 29018/64000 [00:13<00:15, 2250.77it/s]Loading:  46%|████▌     | 29244/64000 [00:13<00:15, 2252.96it/s]Loading:  46%|████▌     | 29470/64000 [00:13<00:15, 2249.58it/s]Loading:  46%|████▋     | 29696/64000 [00:13<00:15, 2252.20it/s]Loading:  47%|████▋     | 29922/64000 [00:13<00:15, 2246.94it/s]Loading:  47%|████▋     | 30148/64000 [00:13<00:15, 2249.37it/s]Loading:  47%|████▋     | 30374/64000 [00:13<00:14, 2252.40it/s]Loading:  48%|████▊     | 30600/64000 [00:14<00:14, 2251.45it/s]Loading:  48%|████▊     | 30828/64000 [00:14<00:14, 2257.13it/s]Loading:  49%|████▊     | 31054/64000 [00:14<00:14, 2253.03it/s]Loading:  49%|████▉     | 31281/64000 [00:14<00:14, 2255.88it/s]Loading:  49%|████▉     | 31507/64000 [00:14<00:14, 2254.60it/s]Loading:  50%|████▉     | 31733/64000 [00:14<00:14, 2251.80it/s]Loading:  50%|████▉     | 31959/64000 [00:14<00:14, 2251.42it/s]Loading:  50%|█████     | 32185/64000 [00:14<00:14, 2247.25it/s]Loading:  51%|█████     | 32411/64000 [00:14<00:14, 2249.07it/s]Loading:  51%|█████     | 32638/64000 [00:15<00:13, 2252.89it/s]Loading:  51%|█████▏    | 32864/64000 [00:15<00:13, 2249.61it/s]Loading:  52%|█████▏    | 33090/64000 [00:15<00:13, 2252.10it/s]Loading:  52%|█████▏    | 33316/64000 [00:15<00:13, 2251.60it/s]Loading:  52%|█████▏    | 33542/64000 [00:15<00:13, 2253.18it/s]Loading:  53%|█████▎    | 33768/64000 [00:15<00:13, 2249.89it/s]Loading:  53%|█████▎    | 33994/64000 [00:15<00:13, 2250.41it/s]Loading:  53%|█████▎    | 34220/64000 [00:15<00:13, 2252.52it/s]Loading:  54%|█████▍    | 34446/64000 [00:15<00:13, 2252.51it/s]Loading:  54%|█████▍    | 34672/64000 [00:15<00:13, 2251.85it/s]Loading:  55%|█████▍    | 34898/64000 [00:16<00:22, 1305.18it/s]Loading:  55%|█████▍    | 35122/64000 [00:16<00:19, 1489.90it/s]Loading:  55%|█████▌    | 35347/64000 [00:16<00:17, 1657.50it/s]Loading:  56%|█████▌    | 35572/64000 [00:16<00:15, 1798.50it/s]Loading:  56%|█████▌    | 35798/64000 [00:16<00:14, 1914.90it/s]Loading:  56%|█████▋    | 36024/64000 [00:16<00:13, 2005.16it/s]Loading:  57%|█████▋    | 36250/64000 [00:16<00:13, 2074.30it/s]Loading:  57%|█████▋    | 36477/64000 [00:16<00:12, 2127.25it/s]Loading:  57%|█████▋    | 36703/64000 [00:17<00:12, 2163.25it/s]Loading:  58%|█████▊    | 36929/64000 [00:17<00:12, 2190.39it/s]Loading:  58%|█████▊    | 37154/64000 [00:17<00:12, 2206.73it/s]Loading:  58%|█████▊    | 37380/64000 [00:17<00:11, 2220.32it/s]Loading:  59%|█████▉    | 37606/64000 [00:17<00:11, 2231.42it/s]Loading:  59%|█████▉    | 37831/64000 [00:17<00:11, 2234.98it/s]Loading:  59%|█████▉    | 38056/64000 [00:17<00:11, 2237.97it/s]Loading:  60%|█████▉    | 38282/64000 [00:17<00:11, 2242.21it/s]Loading:  60%|██████    | 38509/64000 [00:17<00:11, 2248.23it/s]Loading:  61%|██████    | 38735/64000 [00:17<00:11, 2250.09it/s]Loading:  61%|██████    | 38961/64000 [00:18<00:11, 2248.06it/s]Loading:  61%|██████    | 39188/64000 [00:18<00:11, 2253.38it/s]Loading:  62%|██████▏   | 39414/64000 [00:18<00:10, 2252.42it/s]Loading:  62%|██████▏   | 39640/64000 [00:18<00:10, 2253.42it/s]Loading:  62%|██████▏   | 39866/64000 [00:18<00:10, 2250.98it/s]Loading:  63%|██████▎   | 40092/64000 [00:18<00:10, 2251.43it/s]Loading:  63%|██████▎   | 40318/64000 [00:18<00:10, 2252.67it/s]Loading:  63%|██████▎   | 40544/64000 [00:18<00:10, 2248.68it/s]Loading:  64%|██████▎   | 40771/64000 [00:18<00:10, 2253.39it/s]Loading:  64%|██████▍   | 40997/64000 [00:18<00:10, 2252.51it/s]Loading:  64%|██████▍   | 41224/64000 [00:19<00:10, 2255.05it/s]Loading:  65%|██████▍   | 41450/64000 [00:19<00:09, 2255.72it/s]Loading:  65%|██████▌   | 41676/64000 [00:19<00:09, 2251.89it/s]Loading:  65%|██████▌   | 41902/64000 [00:19<00:09, 2254.20it/s]Loading:  66%|██████▌   | 42128/64000 [00:19<00:09, 2250.24it/s]Loading:  66%|██████▌   | 42354/64000 [00:19<00:09, 2250.79it/s]Loading:  67%|██████▋   | 42580/64000 [00:19<00:09, 2249.84it/s]Loading:  67%|██████▋   | 42805/64000 [00:19<00:09, 2243.46it/s]Loading:  67%|██████▋   | 43030/64000 [00:19<00:09, 2242.06it/s]Loading:  68%|██████▊   | 43255/64000 [00:19<00:09, 2237.45it/s]Loading:  68%|██████▊   | 43479/64000 [00:20<00:09, 2236.52it/s]Loading:  68%|██████▊   | 43704/64000 [00:20<00:09, 2237.88it/s]Loading:  69%|██████▊   | 43929/64000 [00:20<00:08, 2239.10it/s]Loading:  69%|██████▉   | 44155/64000 [00:20<00:08, 2244.18it/s]Loading:  69%|██████▉   | 44380/64000 [00:20<00:08, 2243.92it/s]Loading:  70%|██████▉   | 44606/64000 [00:20<00:08, 2245.95it/s]Loading:  70%|███████   | 44831/64000 [00:20<00:08, 2243.71it/s]Loading:  70%|███████   | 45057/64000 [00:20<00:08, 2246.00it/s]Loading:  71%|███████   | 45283/64000 [00:20<00:08, 2249.09it/s]Loading:  71%|███████   | 45508/64000 [00:20<00:08, 2248.75it/s]Loading:  71%|███████▏  | 45734/64000 [00:21<00:08, 2251.32it/s]Loading:  72%|███████▏  | 45960/64000 [00:21<00:08, 2248.02it/s]Loading:  72%|███████▏  | 46185/64000 [00:21<00:07, 2248.17it/s]Loading:  73%|███████▎  | 46411/64000 [00:21<00:07, 2250.31it/s]Loading:  73%|███████▎  | 46637/64000 [00:21<00:07, 2249.42it/s]Loading:  73%|███████▎  | 46862/64000 [00:21<00:07, 2248.12it/s]Loading:  74%|███████▎  | 47087/64000 [00:21<00:07, 2244.50it/s]Loading:  74%|███████▍  | 47312/64000 [00:21<00:07, 2245.72it/s]Loading:  74%|███████▍  | 47538/64000 [00:21<00:07, 2247.65it/s]Loading:  75%|███████▍  | 47763/64000 [00:21<00:07, 2245.39it/s]Loading:  75%|███████▍  | 47989/64000 [00:22<00:07, 2248.48it/s]Loading:  75%|███████▌  | 48214/64000 [00:22<00:07, 2247.85it/s]Loading:  76%|███████▌  | 48440/64000 [00:22<00:06, 2249.16it/s]Loading:  76%|███████▌  | 48666/64000 [00:22<00:06, 2251.35it/s]Loading:  76%|███████▋  | 48892/64000 [00:22<00:06, 2250.46it/s]Loading:  77%|███████▋  | 49118/64000 [00:22<00:06, 2251.41it/s]Loading:  77%|███████▋  | 49344/64000 [00:22<00:06, 2246.76it/s]Loading:  77%|███████▋  | 49570/64000 [00:22<00:06, 2250.20it/s]Loading:  78%|███████▊  | 49796/64000 [00:22<00:06, 2246.20it/s]Loading:  78%|███████▊  | 50021/64000 [00:22<00:06, 2247.22it/s]Loading:  79%|███████▊  | 50246/64000 [00:23<00:06, 2246.86it/s]Loading:  79%|███████▉  | 50471/64000 [00:23<00:06, 2247.21it/s]Loading:  79%|███████▉  | 50696/64000 [00:23<00:05, 2247.21it/s]Loading:  80%|███████▉  | 50921/64000 [00:23<00:05, 2240.98it/s]Loading:  80%|███████▉  | 51146/64000 [00:23<00:10, 1195.90it/s]Loading:  80%|████████  | 51369/64000 [00:23<00:09, 1387.33it/s]Loading:  81%|████████  | 51592/64000 [00:23<00:07, 1563.53it/s]Loading:  81%|████████  | 51817/64000 [00:24<00:07, 1720.40it/s]Loading:  81%|████████▏ | 52041/64000 [00:24<00:06, 1848.83it/s]Loading:  82%|████████▏ | 52266/64000 [00:24<00:06, 1952.14it/s]Loading:  82%|████████▏ | 52491/64000 [00:24<00:05, 2032.90it/s]Loading:  82%|████████▏ | 52715/64000 [00:24<00:05, 2088.76it/s]Loading:  83%|████████▎ | 52941/64000 [00:24<00:05, 2135.30it/s]Loading:  83%|████████▎ | 53166/64000 [00:24<00:05, 2165.85it/s]Loading:  83%|████████▎ | 53391/64000 [00:24<00:04, 2188.95it/s]Loading:  84%|████████▍ | 53617/64000 [00:24<00:04, 2207.25it/s]Loading:  84%|████████▍ | 53841/64000 [00:24<00:04, 2214.02it/s]Loading:  84%|████████▍ | 54065/64000 [00:25<00:04, 2221.58it/s]Loading:  85%|████████▍ | 54290/64000 [00:25<00:04, 2229.68it/s]Loading:  85%|████████▌ | 54516/64000 [00:25<00:04, 2236.88it/s]Loading:  86%|████████▌ | 54741/64000 [00:25<00:04, 2240.76it/s]Loading:  86%|████████▌ | 54966/64000 [00:25<00:04, 2238.91it/s]Loading:  86%|████████▌ | 55192/64000 [00:25<00:03, 2242.68it/s]Loading:  87%|████████▋ | 55417/64000 [00:25<00:03, 2240.43it/s]Loading:  87%|████████▋ | 55643/64000 [00:25<00:03, 2244.74it/s]Loading:  87%|████████▋ | 55868/64000 [00:25<00:03, 2245.14it/s]Loading:  88%|████████▊ | 56093/64000 [00:25<00:03, 2241.02it/s]Loading:  88%|████████▊ | 56318/64000 [00:26<00:03, 2242.87it/s]Loading:  88%|████████▊ | 56543/64000 [00:26<00:03, 2241.57it/s]Loading:  89%|████████▊ | 56769/64000 [00:26<00:03, 2244.18it/s]Loading:  89%|████████▉ | 56995/64000 [00:26<00:03, 2246.90it/s]Loading:  89%|████████▉ | 57220/64000 [00:26<00:03, 2245.51it/s]Loading:  90%|████████▉ | 57445/64000 [00:26<00:02, 2245.64it/s]Loading:  90%|█████████ | 57670/64000 [00:26<00:02, 2243.86it/s]Loading:  90%|█████████ | 57895/64000 [00:26<00:02, 2242.99it/s]Loading:  91%|█████████ | 58120/64000 [00:26<00:02, 2240.07it/s]Loading:  91%|█████████ | 58345/64000 [00:26<00:02, 2242.53it/s]Loading:  92%|█████████▏| 58570/64000 [00:27<00:02, 2242.54it/s]Loading:  92%|█████████▏| 58795/64000 [00:27<00:02, 2242.04it/s]Loading:  92%|█████████▏| 59021/64000 [00:27<00:02, 2246.43it/s]Loading:  93%|█████████▎| 59246/64000 [00:27<00:02, 2246.21it/s]Loading:  93%|█████████▎| 59472/64000 [00:27<00:02, 2249.20it/s]Loading:  93%|█████████▎| 59698/64000 [00:27<00:01, 2251.23it/s]Loading:  94%|█████████▎| 59924/64000 [00:27<00:01, 2246.44it/s]Loading:  94%|█████████▍| 60149/64000 [00:27<00:01, 2244.16it/s]Loading:  94%|█████████▍| 60374/64000 [00:27<00:01, 2240.31it/s]Loading:  95%|█████████▍| 60599/64000 [00:27<00:01, 2239.79it/s]Loading:  95%|█████████▌| 60825/64000 [00:28<00:01, 2243.15it/s]Loading:  95%|█████████▌| 61050/64000 [00:28<00:01, 2241.03it/s]Loading:  96%|█████████▌| 61276/64000 [00:28<00:01, 2245.49it/s]Loading:  96%|█████████▌| 61501/64000 [00:28<00:01, 2241.36it/s]Loading:  96%|█████████▋| 61726/64000 [00:28<00:01, 2241.94it/s]Loading:  97%|█████████▋| 61951/64000 [00:28<00:00, 2243.59it/s]Loading:  97%|█████████▋| 62176/64000 [00:28<00:00, 2241.92it/s]Loading:  98%|█████████▊| 62401/64000 [00:28<00:00, 2242.24it/s]Loading:  98%|█████████▊| 62626/64000 [00:28<00:00, 2240.98it/s]Loading:  98%|█████████▊| 62852/64000 [00:28<00:00, 2244.04it/s]Loading:  99%|█████████▊| 63077/64000 [00:29<00:00, 2244.81it/s]Loading:  99%|█████████▉| 63302/64000 [00:29<00:00, 2243.07it/s]Loading:  99%|█████████▉| 63528/64000 [00:29<00:00, 2246.14it/s]Loading: 100%|█████████▉| 63753/64000 [00:29<00:00, 2243.07it/s]Loading: 100%|█████████▉| 63978/64000 [00:29<00:00, 2242.60it/s]Loading: 100%|██████████| 64000/64000 [00:29<00:00, 2169.66it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 227/49018 [00:00<00:21, 2264.22it/s]Loading:   1%|          | 458/49018 [00:00<00:21, 2290.23it/s]Loading:   1%|▏         | 690/49018 [00:00<00:21, 2300.06it/s]Loading:   2%|▏         | 921/49018 [00:00<00:20, 2303.86it/s]Loading:   2%|▏         | 1154/49018 [00:00<00:20, 2310.31it/s]Loading:   3%|▎         | 1386/49018 [00:00<00:20, 2310.24it/s]Loading:   3%|▎         | 1620/49018 [00:00<00:20, 2318.09it/s]Loading:   4%|▍         | 1854/49018 [00:00<00:20, 2322.99it/s]Loading:   4%|▍         | 2087/49018 [00:00<00:20, 2320.08it/s]Loading:   5%|▍         | 2320/49018 [00:01<00:20, 2322.53it/s]Loading:   5%|▌         | 2553/49018 [00:01<00:20, 2320.30it/s]Loading:   6%|▌         | 2786/49018 [00:01<00:19, 2320.13it/s]Loading:   6%|▌         | 3019/49018 [00:01<00:19, 2317.57it/s]Loading:   7%|▋         | 3251/49018 [00:01<00:19, 2313.98it/s]Loading:   7%|▋         | 3483/49018 [00:01<00:19, 2314.39it/s]Loading:   8%|▊         | 3715/49018 [00:01<00:19, 2314.57it/s]Loading:   8%|▊         | 3947/49018 [00:01<00:19, 2315.92it/s]Loading:   9%|▊         | 4179/49018 [00:01<00:19, 2311.67it/s]Loading:   9%|▉         | 4411/49018 [00:01<00:19, 2312.52it/s]Loading:   9%|▉         | 4643/49018 [00:02<00:19, 2312.23it/s]Loading:  10%|▉         | 4875/49018 [00:02<00:19, 2312.60it/s]Loading:  10%|█         | 5107/49018 [00:02<00:18, 2313.35it/s]Loading:  11%|█         | 5339/49018 [00:02<00:18, 2313.73it/s]Loading:  11%|█▏        | 5572/49018 [00:02<00:18, 2317.98it/s]Loading:  12%|█▏        | 5804/49018 [00:02<00:18, 2315.46it/s]Loading:  12%|█▏        | 6037/49018 [00:02<00:18, 2318.68it/s]Loading:  13%|█▎        | 6270/49018 [00:02<00:18, 2319.17it/s]Loading:  13%|█▎        | 6502/49018 [00:02<00:18, 2318.45it/s]Loading:  14%|█▎        | 6735/49018 [00:02<00:18, 2321.16it/s]Loading:  14%|█▍        | 6968/49018 [00:03<00:18, 2318.46it/s]Loading:  15%|█▍        | 7201/49018 [00:03<00:18, 2318.95it/s]Loading:  15%|█▌        | 7434/49018 [00:03<00:17, 2320.33it/s]Loading:  16%|█▌        | 7667/49018 [00:03<00:17, 2315.79it/s]Loading:  16%|█▌        | 7899/49018 [00:03<00:17, 2311.90it/s]Loading:  17%|█▋        | 8131/49018 [00:03<00:17, 2306.34it/s]Loading:  17%|█▋        | 8362/49018 [00:03<00:17, 2305.19it/s]Loading:  18%|█▊        | 8593/49018 [00:03<00:17, 2299.41it/s]Loading:  18%|█▊        | 8823/49018 [00:03<00:17, 2299.34it/s]Loading:  18%|█▊        | 9054/49018 [00:03<00:17, 2300.70it/s]Loading:  19%|█▉        | 9285/49018 [00:04<00:17, 2298.31it/s]Loading:  19%|█▉        | 9517/49018 [00:04<00:17, 2302.68it/s]Loading:  20%|█▉        | 9748/49018 [00:04<00:17, 2300.19it/s]Loading:  20%|██        | 9979/49018 [00:04<00:16, 2298.84it/s]Loading:  21%|██        | 10209/49018 [00:04<00:16, 2298.87it/s]Loading:  21%|██▏       | 10439/49018 [00:04<00:16, 2293.27it/s]Loading:  22%|██▏       | 10670/49018 [00:04<00:16, 2295.68it/s]Loading:  22%|██▏       | 10900/49018 [00:04<00:16, 2291.80it/s]Loading:  23%|██▎       | 11130/49018 [00:04<00:16, 2293.44it/s]Loading:  23%|██▎       | 11360/49018 [00:04<00:16, 2287.98it/s]Loading:  24%|██▎       | 11590/49018 [00:05<00:16, 2288.66it/s]Loading:  24%|██▍       | 11819/49018 [00:05<00:16, 2287.00it/s]Loading:  25%|██▍       | 12048/49018 [00:05<00:16, 2281.74it/s]Loading:  25%|██▌       | 12277/49018 [00:05<00:16, 2284.08it/s]Loading:  26%|██▌       | 12506/49018 [00:05<00:15, 2283.23it/s]Loading:  26%|██▌       | 12736/49018 [00:05<00:15, 2286.75it/s]Loading:  26%|██▋       | 12966/49018 [00:05<00:15, 2289.51it/s]Loading:  27%|██▋       | 13195/49018 [00:05<00:15, 2287.52it/s]Loading:  27%|██▋       | 13424/49018 [00:05<00:15, 2278.86it/s]Loading:  28%|██▊       | 13652/49018 [00:05<00:15, 2265.79it/s]Loading:  28%|██▊       | 13879/49018 [00:06<00:15, 2258.24it/s]Loading:  29%|██▉       | 14105/49018 [00:06<00:15, 2247.66it/s]Loading:  29%|██▉       | 14330/49018 [00:06<00:15, 2245.98it/s]Loading:  30%|██▉       | 14555/49018 [00:06<00:15, 2245.22it/s]Loading:  30%|███       | 14780/49018 [00:06<00:15, 2236.52it/s]Loading:  31%|███       | 15005/49018 [00:06<00:15, 2238.42it/s]Loading:  31%|███       | 15229/49018 [00:06<00:15, 2237.56it/s]Loading:  32%|███▏      | 15453/49018 [00:06<00:15, 2236.39it/s]Loading:  32%|███▏      | 15678/49018 [00:06<00:14, 2238.72it/s]Loading:  32%|███▏      | 15902/49018 [00:06<00:14, 2230.71it/s]Loading:  33%|███▎      | 16126/49018 [00:07<00:14, 2233.42it/s]Loading:  33%|███▎      | 16350/49018 [00:07<00:14, 2231.51it/s]Loading:  34%|███▍      | 16574/49018 [00:07<00:14, 2230.98it/s]Loading:  34%|███▍      | 16798/49018 [00:07<00:14, 2230.95it/s]Loading:  35%|███▍      | 17022/49018 [00:07<00:14, 2227.19it/s]Loading:  35%|███▌      | 17247/49018 [00:07<00:14, 2231.43it/s]Loading:  36%|███▌      | 17471/49018 [00:07<00:14, 2229.81it/s]Loading:  36%|███▌      | 17694/49018 [00:07<00:14, 2225.13it/s]Loading:  37%|███▋      | 17917/49018 [00:07<00:13, 2226.30it/s]Loading:  37%|███▋      | 18140/49018 [00:08<00:29, 1047.89it/s]Loading:  37%|███▋      | 18362/49018 [00:08<00:24, 1243.28it/s]Loading:  38%|███▊      | 18583/49018 [00:08<00:21, 1429.54it/s]Loading:  38%|███▊      | 18805/49018 [00:08<00:18, 1599.00it/s]Loading:  39%|███▉      | 19027/49018 [00:08<00:17, 1743.79it/s]Loading:  39%|███▉      | 19249/49018 [00:08<00:15, 1861.56it/s]Loading:  40%|███▉      | 19473/49018 [00:08<00:15, 1959.57it/s]Loading:  40%|████      | 19696/49018 [00:09<00:14, 2031.37it/s]Loading:  41%|████      | 19919/49018 [00:09<00:13, 2085.67it/s]Loading:  41%|████      | 20142/49018 [00:09<00:13, 2126.57it/s]Loading:  42%|████▏     | 20363/49018 [00:09<00:13, 2146.08it/s]Loading:  42%|████▏     | 20586/49018 [00:09<00:13, 2169.68it/s]Loading:  42%|████▏     | 20808/49018 [00:09<00:12, 2182.57it/s]Loading:  43%|████▎     | 21031/49018 [00:09<00:12, 2194.36it/s]Loading:  43%|████▎     | 21256/49018 [00:09<00:12, 2209.52it/s]Loading:  44%|████▍     | 21479/49018 [00:09<00:12, 2210.90it/s]Loading:  44%|████▍     | 21702/49018 [00:09<00:12, 2215.88it/s]Loading:  45%|████▍     | 21925/49018 [00:10<00:12, 2213.25it/s]Loading:  45%|████▌     | 22148/49018 [00:10<00:12, 2215.59it/s]Loading:  46%|████▌     | 22371/49018 [00:10<00:12, 2218.77it/s]Loading:  46%|████▌     | 22594/49018 [00:10<00:11, 2213.26it/s]Loading:  47%|████▋     | 22817/49018 [00:10<00:11, 2216.78it/s]Loading:  47%|████▋     | 23039/49018 [00:10<00:11, 2216.20it/s]Loading:  47%|████▋     | 23263/49018 [00:10<00:11, 2222.91it/s]Loading:  48%|████▊     | 23486/49018 [00:10<00:11, 2217.90it/s]Loading:  48%|████▊     | 23708/49018 [00:10<00:11, 2214.31it/s]Loading:  49%|████▉     | 23931/49018 [00:10<00:11, 2218.44it/s]Loading:  49%|████▉     | 24153/49018 [00:11<00:11, 2215.67it/s]Loading:  50%|████▉     | 24377/49018 [00:11<00:11, 2221.70it/s]Loading:  50%|█████     | 24601/49018 [00:11<00:10, 2225.98it/s]Loading:  51%|█████     | 24824/49018 [00:11<00:10, 2225.80it/s]Loading:  51%|█████     | 25048/49018 [00:11<00:10, 2229.45it/s]Loading:  52%|█████▏    | 25271/49018 [00:11<00:10, 2228.18it/s]Loading:  52%|█████▏    | 25495/49018 [00:11<00:10, 2230.92it/s]Loading:  52%|█████▏    | 25719/49018 [00:11<00:10, 2227.19it/s]Loading:  53%|█████▎    | 25944/49018 [00:11<00:10, 2232.35it/s]Loading:  53%|█████▎    | 26168/49018 [00:11<00:10, 2228.20it/s]Loading:  54%|█████▍    | 26391/49018 [00:12<00:10, 2217.42it/s]Loading:  54%|█████▍    | 26616/49018 [00:12<00:10, 2227.02it/s]Loading:  55%|█████▍    | 26839/49018 [00:12<00:09, 2222.35it/s]Loading:  55%|█████▌    | 27064/49018 [00:12<00:09, 2229.84it/s]Loading:  56%|█████▌    | 27289/49018 [00:12<00:09, 2232.93it/s]Loading:  56%|█████▌    | 27513/49018 [00:12<00:09, 2229.12it/s]Loading:  57%|█████▋    | 27737/49018 [00:12<00:09, 2230.84it/s]Loading:  57%|█████▋    | 27961/49018 [00:12<00:09, 2227.25it/s]Loading:  57%|█████▋    | 28185/49018 [00:12<00:09, 2229.79it/s]Loading:  58%|█████▊    | 28408/49018 [00:12<00:09, 2229.50it/s]Loading:  58%|█████▊    | 28631/49018 [00:13<00:09, 2221.33it/s]Loading:  59%|█████▉    | 28854/49018 [00:13<00:09, 2222.81it/s]Loading:  59%|█████▉    | 29077/49018 [00:13<00:08, 2218.45it/s]Loading:  60%|█████▉    | 29300/49018 [00:13<00:08, 2219.88it/s]Loading:  60%|██████    | 29523/49018 [00:13<00:08, 2220.39it/s]Loading:  61%|██████    | 29746/49018 [00:13<00:08, 2215.00it/s]Loading:  61%|██████    | 29968/49018 [00:13<00:08, 2214.05it/s]Loading:  62%|██████▏   | 30190/49018 [00:13<00:08, 2205.69it/s]Loading:  62%|██████▏   | 30411/49018 [00:13<00:08, 2205.40it/s]Loading:  62%|██████▏   | 30633/49018 [00:13<00:08, 2208.35it/s]Loading:  63%|██████▎   | 30855/49018 [00:14<00:08, 2210.71it/s]Loading:  63%|██████▎   | 31077/49018 [00:14<00:08, 2213.03it/s]Loading:  64%|██████▍   | 31299/49018 [00:14<00:08, 2208.71it/s]Loading:  64%|██████▍   | 31521/49018 [00:14<00:07, 2210.63it/s]Loading:  65%|██████▍   | 31743/49018 [00:14<00:07, 2212.22it/s]Loading:  65%|██████▌   | 31965/49018 [00:14<00:07, 2208.64it/s]Loading:  66%|██████▌   | 32187/49018 [00:14<00:07, 2210.76it/s]Loading:  66%|██████▌   | 32409/49018 [00:14<00:07, 2205.60it/s]Loading:  67%|██████▋   | 32631/49018 [00:14<00:07, 2208.21it/s]Loading:  67%|██████▋   | 32853/49018 [00:14<00:07, 2209.32it/s]Loading:  67%|██████▋   | 33074/49018 [00:15<00:07, 2207.47it/s]Loading:  68%|██████▊   | 33296/49018 [00:15<00:07, 2210.96it/s]Loading:  68%|██████▊   | 33518/49018 [00:15<00:07, 2207.88it/s]Loading:  69%|██████▉   | 33740/49018 [00:15<00:06, 2208.97it/s]Loading:  69%|██████▉   | 33962/49018 [00:15<00:06, 2211.12it/s]Loading:  70%|██████▉   | 34184/49018 [00:15<00:06, 2207.48it/s]Loading:  70%|███████   | 34407/49018 [00:15<00:06, 2211.31it/s]Loading:  71%|███████   | 34629/49018 [00:15<00:06, 2208.63it/s]Loading:  71%|███████   | 34852/49018 [00:15<00:06, 2214.56it/s]Loading:  72%|███████▏  | 35074/49018 [00:15<00:06, 2212.26it/s]Loading:  72%|███████▏  | 35296/49018 [00:16<00:06, 2207.43it/s]Loading:  72%|███████▏  | 35517/49018 [00:16<00:06, 2205.38it/s]Loading:  73%|███████▎  | 35738/49018 [00:16<00:06, 2205.27it/s]Loading:  73%|███████▎  | 35960/49018 [00:16<00:05, 2208.92it/s]Loading:  74%|███████▍  | 36181/49018 [00:16<00:05, 2209.12it/s]Loading:  74%|███████▍  | 36402/49018 [00:16<00:05, 2205.64it/s]Loading:  75%|███████▍  | 36624/49018 [00:16<00:05, 2209.17it/s]Loading:  75%|███████▌  | 36845/49018 [00:16<00:05, 2205.86it/s]Loading:  76%|███████▌  | 37066/49018 [00:16<00:05, 2206.46it/s]Loading:  76%|███████▌  | 37288/49018 [00:16<00:05, 2207.57it/s]Loading:  77%|███████▋  | 37509/49018 [00:17<00:05, 2205.13it/s]Loading:  77%|███████▋  | 37731/49018 [00:17<00:05, 2208.57it/s]Loading:  77%|███████▋  | 37952/49018 [00:17<00:05, 2207.20it/s]Loading:  78%|███████▊  | 38174/49018 [00:17<00:04, 2210.42it/s]Loading:  78%|███████▊  | 38396/49018 [00:17<00:04, 2208.17it/s]Loading:  79%|███████▉  | 38617/49018 [00:17<00:04, 2203.01it/s]Loading:  79%|███████▉  | 38839/49018 [00:17<00:04, 2207.07it/s]Loading:  80%|███████▉  | 39062/49018 [00:17<00:04, 2211.64it/s]Loading:  80%|████████  | 39284/49018 [00:17<00:04, 2213.96it/s]Loading:  81%|████████  | 39506/49018 [00:17<00:04, 2213.81it/s]Loading:  81%|████████  | 39728/49018 [00:18<00:04, 2207.98it/s]Loading:  81%|████████▏ | 39949/49018 [00:18<00:04, 2207.50it/s]Loading:  82%|████████▏ | 40170/49018 [00:18<00:04, 2205.47it/s]Loading:  82%|████████▏ | 40392/49018 [00:18<00:03, 2208.74it/s]Loading:  83%|████████▎ | 40613/49018 [00:18<00:03, 2205.26it/s]Loading:  83%|████████▎ | 40835/49018 [00:18<00:03, 2207.86it/s]Loading:  84%|████████▍ | 41057/49018 [00:18<00:03, 2209.05it/s]Loading:  84%|████████▍ | 41278/49018 [00:18<00:03, 2203.76it/s]Loading:  85%|████████▍ | 41499/49018 [00:18<00:03, 2204.43it/s]Loading:  85%|████████▌ | 41720/49018 [00:18<00:03, 2204.14it/s]Loading:  86%|████████▌ | 41942/49018 [00:19<00:03, 2206.41it/s]Loading:  86%|████████▌ | 42164/49018 [00:19<00:03, 2208.55it/s]Loading:  86%|████████▋ | 42385/49018 [00:19<00:03, 2205.74it/s]Loading:  87%|████████▋ | 42606/49018 [00:19<00:02, 2206.63it/s]Loading:  87%|████████▋ | 42827/49018 [00:19<00:02, 2202.69it/s]Loading:  88%|████████▊ | 43049/49018 [00:19<00:02, 2207.71it/s]Loading:  88%|████████▊ | 43272/49018 [00:19<00:02, 2212.11it/s]Loading:  89%|████████▊ | 43494/49018 [00:19<00:02, 2210.52it/s]Loading:  89%|████████▉ | 43716/49018 [00:19<00:02, 2209.55it/s]Loading:  90%|████████▉ | 43937/49018 [00:19<00:02, 2207.86it/s]Loading:  90%|█████████ | 44159/49018 [00:20<00:02, 2209.06it/s]Loading:  91%|█████████ | 44382/49018 [00:20<00:02, 2213.40it/s]Loading:  91%|█████████ | 44604/49018 [00:20<00:01, 2211.30it/s]Loading:  91%|█████████▏| 44826/49018 [00:20<00:01, 2211.76it/s]Loading:  92%|█████████▏| 45048/49018 [00:20<00:01, 2211.24it/s]Loading:  92%|█████████▏| 45270/49018 [00:20<00:01, 2211.31it/s]Loading:  93%|█████████▎| 45492/49018 [00:20<00:01, 2211.74it/s]Loading:  93%|█████████▎| 45714/49018 [00:21<00:03, 940.01it/s] Loading:  94%|█████████▎| 45935/49018 [00:21<00:02, 1134.94it/s]Loading:  94%|█████████▍| 46156/49018 [00:21<00:02, 1328.31it/s]Loading:  95%|█████████▍| 46377/49018 [00:21<00:01, 1507.86it/s]Loading:  95%|█████████▌| 46599/49018 [00:21<00:01, 1667.74it/s]Loading:  96%|█████████▌| 46820/49018 [00:21<00:01, 1798.65it/s]Loading:  96%|█████████▌| 47043/49018 [00:21<00:01, 1908.40it/s]Loading:  96%|█████████▋| 47266/49018 [00:21<00:00, 1994.12it/s]Loading:  97%|█████████▋| 47488/49018 [00:22<00:00, 2056.46it/s]Loading:  97%|█████████▋| 47712/49018 [00:22<00:00, 2107.88it/s]Loading:  98%|█████████▊| 47933/49018 [00:22<00:00, 2136.85it/s]Loading:  98%|█████████▊| 48155/49018 [00:22<00:00, 2160.45it/s]Loading:  99%|█████████▊| 48378/49018 [00:22<00:00, 2180.20it/s]Loading:  99%|█████████▉| 48601/49018 [00:22<00:00, 2194.06it/s]Loading: 100%|█████████▉| 48825/49018 [00:22<00:00, 2205.36it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2158.28it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 228/49018 [00:00<00:21, 2273.13it/s]Loading:   1%|          | 459/49018 [00:00<00:21, 2294.39it/s]Loading:   1%|▏         | 692/49018 [00:00<00:20, 2307.03it/s]Loading:   2%|▏         | 924/49018 [00:00<00:20, 2308.23it/s]Loading:   2%|▏         | 1156/49018 [00:00<00:20, 2311.82it/s]Loading:   3%|▎         | 1388/49018 [00:00<00:20, 2308.00it/s]Loading:   3%|▎         | 1620/49018 [00:00<00:20, 2310.26it/s]Loading:   4%|▍         | 1852/49018 [00:01<00:51, 918.78it/s] Loading:   4%|▍         | 2081/49018 [00:01<00:41, 1127.57it/s]Loading:   5%|▍         | 2312/49018 [00:01<00:34, 1338.26it/s]Loading:   5%|▌         | 2542/49018 [00:01<00:30, 1533.86it/s]Loading:   6%|▌         | 2774/49018 [00:01<00:27, 1709.83it/s]Loading:   6%|▌         | 3005/49018 [00:01<00:24, 1855.24it/s]Loading:   7%|▋         | 3237/49018 [00:01<00:23, 1973.18it/s]Loading:   7%|▋         | 3469/49018 [00:01<00:22, 2065.35it/s]Loading:   8%|▊         | 3700/49018 [00:02<00:21, 2131.70it/s]Loading:   8%|▊         | 3933/49018 [00:02<00:20, 2185.68it/s]Loading:   8%|▊         | 4164/49018 [00:02<00:20, 2220.88it/s]Loading:   9%|▉         | 4398/49018 [00:02<00:19, 2252.75it/s]Loading:   9%|▉         | 4630/49018 [00:02<00:19, 2269.84it/s]Loading:  10%|▉         | 4862/49018 [00:02<00:19, 2283.67it/s]Loading:  10%|█         | 5094/49018 [00:02<00:19, 2291.81it/s]Loading:  11%|█         | 5326/49018 [00:02<00:19, 2295.90it/s]Loading:  11%|█▏        | 5557/49018 [00:02<00:18, 2296.94it/s]Loading:  12%|█▏        | 5788/49018 [00:02<00:18, 2294.44it/s]Loading:  12%|█▏        | 6019/49018 [00:03<00:18, 2298.32it/s]Loading:  13%|█▎        | 6250/49018 [00:03<00:18, 2299.39it/s]Loading:  13%|█▎        | 6481/49018 [00:03<00:18, 2296.72it/s]Loading:  14%|█▎        | 6711/49018 [00:03<00:18, 2296.28it/s]Loading:  14%|█▍        | 6941/49018 [00:03<00:18, 2291.85it/s]Loading:  15%|█▍        | 7171/49018 [00:03<00:18, 2293.89it/s]Loading:  15%|█▌        | 7401/49018 [00:03<00:18, 2288.89it/s]Loading:  16%|█▌        | 7631/49018 [00:03<00:18, 2290.52it/s]Loading:  16%|█▌        | 7862/49018 [00:03<00:17, 2293.80it/s]Loading:  17%|█▋        | 8092/49018 [00:03<00:17, 2290.02it/s]Loading:  17%|█▋        | 8323/49018 [00:04<00:17, 2294.67it/s]Loading:  17%|█▋        | 8553/49018 [00:04<00:17, 2291.71it/s]Loading:  18%|█▊        | 8783/49018 [00:04<00:17, 2293.54it/s]Loading:  18%|█▊        | 9013/49018 [00:04<00:17, 2292.62it/s]Loading:  19%|█▉        | 9243/49018 [00:04<00:17, 2293.79it/s]Loading:  19%|█▉        | 9474/49018 [00:04<00:17, 2295.80it/s]Loading:  20%|█▉        | 9704/49018 [00:04<00:17, 2294.81it/s]Loading:  20%|██        | 9935/49018 [00:04<00:17, 2297.92it/s]Loading:  21%|██        | 10165/49018 [00:04<00:16, 2293.76it/s]Loading:  21%|██        | 10395/49018 [00:04<00:16, 2294.72it/s]Loading:  22%|██▏       | 10625/49018 [00:05<00:16, 2295.89it/s]Loading:  22%|██▏       | 10855/49018 [00:05<00:16, 2292.04it/s]Loading:  23%|██▎       | 11086/49018 [00:05<00:16, 2296.52it/s]Loading:  23%|██▎       | 11316/49018 [00:05<00:16, 2297.55it/s]Loading:  24%|██▎       | 11547/49018 [00:05<00:16, 2299.77it/s]Loading:  24%|██▍       | 11777/49018 [00:05<00:16, 2294.40it/s]Loading:  24%|██▍       | 12007/49018 [00:05<00:16, 2292.52it/s]Loading:  25%|██▍       | 12237/49018 [00:05<00:16, 2290.92it/s]Loading:  25%|██▌       | 12467/49018 [00:05<00:15, 2286.08it/s]Loading:  26%|██▌       | 12697/49018 [00:05<00:15, 2289.69it/s]Loading:  26%|██▋       | 12926/49018 [00:06<00:15, 2288.64it/s]Loading:  27%|██▋       | 13157/49018 [00:06<00:15, 2292.79it/s]Loading:  27%|██▋       | 13387/49018 [00:06<00:15, 2292.20it/s]Loading:  28%|██▊       | 13617/49018 [00:06<00:15, 2288.25it/s]Loading:  28%|██▊       | 13847/49018 [00:06<00:15, 2289.24it/s]Loading:  29%|██▊       | 14076/49018 [00:06<00:15, 2287.84it/s]Loading:  29%|██▉       | 14308/49018 [00:06<00:15, 2295.70it/s]Loading:  30%|██▉       | 14538/49018 [00:06<00:15, 2296.97it/s]Loading:  30%|███       | 14769/49018 [00:06<00:14, 2298.92it/s]Loading:  31%|███       | 14999/49018 [00:07<00:14, 2297.29it/s]Loading:  31%|███       | 15229/49018 [00:07<00:14, 2293.79it/s]Loading:  32%|███▏      | 15460/49018 [00:07<00:14, 2296.87it/s]Loading:  32%|███▏      | 15690/49018 [00:07<00:14, 2294.60it/s]Loading:  32%|███▏      | 15921/49018 [00:07<00:14, 2296.40it/s]Loading:  33%|███▎      | 16152/49018 [00:07<00:14, 2300.19it/s]Loading:  33%|███▎      | 16383/49018 [00:07<00:14, 2295.13it/s]Loading:  34%|███▍      | 16613/49018 [00:07<00:14, 2291.88it/s]Loading:  34%|███▍      | 16843/49018 [00:07<00:14, 2285.92it/s]Loading:  35%|███▍      | 17073/49018 [00:07<00:13, 2289.89it/s]Loading:  35%|███▌      | 17302/49018 [00:08<00:13, 2287.03it/s]Loading:  36%|███▌      | 17532/49018 [00:08<00:13, 2289.48it/s]Loading:  36%|███▌      | 17761/49018 [00:08<00:13, 2288.24it/s]Loading:  37%|███▋      | 17990/49018 [00:08<00:13, 2275.50it/s]Loading:  37%|███▋      | 18218/49018 [00:08<00:13, 2258.90it/s]Loading:  38%|███▊      | 18444/49018 [00:08<00:13, 2243.76it/s]Loading:  38%|███▊      | 18669/49018 [00:08<00:13, 2240.02it/s]Loading:  39%|███▊      | 18894/49018 [00:08<00:13, 2237.99it/s]Loading:  39%|███▉      | 19118/49018 [00:08<00:13, 2228.94it/s]Loading:  39%|███▉      | 19341/49018 [00:08<00:13, 2228.23it/s]Loading:  40%|███▉      | 19564/49018 [00:09<00:13, 2221.51it/s]Loading:  40%|████      | 19788/49018 [00:09<00:13, 2225.29it/s]Loading:  41%|████      | 20011/49018 [00:09<00:13, 2226.40it/s]Loading:  41%|████▏     | 20234/49018 [00:09<00:12, 2224.11it/s]Loading:  42%|████▏     | 20457/49018 [00:09<00:12, 2220.87it/s]Loading:  42%|████▏     | 20680/49018 [00:09<00:12, 2218.37it/s]Loading:  43%|████▎     | 20904/49018 [00:09<00:12, 2222.92it/s]Loading:  43%|████▎     | 21128/49018 [00:09<00:12, 2225.49it/s]Loading:  44%|████▎     | 21351/49018 [00:09<00:12, 2223.42it/s]Loading:  44%|████▍     | 21574/49018 [00:09<00:12, 2222.46it/s]Loading:  44%|████▍     | 21797/49018 [00:10<00:12, 2220.35it/s]Loading:  45%|████▍     | 22020/49018 [00:10<00:12, 2219.64it/s]Loading:  45%|████▌     | 22243/49018 [00:10<00:12, 2220.96it/s]Loading:  46%|████▌     | 22466/49018 [00:10<00:11, 2220.69it/s]Loading:  46%|████▋     | 22690/49018 [00:10<00:11, 2223.64it/s]Loading:  47%|████▋     | 22913/49018 [00:10<00:11, 2222.31it/s]Loading:  47%|████▋     | 23137/49018 [00:10<00:11, 2225.32it/s]Loading:  48%|████▊     | 23360/49018 [00:10<00:11, 2225.80it/s]Loading:  48%|████▊     | 23583/49018 [00:10<00:11, 2220.56it/s]Loading:  49%|████▊     | 23806/49018 [00:10<00:11, 2223.10it/s]Loading:  49%|████▉     | 24029/49018 [00:11<00:11, 2221.33it/s]Loading:  49%|████▉     | 24252/49018 [00:11<00:11, 2220.53it/s]Loading:  50%|████▉     | 24475/49018 [00:11<00:11, 2219.52it/s]Loading:  50%|█████     | 24698/49018 [00:11<00:10, 2222.04it/s]Loading:  51%|█████     | 24921/49018 [00:11<00:10, 2220.19it/s]Loading:  51%|█████▏    | 25144/49018 [00:11<00:10, 2215.43it/s]Loading:  52%|█████▏    | 25367/49018 [00:11<00:10, 2219.44it/s]Loading:  52%|█████▏    | 25589/49018 [00:11<00:10, 2215.28it/s]Loading:  53%|█████▎    | 25813/49018 [00:11<00:10, 2220.49it/s]Loading:  53%|█████▎    | 26037/49018 [00:11<00:10, 2225.39it/s]Loading:  54%|█████▎    | 26260/49018 [00:12<00:10, 2221.22it/s]Loading:  54%|█████▍    | 26483/49018 [00:12<00:10, 2220.85it/s]Loading:  54%|█████▍    | 26706/49018 [00:12<00:10, 2215.74it/s]Loading:  55%|█████▍    | 26929/49018 [00:12<00:09, 2217.69it/s]Loading:  55%|█████▌    | 27152/49018 [00:12<00:09, 2218.80it/s]Loading:  56%|█████▌    | 27374/49018 [00:12<00:09, 2217.28it/s]Loading:  56%|█████▋    | 27598/49018 [00:12<00:09, 2223.40it/s]Loading:  57%|█████▋    | 27821/49018 [00:12<00:09, 2224.22it/s]Loading:  57%|█████▋    | 28044/49018 [00:12<00:09, 2225.91it/s]Loading:  58%|█████▊    | 28267/49018 [00:12<00:09, 2221.99it/s]Loading:  58%|█████▊    | 28490/49018 [00:13<00:09, 2219.41it/s]Loading:  59%|█████▊    | 28713/49018 [00:13<00:09, 2220.61it/s]Loading:  59%|█████▉    | 28936/49018 [00:13<00:09, 2218.49it/s]Loading:  59%|█████▉    | 29160/49018 [00:13<00:08, 2222.71it/s]Loading:  60%|█████▉    | 29383/49018 [00:13<00:08, 2223.50it/s]Loading:  60%|██████    | 29606/49018 [00:13<00:08, 2218.49it/s]Loading:  61%|██████    | 29830/49018 [00:13<00:08, 2222.26it/s]Loading:  61%|██████▏   | 30053/49018 [00:13<00:08, 2220.15it/s]Loading:  62%|██████▏   | 30276/49018 [00:13<00:08, 2220.91it/s]Loading:  62%|██████▏   | 30500/49018 [00:13<00:08, 2224.67it/s]Loading:  63%|██████▎   | 30723/49018 [00:14<00:08, 2222.68it/s]Loading:  63%|██████▎   | 30947/49018 [00:14<00:08, 2227.14it/s]Loading:  64%|██████▎   | 31170/49018 [00:14<00:08, 2221.97it/s]Loading:  64%|██████▍   | 31394/49018 [00:14<00:07, 2225.22it/s]Loading:  65%|██████▍   | 31617/49018 [00:14<00:07, 2224.71it/s]Loading:  65%|██████▍   | 31840/49018 [00:14<00:07, 2221.77it/s]Loading:  65%|██████▌   | 32064/49018 [00:14<00:07, 2224.31it/s]Loading:  66%|██████▌   | 32287/49018 [00:14<00:07, 2224.86it/s]Loading:  66%|██████▋   | 32511/49018 [00:14<00:07, 2229.22it/s]Loading:  67%|██████▋   | 32735/49018 [00:14<00:07, 2229.64it/s]Loading:  67%|██████▋   | 32958/49018 [00:15<00:07, 2227.98it/s]Loading:  68%|██████▊   | 33182/49018 [00:15<00:07, 2229.69it/s]Loading:  68%|██████▊   | 33405/49018 [00:15<00:07, 2227.64it/s]Loading:  69%|██████▊   | 33629/49018 [00:15<00:06, 2228.93it/s]Loading:  69%|██████▉   | 33853/49018 [00:15<00:06, 2227.46it/s]Loading:  70%|██████▉   | 34077/49018 [00:15<00:06, 2228.64it/s]Loading:  70%|██████▉   | 34300/49018 [00:15<00:06, 2225.42it/s]Loading:  70%|███████   | 34523/49018 [00:15<00:06, 2223.46it/s]Loading:  71%|███████   | 34748/49018 [00:15<00:06, 2229.60it/s]Loading:  71%|███████▏  | 34971/49018 [00:15<00:06, 2227.47it/s]Loading:  72%|███████▏  | 35195/49018 [00:16<00:06, 2229.26it/s]Loading:  72%|███████▏  | 35419/49018 [00:16<00:06, 2230.39it/s]Loading:  73%|███████▎  | 35643/49018 [00:16<00:05, 2229.18it/s]Loading:  73%|███████▎  | 35868/49018 [00:16<00:05, 2233.74it/s]Loading:  74%|███████▎  | 36092/49018 [00:16<00:05, 2231.43it/s]Loading:  74%|███████▍  | 36316/49018 [00:16<00:05, 2230.97it/s]Loading:  75%|███████▍  | 36540/49018 [00:16<00:05, 2231.91it/s]Loading:  75%|███████▌  | 36764/49018 [00:16<00:05, 2227.23it/s]Loading:  75%|███████▌  | 36987/49018 [00:16<00:05, 2227.29it/s]Loading:  76%|███████▌  | 37210/49018 [00:16<00:05, 2226.98it/s]Loading:  76%|███████▋  | 37434/49018 [00:17<00:05, 2228.05it/s]Loading:  77%|███████▋  | 37658/49018 [00:17<00:05, 2230.68it/s]Loading:  77%|███████▋  | 37882/49018 [00:17<00:13, 817.56it/s] Loading:  78%|███████▊  | 38105/49018 [00:17<00:10, 1008.46it/s]Loading:  78%|███████▊  | 38328/49018 [00:18<00:08, 1205.78it/s]Loading:  79%|███████▊  | 38552/49018 [00:18<00:07, 1399.55it/s]Loading:  79%|███████▉  | 38777/49018 [00:18<00:06, 1579.09it/s]Loading:  80%|███████▉  | 38999/49018 [00:18<00:05, 1727.16it/s]Loading:  80%|████████  | 39225/49018 [00:18<00:05, 1857.81it/s]Loading:  80%|████████  | 39448/49018 [00:18<00:04, 1953.50it/s]Loading:  81%|████████  | 39671/49018 [00:18<00:04, 2028.54it/s]Loading:  81%|████████▏ | 39896/49018 [00:18<00:04, 2088.38it/s]Loading:  82%|████████▏ | 40119/49018 [00:18<00:04, 2128.40it/s]Loading:  82%|████████▏ | 40343/49018 [00:18<00:04, 2158.79it/s]Loading:  83%|████████▎ | 40566/49018 [00:19<00:03, 2179.54it/s]Loading:  83%|████████▎ | 40790/49018 [00:19<00:03, 2197.29it/s]Loading:  84%|████████▎ | 41014/49018 [00:19<00:03, 2207.99it/s]Loading:  84%|████████▍ | 41238/49018 [00:19<00:03, 2211.61it/s]Loading:  85%|████████▍ | 41461/49018 [00:19<00:03, 2215.66it/s]Loading:  85%|████████▌ | 41684/49018 [00:19<00:03, 2213.99it/s]Loading:  85%|████████▌ | 41907/49018 [00:19<00:03, 2217.39it/s]Loading:  86%|████████▌ | 42130/49018 [00:19<00:03, 2220.41it/s]Loading:  86%|████████▋ | 42353/49018 [00:19<00:03, 2219.90it/s]Loading:  87%|████████▋ | 42577/49018 [00:19<00:02, 2223.48it/s]Loading:  87%|████████▋ | 42800/49018 [00:20<00:02, 2222.10it/s]Loading:  88%|████████▊ | 43023/49018 [00:20<00:02, 2224.10it/s]Loading:  88%|████████▊ | 43247/49018 [00:20<00:02, 2228.79it/s]Loading:  89%|████████▊ | 43470/49018 [00:20<00:02, 2227.62it/s]Loading:  89%|████████▉ | 43694/49018 [00:20<00:02, 2230.22it/s]Loading:  90%|████████▉ | 43918/49018 [00:20<00:02, 2229.36it/s]Loading:  90%|█████████ | 44142/49018 [00:20<00:02, 2229.78it/s]Loading:  91%|█████████ | 44366/49018 [00:20<00:02, 2230.49it/s]Loading:  91%|█████████ | 44591/49018 [00:20<00:01, 2234.28it/s]Loading:  91%|█████████▏| 44815/49018 [00:20<00:01, 2235.10it/s]Loading:  92%|█████████▏| 45039/49018 [00:21<00:01, 2231.30it/s]Loading:  92%|█████████▏| 45264/49018 [00:21<00:01, 2235.66it/s]Loading:  93%|█████████▎| 45488/49018 [00:21<00:01, 2233.50it/s]Loading:  93%|█████████▎| 45713/49018 [00:21<00:01, 2235.92it/s]Loading:  94%|█████████▎| 45937/49018 [00:21<00:01, 2236.94it/s]Loading:  94%|█████████▍| 46161/49018 [00:21<00:01, 2233.70it/s]Loading:  95%|█████████▍| 46385/49018 [00:21<00:01, 2233.62it/s]Loading:  95%|█████████▌| 46609/49018 [00:21<00:01, 2232.38it/s]Loading:  96%|█████████▌| 46833/49018 [00:21<00:00, 2231.77it/s]Loading:  96%|█████████▌| 47057/49018 [00:21<00:00, 2231.33it/s]Loading:  96%|█████████▋| 47281/49018 [00:22<00:00, 2232.07it/s]Loading:  97%|█████████▋| 47506/49018 [00:22<00:00, 2235.60it/s]Loading:  97%|█████████▋| 47730/49018 [00:22<00:00, 2233.44it/s]Loading:  98%|█████████▊| 47954/49018 [00:22<00:00, 2235.07it/s]Loading:  98%|█████████▊| 48178/49018 [00:22<00:00, 2236.15it/s]Loading:  99%|█████████▊| 48402/49018 [00:22<00:00, 2230.86it/s]Loading:  99%|█████████▉| 48626/49018 [00:22<00:00, 2228.01it/s]Loading: 100%|█████████▉| 48850/49018 [00:22<00:00, 2228.85it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2147.65it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank8]:[W424 18:18:10.329958677 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 18:18:10.378371191 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 18:18:10.401451718 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 18:18:10.512937756 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank24]:[W424 18:18:11.114601366 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 18:18:11.142473961 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 18:18:11.144726517 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 18:18:11.151418970 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 18:18:11.160573797 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 18:18:11.161970788 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 18:18:11.175311541 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 18:18:11.332193613 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 18:18:11.352018373 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 18:18:11.355399879 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 18:18:11.358437172 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 18:18:11.360628072 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 18:18:11.379749771 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 18:18:11.385631422 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 18:18:11.397071912 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 18:18:11.728013228 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 18:18:11.767676033 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 18:18:11.770409545 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 18:18:11.771695173 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 18:18:11.786456963 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 18:18:11.791965217 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 18:18:11.011163495 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 18:18:11.030121729 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 18:18:11.034054698 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 18:18:11.034192580 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 18:18:11.041796511 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 18:18:11.046697856 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 18:18:11.391116879 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 18:18:11.280198575 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 18:18:11.651420014 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 18:18:11.652328000 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 18:18:11.652640144 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 18:18:11.281535280 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 18:18:12.604268574 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 18:18:12.865781857 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 18:18:12.794940881 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [00:24<19:49, 24.27s/it]Train:   4%|▍         | 2/50 [00:24<08:05, 10.12s/it]Train:   6%|▌         | 3/50 [00:24<04:23,  5.60s/it]Train:   8%|▊         | 4/50 [00:24<02:39,  3.47s/it]Train:  10%|█         | 5/50 [00:25<01:43,  2.30s/it]Train:  12%|█▏        | 6/50 [00:25<01:09,  1.59s/it]Train:  14%|█▍        | 7/50 [00:25<00:48,  1.14s/it]Train:  16%|█▌        | 8/50 [00:25<00:35,  1.19it/s]Train:  18%|█▊        | 9/50 [00:25<00:26,  1.55it/s]Train:  20%|██        | 10/50 [00:26<00:20,  1.95it/s]Train:  22%|██▏       | 11/50 [00:26<00:16,  2.38it/s]Train:  24%|██▍       | 12/50 [00:26<00:13,  2.80it/s]Train:  26%|██▌       | 13/50 [00:26<00:11,  3.20it/s]Train:  28%|██▊       | 14/50 [00:27<00:10,  3.54it/s]Train:  30%|███       | 15/50 [00:27<00:09,  3.83it/s]Train:  32%|███▏      | 16/50 [00:27<00:08,  4.06it/s]Train:  34%|███▍      | 17/50 [00:27<00:07,  4.23it/s]Train:  36%|███▌      | 18/50 [00:27<00:07,  4.39it/s]Train:  38%|███▊      | 19/50 [00:28<00:06,  4.48it/s]Train:  40%|████      | 20/50 [00:28<00:06,  4.54it/s]Train:  42%|████▏     | 21/50 [00:28<00:06,  4.61it/s]Train:  44%|████▍     | 22/50 [00:28<00:06,  4.63it/s]Train:  46%|████▌     | 23/50 [00:28<00:05,  4.64it/s]Train:  48%|████▊     | 24/50 [00:29<00:05,  4.67it/s]Train:  50%|█████     | 25/50 [00:29<00:05,  4.67it/s]Train:  52%|█████▏    | 26/50 [00:29<00:05,  4.66it/s]Train:  54%|█████▍    | 27/50 [00:29<00:04,  4.69it/s]Train:  56%|█████▌    | 28/50 [00:30<00:04,  4.71it/s]Train:  58%|█████▊    | 29/50 [00:30<00:04,  4.73it/s]Train:  60%|██████    | 30/50 [00:30<00:04,  4.72it/s]Train:  62%|██████▏   | 31/50 [00:30<00:04,  4.71it/s]Train:  64%|██████▍   | 32/50 [00:30<00:03,  4.71it/s]Train:  66%|██████▌   | 33/50 [00:31<00:03,  4.72it/s]Train:  68%|██████▊   | 34/50 [00:31<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:31<00:03,  4.74it/s]Train:  72%|███████▏  | 36/50 [00:31<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:31<00:02,  4.71it/s]Train:  76%|███████▌  | 38/50 [00:32<00:02,  4.70it/s]Train:  78%|███████▊  | 39/50 [00:32<00:02,  4.70it/s]Train:  80%|████████  | 40/50 [00:32<00:02,  4.70it/s]Train:  82%|████████▏ | 41/50 [00:32<00:01,  4.72it/s]Train:  84%|████████▍ | 42/50 [00:32<00:01,  4.72it/s]Train:  86%|████████▌ | 43/50 [00:33<00:01,  4.73it/s]Train:  88%|████████▊ | 44/50 [00:33<00:01,  4.73it/s]Train:  90%|█████████ | 45/50 [00:33<00:01,  4.71it/s]Train:  92%|█████████▏| 46/50 [00:33<00:00,  4.72it/s]Train:  94%|█████████▍| 47/50 [00:34<00:00,  4.72it/s]Train:  96%|█████████▌| 48/50 [00:34<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:34<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:34<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:34<00:00,  1.44it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.17it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.93it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.22it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.52it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.59it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.64it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.68it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.69it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.70it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.69it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.71it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.72it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.71it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.72it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.71it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.70it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.70it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.70it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.71it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.71it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.72it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.71it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.72it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.73it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.73it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.75it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.74it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.74it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.74it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.74it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.73it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.73it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.73it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.74it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.71it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.70it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.71it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.72it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.72it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.14it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.90it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.27it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.43it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.65it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.65it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.69it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.70it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.70it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.71it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.38it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.44it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.50it/s]Train:  36%|███▌      | 18/50 [00:03<00:07,  4.53it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.59it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.62it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.65it/s]Train:  44%|████▍     | 22/50 [00:04<00:06,  4.66it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.68it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.67it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.64it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.62it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.64it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.65it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.66it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.67it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.67it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.68it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.68it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.69it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.70it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.70it/s]Train:  74%|███████▍  | 37/50 [00:08<00:02,  4.70it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.69it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.70it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.70it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.68it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.68it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.69it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.69it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.70it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.69it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.69it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.69it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.70it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.61it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.23it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.96it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.25it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.61it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.65it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.70it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.70it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.69it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.68it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.69it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.69it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.70it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.70it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.73it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.73it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.71it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.72it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.72it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.72it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.72it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.71it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.68it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.69it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.70it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.71it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.71it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.73it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.72it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.75it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.75it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.76it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.76it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.74it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.74it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.75it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.76it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.76it/s]Train:  84%|████████▍ | 42/50 [00:08<00:01,  4.74it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.76it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.77it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.76it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.76it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.75it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.67it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.18it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.27it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.43it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.55it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.62it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.65it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.67it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.69it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.70it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.72it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.73it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.74it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.75it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.74it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.74it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.73it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.73it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.73it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.73it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.74it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.74it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.73it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.74it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.74it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.74it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.75it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.74it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.75it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.75it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.55it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.57it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.58it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.61it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.65it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.67it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.69it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.69it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.69it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.67it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.67it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.68it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.71it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.71it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.71it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.74it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.65it/s]
0: Process 0 - Local timer:  load_data  :  97.31
0: Process 0 - Local timer:  train_validate_test  :  77.91
0: Process 0 - Local timer:  create_model  :  1.3
0: Minimum timers: 
0: load_data  :  97.31
0: train_validate_test  :  77.85
0: create_model  :  1.29
0: Maximum timers: 
0: load_data  :  97.82
0: train_validate_test  :  77.91
0: create_model  :  1.3
0: Average timers: 
0: load_data  :  97.61
0: train_validate_test  :  77.86
0: create_model  :  1.3
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 18:19:07.988343914 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 18:19:09.685499302 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 18:19:09.685681628 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 18:19:09.945830341 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 18:19:09.946026073 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 18:19:09.686051239 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 18:19:09.946060739 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 18:19:09.686060326 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 18:19:09.946132295 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 18:19:09.686494569 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 18:19:09.686927571 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 18:19:09.947030273 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 18:19:09.947181711 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 18:19:09.576160950 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 18:19:09.576173624 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 18:19:09.576156722 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 18:19:09.244662112 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 18:19:09.947468256 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 18:19:09.576142455 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 18:19:09.244671760 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 18:19:09.576360509 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 18:19:09.244669175 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 18:19:09.576372431 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 18:19:09.244649177 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 18:19:09.576371299 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 18:19:09.244706105 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 18:19:09.244705234 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 18:19:09.244655770 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 18:19:09.947813132 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 18:19:09.244841182 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 18:19:09.576777459 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 18:19:09.289774379 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 18:19:09.289762807 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 18:19:09.289774178 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 18:19:09.289789908 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 18:19:09.289902992 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 18:19:09.289766684 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 18:19:09.289927829 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 18:19:09.289950783 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 18:19:09.689318625 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 06:19:11 PM EDT 2025
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Node count: 5, num_processes: 40, effective batch size: 1280, local batch size: 32
Experiment started at Thu Apr 24 06:19:11 PM EDT 2025
+ timeout --signal=TERM --kill-after=10s 10m srun --exclusive -n40 --ntasks-per-node=8 -c4 --gpus-per-task=1 --gpu-bind=closest python -u ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_5 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5
[W424 18:19:24.663972784 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.664048167 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.664149569 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.664246433 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.664251673 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.665782365 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.665829484 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.665861335 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.048991244 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.049328305 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.049432674 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.049431141 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.049439827 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.050528507 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.051028959 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.051450601 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.352778612 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.353019129 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.353050428 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.353067581 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.353180465 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.353222395 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.354334646 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.354674401 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.421391941 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.421659087 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.421691308 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.421709813 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.422117755 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.422970631 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.423193483 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:24.423619319 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
Distributed data parallel: nccl master at frontier07676:8889
[W424 18:19:25.713739646 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.713818265 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.713882918 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.713966777 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.715776950 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.715872421 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.717550904 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
[W424 18:19:25.731545094 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [frontier07676.frontier.olcf.ornl.gov]:8889 (errno: 97 - Address family not supported by protocol).
0: Command: ./examples/multibranch/train.py --log=SC25_multibranch_weakscaling_JOB3393901_N5_NPROCS40_EBS1280_LBS32_MaxNumBatch50_TP0_INTERFERENCE_5 --inputfile=multibranch_GFM260_SC25.json --multi --ddstore --multi_model_list=/mnt/bb/kmehta/ANI1x-v3.bp,/mnt/bb/kmehta/qm7x-v3.bp,/mnt/bb/kmehta/MPTrj-v3.bp,/mnt/bb/kmehta/Alexandria-v3.bp,/mnt/bb/kmehta/transition1x-v3.bp --num_samples=64000 --everyone --batch_size=32 --num_epoch=5

mymodel: 0 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 3 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 24 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 32 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 8 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 4 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 16 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 25 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 33 4 /mnt/bb/kmehta/transition1x-v3.bp
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 9 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 6 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 17 2 /mnt/bb/kmehta/MPTrj-v3.bp
mymodel: 26 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 34 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 10 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 7 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 18 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 27 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 35 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 11 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 1 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 19 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 28 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 36 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 12 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 2 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 20 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 29 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 37 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 13 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 5 0 /mnt/bb/kmehta/ANI1x-v3.bp
mymodel: 21 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 30 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 38 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 14 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 22 3 /mnt/bb/kmehta/Alexandria-v3.bp
mymodel: 31 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 39 4 /mnt/bb/kmehta/transition1x-v3.bp
mymodel: 15 1 /mnt/bb/kmehta/qm7x-v3.bp
mymodel: 23 3 /mnt/bb/kmehta/Alexandria-v3.bp
0: Read attr time (sec):  0.0019156932830810547
0: read and bcast: trainset/x/variable_count 0.18405866622924805
0: read and bcast: trainset/x/variable_offset 0.3705136775970459
0: read and bcast: trainset/x/variable_dim 0.3709297180175781
17 mptrj nsplit: 1422346 2 711173
16 mptrj nsplit: 1422346 2 711173
0: read and bcast: trainset/edge_index/variable_count 0.5789828300476074
0: read and bcast: trainset/edge_index/variable_offset 0.7655842304229736
0: read and bcast: trainset/edge_index/variable_dim 0.7743995189666748
0: read and bcast: trainset/edge_attr/variable_count 0.957939624786377
0: read and bcast: trainset/edge_attr/variable_offset 1.1430160999298096
0: read and bcast: trainset/edge_attr/variable_dim 1.153566837310791
0: read and bcast: trainset/pos/variable_count 1.3380577564239502
0: read and bcast: trainset/pos/variable_offset 1.5226662158966064
0: read and bcast: trainset/pos/variable_dim 1.5337557792663574
0: read and bcast: trainset/energy/variable_count 1.7183690071105957
0: read and bcast: trainset/energy/variable_offset 1.902841567993164
0: read and bcast: trainset/energy/variable_dim 1.914393424987793
0: read and bcast: trainset/forces/variable_count 2.0993494987487793
0: read and bcast: trainset/forces/variable_offset 2.283494234085083
0: read and bcast: trainset/forces/variable_dim 2.294402837753296
0: read and bcast: trainset/y/variable_count 2.4794094562530518
0: read and bcast: trainset/y/variable_offset 2.66338849067688
0: read and bcast: trainset/y/variable_dim 2.674647569656372
0: Overall time (sec):  2.676802158355713
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  2.6811516284942627
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0008246898651123047
0: read and bcast: valset/x/variable_count 0.007979393005371094
0: read and bcast: valset/x/variable_offset 0.015212535858154297
0: read and bcast: valset/x/variable_dim 0.015378952026367188
0: read and bcast: valset/edge_index/variable_count 0.022966623306274414
0: read and bcast: valset/edge_index/variable_offset 0.030365943908691406
0: read and bcast: valset/edge_index/variable_dim 0.030538082122802734
0: read and bcast: valset/edge_attr/variable_count 0.03743863105773926
0: read and bcast: valset/edge_attr/variable_offset 0.04506683349609375
0: read and bcast: valset/edge_attr/variable_dim 0.04525136947631836
0: read and bcast: valset/pos/variable_count 0.05214095115661621
0: read and bcast: valset/pos/variable_offset 0.05925345420837402
0: read and bcast: valset/pos/variable_dim 0.05942106246948242
0: read and bcast: valset/energy/variable_count 0.06698250770568848
0: read and bcast: valset/energy/variable_offset 0.07400774955749512
0: read and bcast: valset/energy/variable_dim 0.07419037818908691
0: read and bcast: valset/forces/variable_count 0.08119320869445801
0: read and bcast: valset/forces/variable_offset 0.08869099617004395
0: read and bcast: valset/forces/variable_dim 0.08887910842895508
0: read and bcast: valset/y/variable_count 0.09591937065124512
0: read and bcast: valset/y/variable_offset 0.1033782958984375
0: read and bcast: valset/y/variable_dim 0.10357785224914551
0: Overall time (sec):  0.10462093353271484
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.10779786109924316
0: Adios reading: /mnt/bb/kmehta/ANI1x-v3.bp
0: Read attr time (sec):  0.0007863044738769531
0: read and bcast: testset/x/variable_count 0.007909059524536133
0: read and bcast: testset/x/variable_offset 0.015434741973876953
0: read and bcast: testset/x/variable_dim 0.01560831069946289
0: read and bcast: testset/edge_index/variable_count 0.023210763931274414
0: read and bcast: testset/edge_index/variable_offset 0.030471324920654297
0: read and bcast: testset/edge_index/variable_dim 0.030661582946777344
0: read and bcast: testset/edge_attr/variable_count 0.03814840316772461
0: read and bcast: testset/edge_attr/variable_offset 0.04500222206115723
0: read and bcast: testset/edge_attr/variable_dim 0.04517388343811035
0: read and bcast: testset/pos/variable_count 0.05191230773925781
0: read and bcast: testset/pos/variable_offset 0.058702945709228516
0: read and bcast: testset/pos/variable_dim 0.05887150764465332
0: read and bcast: testset/energy/variable_count 0.0663154125213623
0: read and bcast: testset/energy/variable_offset 0.07332301139831543
0: read and bcast: testset/energy/variable_dim 0.0734860897064209
0: read and bcast: testset/forces/variable_count 0.08081531524658203
0: read and bcast: testset/forces/variable_offset 0.08803892135620117
0: read and bcast: testset/forces/variable_dim 0.08821988105773926
0: read and bcast: testset/y/variable_count 0.09513330459594727
0: read and bcast: testset/y/variable_offset 0.10203242301940918
0: read and bcast: testset/y/variable_dim 0.10219597816467285
0: Overall time (sec):  0.10317754745483398
0: DDStore adding time (sec):  0.0
0: Data loading time (sec):  0.1061408519744873
1 ani1x nsplit: 4460384 6 743398
2 ani1x nsplit: 4460384 6 743397
4 ani1x nsplit: 4460384 6 743397
3 ani1x nsplit: 4460384 6 743397
5 ani1x nsplit: 4460384 6 743397
0 ani1x nsplit: 4460384 6 743398
7 qm7x nsplit: 7551412 10 755142
6 qm7x nsplit: 7551412 10 755142
9 qm7x nsplit: 7551412 10 755141
11 qm7x nsplit: 7551412 10 755141
14 qm7x nsplit: 7551412 10 755141
10 qm7x nsplit: 7551412 10 755141
12 qm7x nsplit: 7551412 10 755141
13 qm7x nsplit: 7551412 10 755141
15 qm7x nsplit: 7551412 10 755141
8 qm7x nsplit: 7551412 10 755141
31 transition1x nsplit: 8680250 11 789114
30 transition1x nsplit: 8680250 11 789114
29 transition1x nsplit: 8680250 11 789114
33 transition1x nsplit: 8680250 11 789114
36 transition1x nsplit: 8680250 11 789113
37 transition1x nsplit: 8680250 11 789113
38 transition1x nsplit: 8680250 11 789113
39 transition1x nsplit: 8680250 11 789113
34 transition1x nsplit: 8680250 11 789114
35 transition1x nsplit: 8680250 11 789114
32 transition1x nsplit: 8680250 11 789114
26 alexandria nsplit: 9705384 11 882307
19 alexandria nsplit: 9705384 11 882308
27 alexandria nsplit: 9705384 11 882307
20 alexandria nsplit: 9705384 11 882308
28 alexandria nsplit: 9705384 11 882307
23 alexandria nsplit: 9705384 11 882308
25 alexandria nsplit: 9705384 11 882307
21 alexandria nsplit: 9705384 11 882308
22 alexandria nsplit: 9705384 11 882308
24 alexandria nsplit: 9705384 11 882308
18 alexandria nsplit: 9705384 11 882308
24 local dataset: 6 11 trainset 9705384 5293848 5357847 64000 alexandria
25 local dataset: 7 11 trainset 9705384 6176156 6240155 64000 alexandria
26 local dataset: 8 11 trainset 9705384 7058463 7122462 64000 alexandria
32 local dataset: 3 11 trainset 8680250 2367342 2431341 64000 transition1x
8 local dataset: 2 10 trainset 7551412 1510284 1574283 64000 qm7x
0 local dataset: 0 6 trainset 4460384 0 63999 64000 ani1x
16 local dataset: 0 2 trainset 1422346 0 63999 64000 mptrj
27 local dataset: 9 11 trainset 9705384 7940770 8004769 64000 alexandria
33 local dataset: 4 11 trainset 8680250 3156456 3220455 64000 transition1x
9 local dataset: 3 10 trainset 7551412 2265425 2329424 64000 qm7x
1 local dataset: 1 6 trainset 4460384 743398 807397 64000 ani1x
17 local dataset: 1 2 trainset 1422346 711173 775172 64000 mptrj
28 local dataset: 10 11 trainset 9705384 8823077 8887076 64000 alexandria
34 local dataset: 5 11 trainset 8680250 3945570 4009569 64000 transition1x
10 local dataset: 4 10 trainset 7551412 3020566 3084565 64000 qm7x
2 local dataset: 2 6 trainset 4460384 1486796 1550795 64000 ani1x
18 local dataset: 0 11 trainset 9705384 0 63999 64000 alexandria
29 local dataset: 0 11 trainset 8680250 0 63999 64000 transition1x
35 local dataset: 6 11 trainset 8680250 4734684 4798683 64000 transition1x
11 local dataset: 5 10 trainset 7551412 3775707 3839706 64000 qm7x
3 local dataset: 3 6 trainset 4460384 2230193 2294192 64000 ani1x
19 local dataset: 1 11 trainset 9705384 882308 946307 64000 alexandria
30 local dataset: 1 11 trainset 8680250 789114 853113 64000 transition1x
36 local dataset: 7 11 trainset 8680250 5523798 5587797 64000 transition1x
12 local dataset: 6 10 trainset 7551412 4530848 4594847 64000 qm7x
4 local dataset: 4 6 trainset 4460384 2973590 3037589 64000 ani1x
20 local dataset: 2 11 trainset 9705384 1764616 1828615 64000 alexandria
31 local dataset: 2 11 trainset 8680250 1578228 1642227 64000 transition1x
37 local dataset: 8 11 trainset 8680250 6312911 6376910 64000 transition1x
13 local dataset: 7 10 trainset 7551412 5285989 5349988 64000 qm7x
5 local dataset: 5 6 trainset 4460384 3716987 3780986 64000 ani1x
21 local dataset: 3 11 trainset 9705384 2646924 2710923 64000 alexandria
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
38 local dataset: 9 11 trainset 8680250 7102024 7166023 64000 transition1x
14 local dataset: 8 10 trainset 7551412 6041130 6105129 64000 qm7x
6 local dataset: 0 10 trainset 7551412 0 63999 64000 qm7x
22 local dataset: 4 11 trainset 9705384 3529232 3593231 64000 alexandria
39 local dataset: 10 11 trainset 8680250 7891137 7955136 64000 transition1x
15 local dataset: 9 10 trainset 7551412 6796271 6860270 64000 qm7x
7 local dataset: 1 10 trainset 7551412 755142 819141 64000 qm7x
23 local dataset: 5 11 trainset 9705384 4411540 4475539 64000 alexandria
29 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
25 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
26 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
19 alexandria nsplit: 539189 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
27 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
23 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
28 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
21 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
31 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
30 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
20 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539189 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
32 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
36 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
5 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
33 transition1x nsplit: 482234 11 43840
WARN: Requested num_samples is larger than available in transition1x: 64000 43840
1 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
37 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
39 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
34 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
2 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
0: Adios reading time (sec):  0.4796938896179199
0 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
4 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
16 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
3 ani1x nsplit: 247800 6 41300
WARN: Requested num_samples is larger than available in ani1x: 64000 41300
17 mptrj nsplit: 79020 2 39510
WARN: Requested num_samples is larger than available in mptrj: 64000 39510
7 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
15 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
6 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
38 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
13 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
10 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
12 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
35 transition1x nsplit: 482234 11 43839
WARN: Requested num_samples is larger than available in transition1x: 64000 43839
14 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
9 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 qm7x nsplit: 419523 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
11 qm7x nsplit: 419523 10 41952
WARN: Requested num_samples is larger than available in qm7x: 64000 41952
8 local dataset: 2 10 valset 419523 83906 125858 41953 qm7x
0 local dataset: 0 6 valset 247800 0 41299 41300 ani1x
16 local dataset: 0 2 valset 79020 0 39509 39510 mptrj
24 local dataset: 6 11 valset 539189 294104 343120 49017 alexandria
32 local dataset: 3 11 valset 482234 131520 175359 43840 transition1x
9 local dataset: 3 10 valset 419523 125859 167810 41952 qm7x
1 local dataset: 1 6 valset 247800 41300 82599 41300 ani1x
17 local dataset: 1 2 valset 79020 39510 79019 39510 mptrj
25 local dataset: 7 11 valset 539189 343121 392137 49017 alexandria
33 local dataset: 4 11 valset 482234 175360 219199 43840 transition1x
34 local dataset: 5 11 valset 482234 219200 263038 43839 transition1x
35 local dataset: 6 11 valset 482234 263039 306877 43839 transition1x
36 local dataset: 7 11 valset 482234 306878 350716 43839 transition1x
10 local dataset: 4 10 valset 419523 167811 209762 41952 qm7x
18 local dataset: 0 11 valset 539189 0 49017 49018 alexandria
26 local dataset: 8 11 valset 539189 392138 441154 49017 alexandria
37 local dataset: 8 11 valset 482234 350717 394555 43839 transition1x
11 local dataset: 5 10 valset 419523 209763 251714 41952 qm7x
2 local dataset: 2 6 valset 247800 82600 123899 41300 ani1x
19 local dataset: 1 11 valset 539189 49018 98035 49018 alexandria
27 local dataset: 9 11 valset 539189 441155 490171 49017 alexandria
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
38 local dataset: 9 11 valset 482234 394556 438394 43839 transition1x
12 local dataset: 6 10 valset 419523 251715 293666 41952 qm7x
3 local dataset: 3 6 valset 247800 123900 165199 41300 ani1x
20 local dataset: 2 11 valset 539189 98036 147052 49017 alexandria
28 local dataset: 10 11 valset 539189 490172 539188 49017 alexandria
39 local dataset: 10 11 valset 482234 438395 482233 43839 transition1x
13 local dataset: 7 10 valset 419523 293667 335618 41952 qm7x
4 local dataset: 4 6 valset 247800 165200 206499 41300 ani1x
21 local dataset: 3 11 valset 539189 147053 196069 49017 alexandria
29 local dataset: 0 11 valset 482234 0 43839 43840 transition1x
14 local dataset: 8 10 valset 419523 335619 377570 41952 qm7x
5 local dataset: 5 6 valset 247800 206500 247799 41300 ani1x
22 local dataset: 4 11 valset 539189 196070 245086 49017 alexandria
30 local dataset: 1 11 valset 482234 43840 87679 43840 transition1x
15 local dataset: 9 10 valset 419523 377571 419522 41952 qm7x
6 local dataset: 0 10 valset 419523 0 41952 41953 qm7x
23 local dataset: 5 11 valset 539189 245087 294103 49017 alexandria
31 local dataset: 2 11 valset 482234 87680 131519 43840 transition1x
7 local dataset: 1 10 valset 419523 41953 83905 41953 qm7x
29 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
28 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
18 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
26 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
27 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
25 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
24 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
22 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
19 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
20 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
21 alexandria nsplit: 539191 11 49018
WARN: Requested num_samples is larger than available in alexandria: 64000 49018
31 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
23 alexandria nsplit: 539191 11 49017
WARN: Requested num_samples is larger than available in alexandria: 64000 49017
30 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
5 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
4 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
0: Adios reading time (sec):  0.1419963836669922
0 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
1 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
32 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
36 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
33 transition1x nsplit: 482256 11 43842
WARN: Requested num_samples is larger than available in transition1x: 64000 43842
37 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
17 mptrj nsplit: 79029 2 39514
WARN: Requested num_samples is larger than available in mptrj: 64000 39514
16 mptrj nsplit: 79029 2 39515
WARN: Requested num_samples is larger than available in mptrj: 64000 39515
3 ani1x nsplit: 247821 6 41303
WARN: Requested num_samples is larger than available in ani1x: 64000 41303
2 ani1x nsplit: 247821 6 41304
WARN: Requested num_samples is larger than available in ani1x: 64000 41304
39 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
13 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
7 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
15 qm7x nsplit: 419539 10 41953
WARN: Requested num_samples is larger than available in qm7x: 64000 41953
6 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
35 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
14 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
34 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
12 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
38 transition1x nsplit: 482256 11 43841
WARN: Requested num_samples is larger than available in transition1x: 64000 43841
9 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
10 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
11 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 qm7x nsplit: 419539 10 41954
WARN: Requested num_samples is larger than available in qm7x: 64000 41954
8 local dataset: 2 10 testset 419539 83908 125861 41954 qm7x
9 local dataset: 3 10 testset 419539 125862 167815 41954 qm7x
17 local dataset: 1 2 testset 79029 39515 79028 39514 mptrj
25 local dataset: 7 11 testset 539191 343123 392139 49017 alexandria
33 local dataset: 4 11 testset 482256 175368 219209 43842 transition1x
10 local dataset: 4 10 testset 419539 167816 209769 41954 qm7x
0 local dataset: 0 6 testset 247821 0 41303 41304 ani1x
18 local dataset: 0 11 testset 539191 0 49017 49018 alexandria
26 local dataset: 8 11 testset 539191 392140 441156 49017 alexandria
34 local dataset: 5 11 testset 482256 219210 263050 43841 transition1x
11 local dataset: 5 10 testset 419539 209770 251723 41954 qm7x
1 local dataset: 1 6 testset 247821 41304 82607 41304 ani1x
19 local dataset: 1 11 testset 539191 49018 98035 49018 alexandria
27 local dataset: 9 11 testset 539191 441157 490173 49017 alexandria
35 local dataset: 6 11 testset 482256 263051 306891 43841 transition1x
12 local dataset: 6 10 testset 419539 251724 293677 41954 qm7x
2 local dataset: 2 6 testset 247821 82608 123911 41304 ani1x
0: Adios preloading: /mnt/bb/kmehta/ANI1x-v3.bp
20 local dataset: 2 11 testset 539191 98036 147053 49018 alexandria
28 local dataset: 10 11 testset 539191 490174 539190 49017 alexandria
36 local dataset: 7 11 testset 482256 306892 350732 43841 transition1x
13 local dataset: 7 10 testset 419539 293678 335631 41954 qm7x
3 local dataset: 3 6 testset 247821 123912 165214 41303 ani1x
21 local dataset: 3 11 testset 539191 147054 196071 49018 alexandria
29 local dataset: 0 11 testset 482256 0 43841 43842 transition1x
37 local dataset: 8 11 testset 482256 350733 394573 43841 transition1x
14 local dataset: 8 10 testset 419539 335632 377585 41954 qm7x
4 local dataset: 4 6 testset 247821 165215 206517 41303 ani1x
22 local dataset: 4 11 testset 539191 196072 245088 49017 alexandria
30 local dataset: 1 11 testset 482256 43842 87683 43842 transition1x
38 local dataset: 9 11 testset 482256 394574 438414 43841 transition1x
15 local dataset: 9 10 testset 419539 377586 419538 41953 qm7x
5 local dataset: 5 6 testset 247821 206518 247820 41303 ani1x
16 local dataset: 0 2 testset 79029 0 39514 39515 mptrj
31 local dataset: 2 11 testset 482256 87684 131525 43842 transition1x
39 local dataset: 10 11 testset 482256 438415 482255 43841 transition1x
6 local dataset: 0 10 testset 419539 0 41953 41954 qm7x
23 local dataset: 5 11 testset 539191 245089 294105 49017 alexandria
24 local dataset: 6 11 testset 539191 294106 343122 49017 alexandria
32 local dataset: 3 11 testset 482256 131526 175367 43842 transition1x
7 local dataset: 1 10 testset 419539 41954 83907 41954 qm7x
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
0: Adios reading time (sec):  0.13818120956420898
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
num_samples_list: []
Loading:   0%|          | 0/64000 [00:00<?, ?it/s]Loading:   0%|          | 204/64000 [00:00<00:31, 2035.49it/s]Loading:   1%|          | 435/64000 [00:00<00:29, 2191.23it/s]Loading:   1%|          | 665/64000 [00:00<00:28, 2236.48it/s]Loading:   1%|▏         | 896/64000 [00:00<00:27, 2264.13it/s]Loading:   2%|▏         | 1126/64000 [00:00<00:27, 2275.86it/s]Loading:   2%|▏         | 1354/64000 [00:00<00:27, 2272.17it/s]Loading:   2%|▏         | 1582/64000 [00:00<00:27, 2258.61it/s]Loading:   3%|▎         | 1808/64000 [00:00<00:27, 2245.69it/s]Loading:   3%|▎         | 2033/64000 [00:00<00:27, 2238.45it/s]Loading:   4%|▎         | 2257/64000 [00:01<00:27, 2227.15it/s]Loading:   4%|▍         | 2480/64000 [00:01<00:27, 2225.26it/s]Loading:   4%|▍         | 2703/64000 [00:01<00:27, 2217.26it/s]Loading:   5%|▍         | 2925/64000 [00:01<00:27, 2211.33it/s]Loading:   5%|▍         | 3148/64000 [00:01<00:27, 2214.56it/s]Loading:   5%|▌         | 3370/64000 [00:01<00:27, 2212.19it/s]Loading:   6%|▌         | 3593/64000 [00:01<00:27, 2216.74it/s]Loading:   6%|▌         | 3815/64000 [00:01<00:27, 2216.84it/s]Loading:   6%|▋         | 4037/64000 [00:01<00:38, 1548.95it/s]Loading:   7%|▋         | 4260/64000 [00:02<00:35, 1704.21it/s]Loading:   7%|▋         | 4482/64000 [00:02<00:32, 1829.91it/s]Loading:   7%|▋         | 4706/64000 [00:02<00:30, 1934.91it/s]Loading:   8%|▊         | 4928/64000 [00:02<00:29, 2011.69it/s]Loading:   8%|▊         | 5149/64000 [00:02<00:28, 2066.82it/s]Loading:   8%|▊         | 5373/64000 [00:02<00:27, 2113.72it/s]Loading:   9%|▊         | 5595/64000 [00:02<00:27, 2142.98it/s]Loading:   9%|▉         | 5818/64000 [00:02<00:26, 2167.07it/s]Loading:   9%|▉         | 6041/64000 [00:02<00:26, 2182.93it/s]Loading:  10%|▉         | 6262/64000 [00:02<00:26, 2187.66it/s]Loading:  10%|█         | 6485/64000 [00:03<00:26, 2197.40it/s]Loading:  10%|█         | 6706/64000 [00:03<00:26, 2200.74it/s]Loading:  11%|█         | 6928/64000 [00:03<00:25, 2203.79it/s]Loading:  11%|█         | 7151/64000 [00:03<00:25, 2210.71it/s]Loading:  12%|█▏        | 7373/64000 [00:03<00:25, 2212.70it/s]Loading:  12%|█▏        | 7596/64000 [00:03<00:25, 2217.18it/s]Loading:  12%|█▏        | 7818/64000 [00:03<00:25, 2217.51it/s]Loading:  13%|█▎        | 8042/64000 [00:03<00:25, 2221.53it/s]Loading:  13%|█▎        | 8265/64000 [00:03<00:25, 2223.55it/s]Loading:  13%|█▎        | 8488/64000 [00:03<00:24, 2221.96it/s]Loading:  14%|█▎        | 8711/64000 [00:04<00:24, 2223.91it/s]Loading:  14%|█▍        | 8934/64000 [00:04<00:24, 2217.97it/s]Loading:  14%|█▍        | 9157/64000 [00:04<00:24, 2220.42it/s]Loading:  15%|█▍        | 9380/64000 [00:04<00:24, 2217.75it/s]Loading:  15%|█▌        | 9603/64000 [00:04<00:24, 2220.58it/s]Loading:  15%|█▌        | 9826/64000 [00:04<00:24, 2219.86it/s]Loading:  16%|█▌        | 10048/64000 [00:04<00:24, 2217.09it/s]Loading:  16%|█▌        | 10271/64000 [00:04<00:24, 2219.16it/s]Loading:  16%|█▋        | 10493/64000 [00:04<00:24, 2213.49it/s]Loading:  17%|█▋        | 10715/64000 [00:04<00:24, 2215.37it/s]Loading:  17%|█▋        | 10938/64000 [00:05<00:23, 2219.09it/s]Loading:  17%|█▋        | 11160/64000 [00:05<00:23, 2213.88it/s]Loading:  18%|█▊        | 11382/64000 [00:05<00:23, 2215.34it/s]Loading:  18%|█▊        | 11604/64000 [00:05<00:23, 2212.68it/s]Loading:  18%|█▊        | 11827/64000 [00:05<00:23, 2216.07it/s]Loading:  19%|█▉        | 12049/64000 [00:05<00:23, 2212.61it/s]Loading:  19%|█▉        | 12271/64000 [00:05<00:34, 1499.65it/s]Loading:  20%|█▉        | 12493/64000 [00:05<00:31, 1659.70it/s]Loading:  20%|█▉        | 12715/64000 [00:06<00:28, 1793.69it/s]Loading:  20%|██        | 12936/64000 [00:06<00:26, 1900.45it/s]Loading:  21%|██        | 13159/64000 [00:06<00:25, 1987.72it/s]Loading:  21%|██        | 13380/64000 [00:06<00:24, 2046.97it/s]Loading:  21%|██▏       | 13602/64000 [00:06<00:24, 2095.93it/s]Loading:  22%|██▏       | 13824/64000 [00:06<00:23, 2130.06it/s]Loading:  22%|██▏       | 14046/64000 [00:06<00:23, 2155.04it/s]Loading:  22%|██▏       | 14271/64000 [00:06<00:22, 2180.78it/s]Loading:  23%|██▎       | 14494/64000 [00:06<00:22, 2194.36it/s]Loading:  23%|██▎       | 14718/64000 [00:06<00:22, 2206.40it/s]Loading:  23%|██▎       | 14941/64000 [00:07<00:22, 2212.60it/s]Loading:  24%|██▎       | 15165/64000 [00:07<00:22, 2217.98it/s]Loading:  24%|██▍       | 15389/64000 [00:07<00:21, 2222.74it/s]Loading:  24%|██▍       | 15612/64000 [00:07<00:21, 2223.25it/s]Loading:  25%|██▍       | 15836/64000 [00:07<00:21, 2228.05it/s]Loading:  25%|██▌       | 16060/64000 [00:07<00:21, 2225.76it/s]Loading:  25%|██▌       | 16283/64000 [00:07<00:21, 2224.77it/s]Loading:  26%|██▌       | 16506/64000 [00:07<00:21, 2224.10it/s]Loading:  26%|██▌       | 16729/64000 [00:07<00:21, 2216.67it/s]Loading:  26%|██▋       | 16952/64000 [00:07<00:21, 2219.76it/s]Loading:  27%|██▋       | 17175/64000 [00:08<00:21, 2218.60it/s]Loading:  27%|██▋       | 17398/64000 [00:08<00:20, 2219.81it/s]Loading:  28%|██▊       | 17621/64000 [00:08<00:20, 2222.45it/s]Loading:  28%|██▊       | 17844/64000 [00:08<00:20, 2221.16it/s]Loading:  28%|██▊       | 18067/64000 [00:08<00:20, 2222.21it/s]Loading:  29%|██▊       | 18290/64000 [00:08<00:20, 2220.85it/s]Loading:  29%|██▉       | 18513/64000 [00:08<00:20, 2221.68it/s]Loading:  29%|██▉       | 18736/64000 [00:08<00:20, 2220.92it/s]Loading:  30%|██▉       | 18959/64000 [00:08<00:20, 2216.29it/s]Loading:  30%|██▉       | 19182/64000 [00:08<00:20, 2218.65it/s]Loading:  30%|███       | 19404/64000 [00:09<00:20, 2217.32it/s]Loading:  31%|███       | 19627/64000 [00:09<00:19, 2219.61it/s]Loading:  31%|███       | 19850/64000 [00:09<00:19, 2222.00it/s]Loading:  31%|███▏      | 20073/64000 [00:09<00:19, 2219.75it/s]Loading:  32%|███▏      | 20297/64000 [00:09<00:19, 2223.60it/s]Loading:  32%|███▏      | 20520/64000 [00:09<00:19, 2219.50it/s]Loading:  32%|███▏      | 20743/64000 [00:09<00:19, 2220.73it/s]Loading:  33%|███▎      | 20967/64000 [00:09<00:19, 2224.64it/s]Loading:  33%|███▎      | 21190/64000 [00:09<00:19, 2222.13it/s]Loading:  33%|███▎      | 21413/64000 [00:09<00:19, 2221.91it/s]Loading:  34%|███▍      | 21636/64000 [00:10<00:19, 2219.50it/s]Loading:  34%|███▍      | 21859/64000 [00:10<00:18, 2220.78it/s]Loading:  35%|███▍      | 22083/64000 [00:10<00:18, 2225.13it/s]Loading:  35%|███▍      | 22306/64000 [00:10<00:29, 1413.52it/s]Loading:  35%|███▌      | 22527/64000 [00:10<00:26, 1582.91it/s]Loading:  36%|███▌      | 22747/64000 [00:10<00:23, 1725.40it/s]Loading:  36%|███▌      | 22969/64000 [00:10<00:22, 1848.41it/s]Loading:  36%|███▌      | 23191/64000 [00:10<00:20, 1945.93it/s]Loading:  37%|███▋      | 23413/64000 [00:11<00:20, 2019.13it/s]Loading:  37%|███▋      | 23636/64000 [00:11<00:19, 2076.21it/s]Loading:  37%|███▋      | 23857/64000 [00:11<00:19, 2112.26it/s]Loading:  38%|███▊      | 24080/64000 [00:11<00:18, 2144.48it/s]Loading:  38%|███▊      | 24301/64000 [00:11<00:18, 2163.33it/s]Loading:  38%|███▊      | 24523/64000 [00:11<00:18, 2177.35it/s]Loading:  39%|███▊      | 24745/64000 [00:11<00:17, 2188.96it/s]Loading:  39%|███▉      | 24966/64000 [00:11<00:17, 2192.81it/s]Loading:  39%|███▉      | 25188/64000 [00:11<00:17, 2199.53it/s]Loading:  40%|███▉      | 25410/64000 [00:11<00:17, 2205.12it/s]Loading:  40%|████      | 25632/64000 [00:12<00:17, 2207.06it/s]Loading:  40%|████      | 25855/64000 [00:12<00:17, 2212.67it/s]Loading:  41%|████      | 26077/64000 [00:12<00:17, 2211.60it/s]Loading:  41%|████      | 26299/64000 [00:12<00:17, 2209.96it/s]Loading:  41%|████▏     | 26522/64000 [00:12<00:16, 2213.64it/s]Loading:  42%|████▏     | 26744/64000 [00:12<00:16, 2211.69it/s]Loading:  42%|████▏     | 26966/64000 [00:12<00:16, 2211.26it/s]Loading:  42%|████▏     | 27188/64000 [00:12<00:16, 2208.90it/s]Loading:  43%|████▎     | 27411/64000 [00:12<00:16, 2213.82it/s]Loading:  43%|████▎     | 27634/64000 [00:12<00:16, 2215.92it/s]Loading:  44%|████▎     | 27856/64000 [00:13<00:16, 2213.06it/s]Loading:  44%|████▍     | 28079/64000 [00:13<00:16, 2215.19it/s]Loading:  44%|████▍     | 28301/64000 [00:13<00:16, 2212.77it/s]Loading:  45%|████▍     | 28523/64000 [00:13<00:16, 2214.62it/s]Loading:  45%|████▍     | 28745/64000 [00:13<00:15, 2215.27it/s]Loading:  45%|████▌     | 28967/64000 [00:13<00:15, 2210.26it/s]Loading:  46%|████▌     | 29190/64000 [00:13<00:15, 2214.51it/s]Loading:  46%|████▌     | 29412/64000 [00:13<00:15, 2213.77it/s]Loading:  46%|████▋     | 29634/64000 [00:13<00:15, 2215.10it/s]Loading:  47%|████▋     | 29856/64000 [00:13<00:15, 2215.22it/s]Loading:  47%|████▋     | 30078/64000 [00:14<00:15, 2215.21it/s]Loading:  47%|████▋     | 30301/64000 [00:14<00:15, 2218.96it/s]Loading:  48%|████▊     | 30523/64000 [00:14<00:15, 2218.99it/s]Loading:  48%|████▊     | 30746/64000 [00:14<00:14, 2222.19it/s]Loading:  48%|████▊     | 30971/64000 [00:14<00:14, 2228.81it/s]Loading:  49%|████▊     | 31195/64000 [00:14<00:14, 2229.46it/s]Loading:  49%|████▉     | 31418/64000 [00:14<00:14, 2227.56it/s]Loading:  49%|████▉     | 31641/64000 [00:14<00:14, 2222.97it/s]Loading:  50%|████▉     | 31864/64000 [00:14<00:14, 2223.81it/s]Loading:  50%|█████     | 32087/64000 [00:14<00:14, 2224.35it/s]Loading:  50%|█████     | 32310/64000 [00:15<00:14, 2223.34it/s]Loading:  51%|█████     | 32533/64000 [00:15<00:14, 2222.19it/s]Loading:  51%|█████     | 32756/64000 [00:15<00:14, 2214.67it/s]Loading:  52%|█████▏    | 32979/64000 [00:15<00:13, 2217.01it/s]Loading:  52%|█████▏    | 33201/64000 [00:15<00:13, 2215.76it/s]Loading:  52%|█████▏    | 33423/64000 [00:15<00:13, 2216.37it/s]Loading:  53%|█████▎    | 33645/64000 [00:15<00:13, 2216.66it/s]Loading:  53%|█████▎    | 33867/64000 [00:15<00:13, 2213.65it/s]Loading:  53%|█████▎    | 34089/64000 [00:15<00:13, 2215.35it/s]Loading:  54%|█████▎    | 34311/64000 [00:15<00:13, 2210.15it/s]Loading:  54%|█████▍    | 34534/64000 [00:16<00:13, 2213.22it/s]Loading:  54%|█████▍    | 34757/64000 [00:16<00:13, 2218.15it/s]Loading:  55%|█████▍    | 34979/64000 [00:16<00:22, 1308.36it/s]Loading:  55%|█████▌    | 35201/64000 [00:16<00:19, 1491.17it/s]Loading:  55%|█████▌    | 35423/64000 [00:16<00:17, 1652.55it/s]Loading:  56%|█████▌    | 35644/64000 [00:16<00:15, 1786.80it/s]Loading:  56%|█████▌    | 35867/64000 [00:16<00:14, 1899.02it/s]Loading:  56%|█████▋    | 36089/64000 [00:16<00:14, 1983.52it/s]Loading:  57%|█████▋    | 36312/64000 [00:17<00:13, 2049.91it/s]Loading:  57%|█████▋    | 36536/64000 [00:17<00:13, 2101.40it/s]Loading:  57%|█████▋    | 36759/64000 [00:17<00:12, 2136.73it/s]Loading:  58%|█████▊    | 36982/64000 [00:17<00:12, 2161.68it/s]Loading:  58%|█████▊    | 37203/64000 [00:17<00:12, 2174.58it/s]Loading:  58%|█████▊    | 37425/64000 [00:17<00:12, 2187.37it/s]Loading:  59%|█████▉    | 37647/64000 [00:17<00:11, 2196.82it/s]Loading:  59%|█████▉    | 37870/64000 [00:17<00:11, 2204.01it/s]Loading:  60%|█████▉    | 38092/64000 [00:17<00:11, 2208.75it/s]Loading:  60%|█████▉    | 38314/64000 [00:17<00:11, 2211.77it/s]Loading:  60%|██████    | 38537/64000 [00:18<00:11, 2215.66it/s]Loading:  61%|██████    | 38759/64000 [00:18<00:11, 2212.99it/s]Loading:  61%|██████    | 38982/64000 [00:18<00:11, 2215.97it/s]Loading:  61%|██████▏   | 39206/64000 [00:18<00:11, 2221.21it/s]Loading:  62%|██████▏   | 39429/64000 [00:18<00:11, 2216.80it/s]Loading:  62%|██████▏   | 39652/64000 [00:18<00:10, 2218.73it/s]Loading:  62%|██████▏   | 39874/64000 [00:18<00:10, 2210.71it/s]Loading:  63%|██████▎   | 40096/64000 [00:18<00:10, 2207.79it/s]Loading:  63%|██████▎   | 40317/64000 [00:18<00:10, 2207.68it/s]Loading:  63%|██████▎   | 40538/64000 [00:18<00:10, 2206.04it/s]Loading:  64%|██████▎   | 40760/64000 [00:19<00:10, 2208.76it/s]Loading:  64%|██████▍   | 40981/64000 [00:19<00:10, 2204.65it/s]Loading:  64%|██████▍   | 41203/64000 [00:19<00:10, 2207.53it/s]Loading:  65%|██████▍   | 41425/64000 [00:19<00:10, 2209.02it/s]Loading:  65%|██████▌   | 41646/64000 [00:19<00:10, 2201.79it/s]Loading:  65%|██████▌   | 41868/64000 [00:19<00:10, 2206.98it/s]Loading:  66%|██████▌   | 42089/64000 [00:19<00:09, 2203.04it/s]Loading:  66%|██████▌   | 42310/64000 [00:19<00:09, 2204.79it/s]Loading:  66%|██████▋   | 42531/64000 [00:19<00:09, 2205.36it/s]Loading:  67%|██████▋   | 42752/64000 [00:20<00:09, 2199.66it/s]Loading:  67%|██████▋   | 42973/64000 [00:20<00:09, 2199.75it/s]Loading:  67%|██████▋   | 43193/64000 [00:20<00:09, 2196.48it/s]Loading:  68%|██████▊   | 43416/64000 [00:20<00:09, 2203.95it/s]Loading:  68%|██████▊   | 43637/64000 [00:20<00:09, 2203.11it/s]Loading:  69%|██████▊   | 43858/64000 [00:20<00:09, 2203.01it/s]Loading:  69%|██████▉   | 44081/64000 [00:20<00:09, 2209.82it/s]Loading:  69%|██████▉   | 44303/64000 [00:20<00:08, 2210.51it/s]Loading:  70%|██████▉   | 44525/64000 [00:20<00:08, 2207.64it/s]Loading:  70%|██████▉   | 44746/64000 [00:20<00:08, 2207.60it/s]Loading:  70%|███████   | 44967/64000 [00:21<00:08, 2203.35it/s]Loading:  71%|███████   | 45189/64000 [00:21<00:08, 2207.09it/s]Loading:  71%|███████   | 45410/64000 [00:21<00:08, 2206.37it/s]Loading:  71%|███████▏  | 45632/64000 [00:21<00:08, 2209.60it/s]Loading:  72%|███████▏  | 45853/64000 [00:21<00:08, 2206.45it/s]Loading:  72%|███████▏  | 46074/64000 [00:21<00:08, 2202.26it/s]Loading:  72%|███████▏  | 46296/64000 [00:21<00:08, 2205.89it/s]Loading:  73%|███████▎  | 46517/64000 [00:21<00:07, 2202.42it/s]Loading:  73%|███████▎  | 46738/64000 [00:21<00:07, 2203.94it/s]Loading:  73%|███████▎  | 46960/64000 [00:21<00:07, 2205.86it/s]Loading:  74%|███████▎  | 47181/64000 [00:22<00:07, 2200.37it/s]Loading:  74%|███████▍  | 47403/64000 [00:22<00:07, 2203.56it/s]Loading:  74%|███████▍  | 47624/64000 [00:22<00:07, 2200.62it/s]Loading:  75%|███████▍  | 47845/64000 [00:22<00:07, 2202.00it/s]Loading:  75%|███████▌  | 48067/64000 [00:22<00:07, 2205.57it/s]Loading:  75%|███████▌  | 48288/64000 [00:22<00:07, 2203.66it/s]Loading:  76%|███████▌  | 48510/64000 [00:22<00:07, 2206.66it/s]Loading:  76%|███████▌  | 48731/64000 [00:22<00:06, 2202.14it/s]Loading:  76%|███████▋  | 48952/64000 [00:22<00:06, 2203.62it/s]Loading:  77%|███████▋  | 49173/64000 [00:22<00:06, 2203.00it/s]Loading:  77%|███████▋  | 49394/64000 [00:23<00:06, 2203.43it/s]Loading:  78%|███████▊  | 49616/64000 [00:23<00:06, 2206.15it/s]Loading:  78%|███████▊  | 49837/64000 [00:23<00:06, 2201.95it/s]Loading:  78%|███████▊  | 50058/64000 [00:23<00:06, 2204.14it/s]Loading:  79%|███████▊  | 50279/64000 [00:23<00:06, 2205.23it/s]Loading:  79%|███████▉  | 50500/64000 [00:23<00:06, 2202.86it/s]Loading:  79%|███████▉  | 50721/64000 [00:23<00:06, 2203.37it/s]Loading:  80%|███████▉  | 50942/64000 [00:23<00:05, 2198.63it/s]Loading:  80%|███████▉  | 51162/64000 [00:24<00:10, 1174.23it/s]Loading:  80%|████████  | 51383/64000 [00:24<00:09, 1365.66it/s]Loading:  81%|████████  | 51604/64000 [00:24<00:08, 1541.37it/s]Loading:  81%|████████  | 51826/64000 [00:24<00:07, 1697.13it/s]Loading:  81%|████████▏ | 52047/64000 [00:24<00:06, 1823.27it/s]Loading:  82%|████████▏ | 52269/64000 [00:24<00:06, 1926.39it/s]Loading:  82%|████████▏ | 52492/64000 [00:24<00:05, 2006.54it/s]Loading:  82%|████████▏ | 52714/64000 [00:24<00:05, 2064.53it/s]Loading:  83%|████████▎ | 52938/64000 [00:24<00:05, 2113.35it/s]Loading:  83%|████████▎ | 53161/64000 [00:25<00:05, 2144.76it/s]Loading:  83%|████████▎ | 53384/64000 [00:25<00:04, 2168.69it/s]Loading:  84%|████████▍ | 53608/64000 [00:25<00:04, 2187.98it/s]Loading:  84%|████████▍ | 53830/64000 [00:25<00:04, 2194.47it/s]Loading:  84%|████████▍ | 54052/64000 [00:25<00:04, 2201.32it/s]Loading:  85%|████████▍ | 54274/64000 [00:25<00:04, 2202.17it/s]Loading:  85%|████████▌ | 54497/64000 [00:25<00:04, 2209.59it/s]Loading:  86%|████████▌ | 54720/64000 [00:25<00:04, 2214.98it/s]Loading:  86%|████████▌ | 54942/64000 [00:25<00:04, 2215.46it/s]Loading:  86%|████████▌ | 55166/64000 [00:25<00:03, 2220.19it/s]Loading:  87%|████████▋ | 55389/64000 [00:26<00:03, 2217.53it/s]Loading:  87%|████████▋ | 55612/64000 [00:26<00:03, 2218.76it/s]Loading:  87%|████████▋ | 55834/64000 [00:26<00:03, 2213.89it/s]Loading:  88%|████████▊ | 56056/64000 [00:26<00:03, 2204.97it/s]Loading:  88%|████████▊ | 56277/64000 [00:26<00:03, 2205.83it/s]Loading:  88%|████████▊ | 56498/64000 [00:26<00:03, 2207.03it/s]Loading:  89%|████████▊ | 56720/64000 [00:26<00:03, 2209.57it/s]Loading:  89%|████████▉ | 56942/64000 [00:26<00:03, 2212.55it/s]Loading:  89%|████████▉ | 57164/64000 [00:26<00:03, 2211.67it/s]Loading:  90%|████████▉ | 57386/64000 [00:26<00:02, 2213.44it/s]Loading:  90%|█████████ | 57608/64000 [00:27<00:02, 2214.43it/s]Loading:  90%|█████████ | 57831/64000 [00:27<00:02, 2218.88it/s]Loading:  91%|█████████ | 58053/64000 [00:27<00:02, 2216.57it/s]Loading:  91%|█████████ | 58275/64000 [00:27<00:02, 2212.42it/s]Loading:  91%|█████████▏| 58497/64000 [00:27<00:02, 2213.03it/s]Loading:  92%|█████████▏| 58719/64000 [00:27<00:02, 2212.46it/s]Loading:  92%|█████████▏| 58942/64000 [00:27<00:02, 2217.58it/s]Loading:  92%|█████████▏| 59166/64000 [00:27<00:02, 2224.14it/s]Loading:  93%|█████████▎| 59389/64000 [00:27<00:02, 2225.01it/s]Loading:  93%|█████████▎| 59613/64000 [00:27<00:01, 2228.00it/s]Loading:  93%|█████████▎| 59837/64000 [00:28<00:01, 2229.27it/s]Loading:  94%|█████████▍| 60060/64000 [00:28<00:01, 2228.87it/s]Loading:  94%|█████████▍| 60283/64000 [00:28<00:01, 2226.94it/s]Loading:  95%|█████████▍| 60506/64000 [00:28<00:01, 2219.59it/s]Loading:  95%|█████████▍| 60728/64000 [00:28<00:01, 2219.61it/s]Loading:  95%|█████████▌| 60950/64000 [00:28<00:01, 2214.86it/s]Loading:  96%|█████████▌| 61172/64000 [00:28<00:01, 2216.29it/s]Loading:  96%|█████████▌| 61394/64000 [00:28<00:01, 2214.92it/s]Loading:  96%|█████████▋| 61616/64000 [00:28<00:01, 2213.18it/s]Loading:  97%|█████████▋| 61839/64000 [00:28<00:00, 2216.33it/s]Loading:  97%|█████████▋| 62061/64000 [00:29<00:00, 2213.21it/s]Loading:  97%|█████████▋| 62285/64000 [00:29<00:00, 2218.61it/s]Loading:  98%|█████████▊| 62508/64000 [00:29<00:00, 2221.32it/s]Loading:  98%|█████████▊| 62731/64000 [00:29<00:00, 2219.12it/s]Loading:  98%|█████████▊| 62953/64000 [00:29<00:00, 2217.91it/s]Loading:  99%|█████████▊| 63175/64000 [00:29<00:00, 2213.20it/s]Loading:  99%|█████████▉| 63398/64000 [00:29<00:00, 2215.42it/s]Loading:  99%|█████████▉| 63621/64000 [00:29<00:00, 2219.71it/s]Loading: 100%|█████████▉| 63844/64000 [00:29<00:00, 2219.94it/s]Loading: 100%|██████████| 64000/64000 [00:29<00:00, 2140.38it/s]
0: DDStore add: ('trainset/dataset_name', 0, dtype('int64'), (64000, 1), 0, 0.000476837158203125)
0: DDStore add: ('trainset/edge_attr', 0, dtype('float32'), (13803296, 3), -0.002254486, 0.15426385402679443)
0: DDStore add: ('trainset/edge_index', 1, dtype('int64'), (13803296, 2), 259462034, 0.20568513870239258)
0: DDStore add: ('trainset/energy', 0, dtype('float32'), (64000, 1), -25358396.0, 0.0002384185791015625)
0: DDStore add: ('trainset/forces', 0, dtype('float32'), (987671, 3), -1.0877848e-05, 0.011038083583116531)
0: DDStore add: ('trainset/pos', 0, dtype('float32'), (987671, 3), 53710.555, 0.011038083583116531)
0: DDStore add: ('trainset/x', 0, dtype('float32'), (987671, 4), 3929915.0, 0.014717444777488708)
0: DDStore add: ('trainset/y', 0, dtype('float32'), (3027013, 1), -13.946869, 0.011276502162218094)
0: DDStore add: ('trainset/y_loc', 0, dtype('int64'), (64000, 3), 3091013, 0.001430511474609375)
0: DDStore total (GB): 0.41016487404704094
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 227/49018 [00:00<00:21, 2264.39it/s]Loading:   1%|          | 458/49018 [00:00<00:21, 2285.38it/s]Loading:   1%|▏         | 690/49018 [00:00<00:21, 2297.96it/s]Loading:   2%|▏         | 921/49018 [00:00<00:20, 2300.36it/s]Loading:   2%|▏         | 1154/49018 [00:00<00:20, 2309.29it/s]Loading:   3%|▎         | 1386/49018 [00:00<00:20, 2309.77it/s]Loading:   3%|▎         | 1619/49018 [00:00<00:20, 2314.03it/s]Loading:   4%|▍         | 1851/49018 [00:00<00:20, 2314.87it/s]Loading:   4%|▍         | 2083/49018 [00:00<00:20, 2312.48it/s]Loading:   5%|▍         | 2315/49018 [00:01<00:20, 2314.64it/s]Loading:   5%|▌         | 2547/49018 [00:01<00:20, 2314.73it/s]Loading:   6%|▌         | 2779/49018 [00:01<00:19, 2315.63it/s]Loading:   6%|▌         | 3011/49018 [00:01<00:19, 2314.84it/s]Loading:   7%|▋         | 3243/49018 [00:01<00:19, 2311.89it/s]Loading:   7%|▋         | 3476/49018 [00:01<00:19, 2315.97it/s]Loading:   8%|▊         | 3708/49018 [00:01<00:19, 2313.29it/s]Loading:   8%|▊         | 3941/49018 [00:01<00:19, 2315.49it/s]Loading:   9%|▊         | 4173/49018 [00:01<00:19, 2313.07it/s]Loading:   9%|▉         | 4406/49018 [00:01<00:19, 2315.57it/s]Loading:   9%|▉         | 4639/49018 [00:02<00:19, 2316.94it/s]Loading:  10%|▉         | 4871/49018 [00:02<00:19, 2315.24it/s]Loading:  10%|█         | 5104/49018 [00:02<00:18, 2318.91it/s]Loading:  11%|█         | 5336/49018 [00:02<00:18, 2316.09it/s]Loading:  11%|█▏        | 5569/49018 [00:02<00:18, 2318.08it/s]Loading:  12%|█▏        | 5801/49018 [00:02<00:18, 2317.05it/s]Loading:  12%|█▏        | 6033/49018 [00:02<00:18, 2316.28it/s]Loading:  13%|█▎        | 6265/49018 [00:02<00:18, 2315.09it/s]Loading:  13%|█▎        | 6497/49018 [00:02<00:18, 2310.47it/s]Loading:  14%|█▎        | 6729/49018 [00:02<00:18, 2310.95it/s]Loading:  14%|█▍        | 6961/49018 [00:03<00:18, 2306.29it/s]Loading:  15%|█▍        | 7193/49018 [00:03<00:18, 2308.29it/s]Loading:  15%|█▌        | 7425/49018 [00:03<00:18, 2309.19it/s]Loading:  16%|█▌        | 7656/49018 [00:03<00:17, 2305.32it/s]Loading:  16%|█▌        | 7887/49018 [00:03<00:17, 2304.57it/s]Loading:  17%|█▋        | 8118/49018 [00:03<00:17, 2303.12it/s]Loading:  17%|█▋        | 8349/49018 [00:03<00:17, 2303.70it/s]Loading:  18%|█▊        | 8580/49018 [00:03<00:17, 2301.08it/s]Loading:  18%|█▊        | 8811/49018 [00:03<00:17, 2301.75it/s]Loading:  18%|█▊        | 9042/49018 [00:03<00:17, 2300.79it/s]Loading:  19%|█▉        | 9273/49018 [00:04<00:17, 2297.29it/s]Loading:  19%|█▉        | 9504/49018 [00:04<00:17, 2298.81it/s]Loading:  20%|█▉        | 9734/49018 [00:04<00:17, 2295.83it/s]Loading:  20%|██        | 9964/49018 [00:04<00:17, 2296.91it/s]Loading:  21%|██        | 10194/49018 [00:04<00:16, 2297.17it/s]Loading:  21%|██▏       | 10424/49018 [00:04<00:16, 2294.15it/s]Loading:  22%|██▏       | 10654/49018 [00:04<00:16, 2295.17it/s]Loading:  22%|██▏       | 10884/49018 [00:04<00:16, 2292.05it/s]Loading:  23%|██▎       | 11114/49018 [00:04<00:16, 2292.36it/s]Loading:  23%|██▎       | 11344/49018 [00:04<00:16, 2290.60it/s]Loading:  24%|██▎       | 11574/49018 [00:05<00:16, 2290.67it/s]Loading:  24%|██▍       | 11805/49018 [00:05<00:16, 2295.34it/s]Loading:  25%|██▍       | 12035/49018 [00:05<00:16, 2293.40it/s]Loading:  25%|██▌       | 12266/49018 [00:05<00:16, 2296.05it/s]Loading:  25%|██▌       | 12496/49018 [00:05<00:15, 2295.71it/s]Loading:  26%|██▌       | 12727/49018 [00:05<00:15, 2298.09it/s]Loading:  26%|██▋       | 12959/49018 [00:05<00:15, 2302.83it/s]Loading:  27%|██▋       | 13190/49018 [00:05<00:15, 2290.72it/s]Loading:  27%|██▋       | 13420/49018 [00:05<00:15, 2274.88it/s]Loading:  28%|██▊       | 13648/49018 [00:05<00:15, 2262.53it/s]Loading:  28%|██▊       | 13875/49018 [00:06<00:15, 2258.22it/s]Loading:  29%|██▉       | 14101/49018 [00:06<00:15, 2247.26it/s]Loading:  29%|██▉       | 14326/49018 [00:06<00:15, 2246.40it/s]Loading:  30%|██▉       | 14551/49018 [00:06<00:15, 2243.32it/s]Loading:  30%|███       | 14776/49018 [00:06<00:15, 2241.34it/s]Loading:  31%|███       | 15001/49018 [00:06<00:15, 2241.59it/s]Loading:  31%|███       | 15226/49018 [00:06<00:15, 2237.38it/s]Loading:  32%|███▏      | 15451/49018 [00:06<00:14, 2238.94it/s]Loading:  32%|███▏      | 15676/49018 [00:06<00:14, 2239.84it/s]Loading:  32%|███▏      | 15901/49018 [00:06<00:14, 2241.35it/s]Loading:  33%|███▎      | 16126/49018 [00:07<00:14, 2241.60it/s]Loading:  33%|███▎      | 16351/49018 [00:07<00:14, 2237.66it/s]Loading:  34%|███▍      | 16575/49018 [00:07<00:14, 2236.14it/s]Loading:  34%|███▍      | 16799/49018 [00:07<00:14, 2236.26it/s]Loading:  35%|███▍      | 17023/49018 [00:07<00:14, 2236.52it/s]Loading:  35%|███▌      | 17249/49018 [00:07<00:14, 2240.53it/s]Loading:  36%|███▌      | 17474/49018 [00:07<00:14, 2238.87it/s]Loading:  36%|███▌      | 17699/49018 [00:07<00:13, 2240.00it/s]Loading:  37%|███▋      | 17923/49018 [00:07<00:13, 2239.29it/s]Loading:  37%|███▋      | 18147/49018 [00:08<00:29, 1052.34it/s]Loading:  37%|███▋      | 18369/49018 [00:08<00:24, 1247.63it/s]Loading:  38%|███▊      | 18591/49018 [00:08<00:21, 1434.75it/s]Loading:  38%|███▊      | 18815/49018 [00:08<00:18, 1607.73it/s]Loading:  39%|███▉      | 19039/49018 [00:08<00:17, 1755.74it/s]Loading:  39%|███▉      | 19263/49018 [00:08<00:15, 1875.92it/s]Loading:  40%|███▉      | 19486/49018 [00:08<00:15, 1968.76it/s]Loading:  40%|████      | 19709/49018 [00:09<00:14, 2040.17it/s]Loading:  41%|████      | 19933/49018 [00:09<00:13, 2095.70it/s]Loading:  41%|████      | 20158/49018 [00:09<00:13, 2137.76it/s]Loading:  42%|████▏     | 20381/49018 [00:09<00:13, 2163.28it/s]Loading:  42%|████▏     | 20606/49018 [00:09<00:12, 2186.90it/s]Loading:  42%|████▏     | 20829/49018 [00:09<00:12, 2198.77it/s]Loading:  43%|████▎     | 21053/49018 [00:09<00:12, 2209.98it/s]Loading:  43%|████▎     | 21278/49018 [00:09<00:12, 2218.96it/s]Loading:  44%|████▍     | 21502/49018 [00:09<00:12, 2220.48it/s]Loading:  44%|████▍     | 21726/49018 [00:09<00:12, 2225.57it/s]Loading:  45%|████▍     | 21950/49018 [00:10<00:12, 2224.56it/s]Loading:  45%|████▌     | 22174/49018 [00:10<00:12, 2227.70it/s]Loading:  46%|████▌     | 22398/49018 [00:10<00:11, 2227.56it/s]Loading:  46%|████▌     | 22622/49018 [00:10<00:11, 2231.18it/s]Loading:  47%|████▋     | 22847/49018 [00:10<00:11, 2234.23it/s]Loading:  47%|████▋     | 23071/49018 [00:10<00:11, 2230.75it/s]Loading:  48%|████▊     | 23296/49018 [00:10<00:11, 2235.57it/s]Loading:  48%|████▊     | 23520/49018 [00:10<00:11, 2233.22it/s]Loading:  48%|████▊     | 23744/49018 [00:10<00:11, 2234.81it/s]Loading:  49%|████▉     | 23969/49018 [00:10<00:11, 2236.69it/s]Loading:  49%|████▉     | 24193/49018 [00:11<00:11, 2233.08it/s]Loading:  50%|████▉     | 24418/49018 [00:11<00:10, 2237.72it/s]Loading:  50%|█████     | 24642/49018 [00:11<00:10, 2233.71it/s]Loading:  51%|█████     | 24866/49018 [00:11<00:10, 2232.97it/s]Loading:  51%|█████     | 25091/49018 [00:11<00:10, 2236.73it/s]Loading:  52%|█████▏    | 25315/49018 [00:11<00:10, 2235.10it/s]Loading:  52%|█████▏    | 25540/49018 [00:11<00:10, 2238.10it/s]Loading:  53%|█████▎    | 25764/49018 [00:11<00:10, 2235.91it/s]Loading:  53%|█████▎    | 25988/49018 [00:11<00:10, 2236.45it/s]Loading:  53%|█████▎    | 26213/49018 [00:11<00:10, 2237.97it/s]Loading:  54%|█████▍    | 26437/49018 [00:12<00:10, 2234.98it/s]Loading:  54%|█████▍    | 26661/49018 [00:12<00:09, 2236.23it/s]Loading:  55%|█████▍    | 26885/49018 [00:12<00:09, 2235.68it/s]Loading:  55%|█████▌    | 27110/49018 [00:12<00:09, 2237.35it/s]Loading:  56%|█████▌    | 27335/49018 [00:12<00:09, 2238.67it/s]Loading:  56%|█████▌    | 27559/49018 [00:12<00:09, 2235.18it/s]Loading:  57%|█████▋    | 27783/49018 [00:12<00:09, 2234.98it/s]Loading:  57%|█████▋    | 28007/49018 [00:12<00:09, 2233.50it/s]Loading:  58%|█████▊    | 28231/49018 [00:12<00:09, 2233.17it/s]Loading:  58%|█████▊    | 28455/49018 [00:12<00:09, 2234.96it/s]Loading:  59%|█████▊    | 28679/49018 [00:13<00:09, 2232.35it/s]Loading:  59%|█████▉    | 28903/49018 [00:13<00:09, 2234.10it/s]Loading:  59%|█████▉    | 29127/49018 [00:13<00:08, 2232.34it/s]Loading:  60%|█████▉    | 29352/49018 [00:13<00:08, 2235.72it/s]Loading:  60%|██████    | 29576/49018 [00:13<00:08, 2233.14it/s]Loading:  61%|██████    | 29801/49018 [00:13<00:08, 2235.71it/s]Loading:  61%|██████▏   | 30025/49018 [00:13<00:08, 2236.36it/s]Loading:  62%|██████▏   | 30249/49018 [00:13<00:08, 2232.58it/s]Loading:  62%|██████▏   | 30473/49018 [00:13<00:08, 2234.78it/s]Loading:  63%|██████▎   | 30697/49018 [00:13<00:08, 2233.30it/s]Loading:  63%|██████▎   | 30922/49018 [00:14<00:08, 2237.99it/s]Loading:  64%|██████▎   | 31147/49018 [00:14<00:07, 2238.68it/s]Loading:  64%|██████▍   | 31371/49018 [00:14<00:07, 2233.46it/s]Loading:  64%|██████▍   | 31595/49018 [00:14<00:07, 2233.40it/s]Loading:  65%|██████▍   | 31819/49018 [00:14<00:07, 2231.16it/s]Loading:  65%|██████▌   | 32043/49018 [00:14<00:07, 2231.61it/s]Loading:  66%|██████▌   | 32267/49018 [00:14<00:07, 2233.76it/s]Loading:  66%|██████▋   | 32491/49018 [00:14<00:07, 2230.41it/s]Loading:  67%|██████▋   | 32715/49018 [00:14<00:07, 2232.46it/s]Loading:  67%|██████▋   | 32939/49018 [00:14<00:07, 2232.40it/s]Loading:  68%|██████▊   | 33164/49018 [00:15<00:07, 2235.27it/s]Loading:  68%|██████▊   | 33389/49018 [00:15<00:06, 2237.23it/s]Loading:  69%|██████▊   | 33613/49018 [00:15<00:06, 2233.99it/s]Loading:  69%|██████▉   | 33837/49018 [00:15<00:06, 2234.11it/s]Loading:  69%|██████▉   | 34061/49018 [00:15<00:06, 2228.72it/s]Loading:  70%|██████▉   | 34286/49018 [00:15<00:06, 2232.80it/s]Loading:  70%|███████   | 34510/49018 [00:15<00:06, 2232.16it/s]Loading:  71%|███████   | 34734/49018 [00:15<00:06, 2231.31it/s]Loading:  71%|███████▏  | 34958/49018 [00:15<00:06, 2232.91it/s]Loading:  72%|███████▏  | 35182/49018 [00:15<00:06, 2231.04it/s]Loading:  72%|███████▏  | 35406/49018 [00:16<00:06, 2233.69it/s]Loading:  73%|███████▎  | 35630/49018 [00:16<00:05, 2235.49it/s]Loading:  73%|███████▎  | 35855/49018 [00:16<00:05, 2239.70it/s]Loading:  74%|███████▎  | 36081/49018 [00:16<00:05, 2243.12it/s]Loading:  74%|███████▍  | 36306/49018 [00:16<00:05, 2237.68it/s]Loading:  75%|███████▍  | 36531/49018 [00:16<00:05, 2240.61it/s]Loading:  75%|███████▍  | 36756/49018 [00:16<00:05, 2236.22it/s]Loading:  75%|███████▌  | 36981/49018 [00:16<00:05, 2237.51it/s]Loading:  76%|███████▌  | 37206/49018 [00:16<00:05, 2239.09it/s]Loading:  76%|███████▋  | 37430/49018 [00:16<00:05, 2235.69it/s]Loading:  77%|███████▋  | 37654/49018 [00:17<00:05, 2231.40it/s]Loading:  77%|███████▋  | 37878/49018 [00:17<00:04, 2231.17it/s]Loading:  78%|███████▊  | 38102/49018 [00:17<00:04, 2233.45it/s]Loading:  78%|███████▊  | 38327/49018 [00:17<00:04, 2236.38it/s]Loading:  79%|███████▊  | 38551/49018 [00:17<00:04, 2230.58it/s]Loading:  79%|███████▉  | 38776/49018 [00:17<00:04, 2234.64it/s]Loading:  80%|███████▉  | 39000/49018 [00:17<00:04, 2232.34it/s]Loading:  80%|████████  | 39226/49018 [00:17<00:04, 2238.24it/s]Loading:  80%|████████  | 39451/49018 [00:17<00:04, 2239.65it/s]Loading:  81%|████████  | 39675/49018 [00:17<00:04, 2234.31it/s]Loading:  81%|████████▏ | 39899/49018 [00:18<00:04, 2233.64it/s]Loading:  82%|████████▏ | 40123/49018 [00:18<00:03, 2234.26it/s]Loading:  82%|████████▏ | 40348/49018 [00:18<00:03, 2236.40it/s]Loading:  83%|████████▎ | 40572/49018 [00:18<00:03, 2233.53it/s]Loading:  83%|████████▎ | 40796/49018 [00:18<00:03, 2228.00it/s]Loading:  84%|████████▎ | 41021/49018 [00:18<00:03, 2232.40it/s]Loading:  84%|████████▍ | 41245/49018 [00:18<00:03, 2233.03it/s]Loading:  85%|████████▍ | 41469/49018 [00:18<00:03, 2234.77it/s]Loading:  85%|████████▌ | 41693/49018 [00:18<00:03, 2233.91it/s]Loading:  86%|████████▌ | 41917/49018 [00:18<00:03, 2228.49it/s]Loading:  86%|████████▌ | 42141/49018 [00:19<00:03, 2228.94it/s]Loading:  86%|████████▋ | 42364/49018 [00:19<00:02, 2227.74it/s]Loading:  87%|████████▋ | 42589/49018 [00:19<00:02, 2233.78it/s]Loading:  87%|████████▋ | 42813/49018 [00:19<00:02, 2233.94it/s]Loading:  88%|████████▊ | 43037/49018 [00:19<00:02, 2229.37it/s]Loading:  88%|████████▊ | 43261/49018 [00:19<00:02, 2231.50it/s]Loading:  89%|████████▊ | 43485/49018 [00:19<00:02, 2225.29it/s]Loading:  89%|████████▉ | 43708/49018 [00:19<00:02, 2226.05it/s]Loading:  90%|████████▉ | 43931/49018 [00:19<00:02, 2224.70it/s]Loading:  90%|█████████ | 44154/49018 [00:19<00:02, 2224.26it/s]Loading:  91%|█████████ | 44378/49018 [00:20<00:02, 2228.59it/s]Loading:  91%|█████████ | 44601/49018 [00:20<00:01, 2226.58it/s]Loading:  91%|█████████▏| 44824/49018 [00:20<00:01, 2227.23it/s]Loading:  92%|█████████▏| 45047/49018 [00:20<00:01, 2224.41it/s]Loading:  92%|█████████▏| 45270/49018 [00:20<00:01, 2223.71it/s]Loading:  93%|█████████▎| 45493/49018 [00:20<00:01, 2223.46it/s]Loading:  93%|█████████▎| 45716/49018 [00:21<00:03, 934.06it/s] Loading:  94%|█████████▎| 45938/49018 [00:21<00:02, 1129.22it/s]Loading:  94%|█████████▍| 46160/49018 [00:21<00:02, 1323.66it/s]Loading:  95%|█████████▍| 46382/49018 [00:21<00:01, 1504.47it/s]Loading:  95%|█████████▌| 46605/49018 [00:21<00:01, 1666.31it/s]Loading:  96%|█████████▌| 46827/49018 [00:21<00:01, 1800.74it/s]Loading:  96%|█████████▌| 47051/49018 [00:21<00:01, 1912.84it/s]Loading:  96%|█████████▋| 47274/49018 [00:21<00:00, 1996.83it/s]Loading:  97%|█████████▋| 47496/49018 [00:21<00:00, 2058.01it/s]Loading:  97%|█████████▋| 47719/49018 [00:22<00:00, 2105.97it/s]Loading:  98%|█████████▊| 47941/49018 [00:22<00:00, 2138.14it/s]Loading:  98%|█████████▊| 48165/49018 [00:22<00:00, 2165.82it/s]Loading:  99%|█████████▊| 48387/49018 [00:22<00:00, 2180.25it/s]Loading:  99%|█████████▉| 48611/49018 [00:22<00:00, 2195.79it/s]Loading: 100%|█████████▉| 48835/49018 [00:22<00:00, 2207.66it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2168.08it/s]
0: DDStore add: ('valset/dataset_name', 0, dtype('int64'), (41300, 1), 0, 0.0003077089786529541)
0: DDStore add: ('valset/edge_attr', 0, dtype('float32'), (8761274, 3), 0.0005569458, 0.09791486710309982)
0: DDStore add: ('valset/edge_index', 1, dtype('int64'), (8761274, 2), 162400348, 0.13055315613746643)
0: DDStore add: ('valset/energy', 0, dtype('float32'), (41300, 1), -16276977.0, 0.00015385448932647705)
0: DDStore add: ('valset/forces', 0, dtype('float32'), (634552, 3), 7.5101852e-06, 0.0070916712284088135)
0: DDStore add: ('valset/pos', 0, dtype('float32'), (634552, 3), 34309.84, 0.0070916712284088135)
0: DDStore add: ('valset/x', 0, dtype('float32'), (634552, 4), 2523605.5, 0.009455561637878418)
0: DDStore add: ('valset/y', 0, dtype('float32'), (1944956, 1), -31.46448, 0.0072455257177352905)
0: DDStore add: ('valset/y_loc', 0, dtype('int64'), (41300, 3), 1986256, 0.0009231269359588623)
0: DDStore total (GB): 0.2607371434569359
Loading:   0%|          | 0/49018 [00:00<?, ?it/s]Loading:   0%|          | 229/49018 [00:00<00:21, 2285.39it/s]Loading:   1%|          | 461/49018 [00:00<00:21, 2305.07it/s]Loading:   1%|▏         | 693/49018 [00:00<00:20, 2309.68it/s]Loading:   2%|▏         | 926/49018 [00:00<00:20, 2315.81it/s]Loading:   2%|▏         | 1159/49018 [00:00<00:20, 2319.98it/s]Loading:   3%|▎         | 1391/49018 [00:00<00:20, 2315.69it/s]Loading:   3%|▎         | 1624/49018 [00:00<00:20, 2318.22it/s]Loading:   4%|▍         | 1856/49018 [00:01<00:51, 913.25it/s] Loading:   4%|▍         | 2085/49018 [00:01<00:41, 1121.99it/s]Loading:   5%|▍         | 2317/49018 [00:01<00:34, 1335.15it/s]Loading:   5%|▌         | 2549/49018 [00:01<00:30, 1534.21it/s]Loading:   6%|▌         | 2782/49018 [00:01<00:27, 1712.25it/s]Loading:   6%|▌         | 3014/49018 [00:01<00:24, 1858.84it/s]Loading:   7%|▋         | 3248/49018 [00:01<00:23, 1981.05it/s]Loading:   7%|▋         | 3481/49018 [00:01<00:21, 2074.44it/s]Loading:   8%|▊         | 3713/49018 [00:02<00:21, 2141.13it/s]Loading:   8%|▊         | 3946/49018 [00:02<00:20, 2193.27it/s]Loading:   9%|▊         | 4176/49018 [00:02<00:20, 2220.08it/s]Loading:   9%|▉         | 4407/49018 [00:02<00:19, 2244.24it/s]Loading:   9%|▉         | 4637/49018 [00:02<00:19, 2256.84it/s]Loading:  10%|▉         | 4868/49018 [00:02<00:19, 2269.71it/s]Loading:  10%|█         | 5099/49018 [00:02<00:19, 2279.69it/s]Loading:  11%|█         | 5329/49018 [00:02<00:19, 2283.06it/s]Loading:  11%|█▏        | 5559/49018 [00:02<00:19, 2284.17it/s]Loading:  12%|█▏        | 5789/49018 [00:02<00:18, 2281.10it/s]Loading:  12%|█▏        | 6018/49018 [00:03<00:18, 2283.67it/s]Loading:  13%|█▎        | 6247/49018 [00:03<00:18, 2285.34it/s]Loading:  13%|█▎        | 6476/49018 [00:03<00:18, 2284.80it/s]Loading:  14%|█▎        | 6708/49018 [00:03<00:18, 2293.17it/s]Loading:  14%|█▍        | 6938/49018 [00:03<00:18, 2292.26it/s]Loading:  15%|█▍        | 7168/49018 [00:03<00:18, 2294.08it/s]Loading:  15%|█▌        | 7398/49018 [00:03<00:18, 2289.62it/s]Loading:  16%|█▌        | 7629/49018 [00:03<00:18, 2293.11it/s]Loading:  16%|█▌        | 7860/49018 [00:03<00:17, 2298.08it/s]Loading:  17%|█▋        | 8090/49018 [00:03<00:17, 2297.67it/s]Loading:  17%|█▋        | 8322/49018 [00:04<00:17, 2302.24it/s]Loading:  17%|█▋        | 8553/49018 [00:04<00:17, 2300.21it/s]Loading:  18%|█▊        | 8784/49018 [00:04<00:17, 2299.91it/s]Loading:  18%|█▊        | 9014/49018 [00:04<00:17, 2298.31it/s]Loading:  19%|█▉        | 9245/49018 [00:04<00:17, 2300.64it/s]Loading:  19%|█▉        | 9476/49018 [00:04<00:17, 2302.04it/s]Loading:  20%|█▉        | 9707/49018 [00:04<00:17, 2298.92it/s]Loading:  20%|██        | 9938/49018 [00:04<00:16, 2301.86it/s]Loading:  21%|██        | 10169/49018 [00:04<00:16, 2298.48it/s]Loading:  21%|██        | 10400/49018 [00:04<00:16, 2301.06it/s]Loading:  22%|██▏       | 10631/49018 [00:05<00:16, 2300.54it/s]Loading:  22%|██▏       | 10862/49018 [00:05<00:16, 2297.64it/s]Loading:  23%|██▎       | 11094/49018 [00:05<00:16, 2303.13it/s]Loading:  23%|██▎       | 11325/49018 [00:05<00:16, 2300.69it/s]Loading:  24%|██▎       | 11557/49018 [00:05<00:16, 2305.34it/s]Loading:  24%|██▍       | 11788/49018 [00:05<00:16, 2301.86it/s]Loading:  25%|██▍       | 12019/49018 [00:05<00:16, 2301.54it/s]Loading:  25%|██▍       | 12250/49018 [00:05<00:15, 2302.91it/s]Loading:  25%|██▌       | 12481/49018 [00:05<00:15, 2300.10it/s]Loading:  26%|██▌       | 12713/49018 [00:06<00:15, 2303.29it/s]Loading:  26%|██▋       | 12944/49018 [00:06<00:15, 2303.88it/s]Loading:  27%|██▋       | 13175/49018 [00:06<00:15, 2301.93it/s]Loading:  27%|██▋       | 13406/49018 [00:06<00:15, 2301.80it/s]Loading:  28%|██▊       | 13637/49018 [00:06<00:15, 2298.33it/s]Loading:  28%|██▊       | 13867/49018 [00:06<00:15, 2297.46it/s]Loading:  29%|██▉       | 14097/49018 [00:06<00:15, 2296.02it/s]Loading:  29%|██▉       | 14328/49018 [00:06<00:15, 2300.04it/s]Loading:  30%|██▉       | 14559/49018 [00:06<00:15, 2297.23it/s]Loading:  30%|███       | 14790/49018 [00:06<00:14, 2299.89it/s]Loading:  31%|███       | 15021/49018 [00:07<00:14, 2301.43it/s]Loading:  31%|███       | 15252/49018 [00:07<00:14, 2296.87it/s]Loading:  32%|███▏      | 15482/49018 [00:07<00:14, 2296.75it/s]Loading:  32%|███▏      | 15712/49018 [00:07<00:14, 2293.09it/s]Loading:  33%|███▎      | 15942/49018 [00:07<00:14, 2293.71it/s]Loading:  33%|███▎      | 16172/49018 [00:07<00:14, 2294.86it/s]Loading:  33%|███▎      | 16402/49018 [00:07<00:14, 2292.58it/s]Loading:  34%|███▍      | 16633/49018 [00:07<00:14, 2295.09it/s]Loading:  34%|███▍      | 16863/49018 [00:07<00:14, 2289.44it/s]Loading:  35%|███▍      | 17093/49018 [00:07<00:13, 2290.95it/s]Loading:  35%|███▌      | 17323/49018 [00:08<00:13, 2287.55it/s]Loading:  36%|███▌      | 17553/49018 [00:08<00:13, 2288.56it/s]Loading:  36%|███▋      | 17783/49018 [00:08<00:13, 2290.79it/s]Loading:  37%|███▋      | 18013/49018 [00:08<00:13, 2282.33it/s]Loading:  37%|███▋      | 18242/49018 [00:08<00:13, 2268.56it/s]Loading:  38%|███▊      | 18469/49018 [00:08<00:13, 2250.58it/s]Loading:  38%|███▊      | 18695/49018 [00:08<00:13, 2239.94it/s]Loading:  39%|███▊      | 18920/49018 [00:08<00:13, 2237.83it/s]Loading:  39%|███▉      | 19144/49018 [00:08<00:13, 2228.38it/s]Loading:  40%|███▉      | 19369/49018 [00:08<00:13, 2233.21it/s]Loading:  40%|███▉      | 19593/49018 [00:09<00:13, 2232.51it/s]Loading:  40%|████      | 19818/49018 [00:09<00:13, 2236.06it/s]Loading:  41%|████      | 20042/49018 [00:09<00:12, 2235.15it/s]Loading:  41%|████▏     | 20266/49018 [00:09<00:12, 2229.19it/s]Loading:  42%|████▏     | 20490/49018 [00:09<00:12, 2231.16it/s]Loading:  42%|████▏     | 20714/49018 [00:09<00:12, 2228.43it/s]Loading:  43%|████▎     | 20938/49018 [00:09<00:12, 2230.47it/s]Loading:  43%|████▎     | 21162/49018 [00:09<00:12, 2225.92it/s]Loading:  44%|████▎     | 21386/49018 [00:09<00:12, 2228.74it/s]Loading:  44%|████▍     | 21610/49018 [00:09<00:12, 2230.50it/s]Loading:  45%|████▍     | 21834/49018 [00:10<00:12, 2226.75it/s]Loading:  45%|████▍     | 22058/49018 [00:10<00:12, 2229.84it/s]Loading:  45%|████▌     | 22281/49018 [00:10<00:11, 2228.48it/s]Loading:  46%|████▌     | 22505/49018 [00:10<00:11, 2230.80it/s]Loading:  46%|████▋     | 22729/49018 [00:10<00:11, 2232.32it/s]Loading:  47%|████▋     | 22953/49018 [00:10<00:11, 2231.54it/s]Loading:  47%|████▋     | 23177/49018 [00:10<00:11, 2232.06it/s]Loading:  48%|████▊     | 23401/49018 [00:10<00:11, 2229.83it/s]Loading:  48%|████▊     | 23624/49018 [00:10<00:11, 2229.60it/s]Loading:  49%|████▊     | 23847/49018 [00:10<00:11, 2228.91it/s]Loading:  49%|████▉     | 24070/49018 [00:11<00:11, 2225.65it/s]Loading:  50%|████▉     | 24294/49018 [00:11<00:11, 2228.38it/s]Loading:  50%|█████     | 24517/49018 [00:11<00:11, 2225.49it/s]Loading:  50%|█████     | 24741/49018 [00:11<00:10, 2226.90it/s]Loading:  51%|█████     | 24964/49018 [00:11<00:10, 2226.78it/s]Loading:  51%|█████▏    | 25187/49018 [00:11<00:10, 2220.11it/s]Loading:  52%|█████▏    | 25411/49018 [00:11<00:10, 2223.63it/s]Loading:  52%|█████▏    | 25634/49018 [00:11<00:10, 2224.09it/s]Loading:  53%|█████▎    | 25858/49018 [00:11<00:10, 2227.32it/s]Loading:  53%|█████▎    | 26082/49018 [00:11<00:10, 2229.93it/s]Loading:  54%|█████▎    | 26305/49018 [00:12<00:10, 2224.82it/s]Loading:  54%|█████▍    | 26528/49018 [00:12<00:10, 2222.38it/s]Loading:  55%|█████▍    | 26751/49018 [00:12<00:10, 2215.60it/s]Loading:  55%|█████▌    | 26974/49018 [00:12<00:09, 2218.09it/s]Loading:  55%|█████▌    | 27197/49018 [00:12<00:09, 2220.93it/s]Loading:  56%|█████▌    | 27420/49018 [00:12<00:09, 2216.83it/s]Loading:  56%|█████▋    | 27643/49018 [00:12<00:09, 2218.96it/s]Loading:  57%|█████▋    | 27865/49018 [00:12<00:09, 2218.89it/s]Loading:  57%|█████▋    | 28090/49018 [00:12<00:09, 2227.33it/s]Loading:  58%|█████▊    | 28314/49018 [00:12<00:09, 2228.81it/s]Loading:  58%|█████▊    | 28537/49018 [00:13<00:09, 2223.26it/s]Loading:  59%|█████▊    | 28760/49018 [00:13<00:09, 2222.62it/s]Loading:  59%|█████▉    | 28983/49018 [00:13<00:09, 2217.44it/s]Loading:  60%|█████▉    | 29207/49018 [00:13<00:08, 2221.18it/s]Loading:  60%|██████    | 29431/49018 [00:13<00:08, 2225.00it/s]Loading:  60%|██████    | 29654/49018 [00:13<00:08, 2222.04it/s]Loading:  61%|██████    | 29878/49018 [00:13<00:08, 2226.16it/s]Loading:  61%|██████▏   | 30101/49018 [00:13<00:08, 2223.13it/s]Loading:  62%|██████▏   | 30324/49018 [00:13<00:08, 2224.58it/s]Loading:  62%|██████▏   | 30547/49018 [00:13<00:08, 2217.21it/s]Loading:  63%|██████▎   | 30770/49018 [00:14<00:08, 2219.20it/s]Loading:  63%|██████▎   | 30994/49018 [00:14<00:08, 2222.67it/s]Loading:  64%|██████▎   | 31217/49018 [00:14<00:08, 2220.99it/s]Loading:  64%|██████▍   | 31441/49018 [00:14<00:07, 2225.14it/s]Loading:  65%|██████▍   | 31664/49018 [00:14<00:07, 2216.89it/s]Loading:  65%|██████▌   | 31886/49018 [00:14<00:07, 2216.14it/s]Loading:  66%|██████▌   | 32111/49018 [00:14<00:07, 2223.86it/s]Loading:  66%|██████▌   | 32334/49018 [00:14<00:07, 2219.95it/s]Loading:  66%|██████▋   | 32557/49018 [00:14<00:07, 2221.99it/s]Loading:  67%|██████▋   | 32780/49018 [00:14<00:07, 2222.78it/s]Loading:  67%|██████▋   | 33005/49018 [00:15<00:07, 2228.33it/s]Loading:  68%|██████▊   | 33228/49018 [00:15<00:07, 2228.49it/s]Loading:  68%|██████▊   | 33451/49018 [00:15<00:07, 2223.64it/s]Loading:  69%|██████▊   | 33676/49018 [00:15<00:06, 2230.52it/s]Loading:  69%|██████▉   | 33900/49018 [00:15<00:06, 2230.06it/s]Loading:  70%|██████▉   | 34125/49018 [00:15<00:06, 2235.77it/s]Loading:  70%|███████   | 34349/49018 [00:15<00:06, 2236.97it/s]Loading:  71%|███████   | 34573/49018 [00:15<00:06, 2225.82it/s]Loading:  71%|███████   | 34797/49018 [00:15<00:06, 2228.31it/s]Loading:  71%|███████▏  | 35020/49018 [00:15<00:06, 2223.32it/s]Loading:  72%|███████▏  | 35244/49018 [00:16<00:06, 2227.57it/s]Loading:  72%|███████▏  | 35467/49018 [00:16<00:06, 2227.52it/s]Loading:  73%|███████▎  | 35690/49018 [00:16<00:05, 2225.42it/s]Loading:  73%|███████▎  | 35914/49018 [00:16<00:05, 2228.64it/s]Loading:  74%|███████▎  | 36137/49018 [00:16<00:05, 2227.96it/s]Loading:  74%|███████▍  | 36360/49018 [00:16<00:05, 2226.77it/s]Loading:  75%|███████▍  | 36583/49018 [00:16<00:05, 2226.59it/s]Loading:  75%|███████▌  | 36806/49018 [00:16<00:05, 2222.53it/s]Loading:  76%|███████▌  | 37031/49018 [00:16<00:05, 2228.28it/s]Loading:  76%|███████▌  | 37254/49018 [00:16<00:05, 2226.67it/s]Loading:  76%|███████▋  | 37479/49018 [00:17<00:05, 2232.87it/s]Loading:  77%|███████▋  | 37703/49018 [00:17<00:05, 2231.52it/s]Loading:  77%|███████▋  | 37927/49018 [00:17<00:13, 798.92it/s] Loading:  78%|███████▊  | 38149/49018 [00:17<00:11, 986.90it/s]Loading:  78%|███████▊  | 38370/49018 [00:18<00:09, 1181.48it/s]Loading:  79%|███████▊  | 38594/49018 [00:18<00:07, 1376.66it/s]Loading:  79%|███████▉  | 38818/49018 [00:18<00:06, 1556.58it/s]Loading:  80%|███████▉  | 39040/49018 [00:18<00:05, 1708.42it/s]Loading:  80%|████████  | 39264/49018 [00:18<00:05, 1838.92it/s]Loading:  81%|████████  | 39487/49018 [00:18<00:04, 1939.30it/s]Loading:  81%|████████  | 39710/49018 [00:18<00:04, 2017.82it/s]Loading:  81%|████████▏ | 39935/49018 [00:18<00:04, 2081.43it/s]Loading:  82%|████████▏ | 40158/49018 [00:18<00:04, 2121.27it/s]Loading:  82%|████████▏ | 40380/49018 [00:18<00:04, 2149.63it/s]Loading:  83%|████████▎ | 40603/49018 [00:19<00:03, 2170.35it/s]Loading:  83%|████████▎ | 40827/49018 [00:19<00:03, 2190.73it/s]Loading:  84%|████████▎ | 41052/49018 [00:19<00:03, 2205.81it/s]Loading:  84%|████████▍ | 41276/49018 [00:19<00:03, 2213.01it/s]Loading:  85%|████████▍ | 41501/49018 [00:19<00:03, 2223.48it/s]Loading:  85%|████████▌ | 41725/49018 [00:19<00:03, 2225.86it/s]Loading:  86%|████████▌ | 41949/49018 [00:19<00:03, 2227.83it/s]Loading:  86%|████████▌ | 42173/49018 [00:19<00:03, 2229.67it/s]Loading:  86%|████████▋ | 42398/49018 [00:19<00:02, 2233.43it/s]Loading:  87%|████████▋ | 42623/49018 [00:19<00:02, 2237.88it/s]Loading:  87%|████████▋ | 42847/49018 [00:20<00:02, 2237.17it/s]Loading:  88%|████████▊ | 43073/49018 [00:20<00:02, 2242.39it/s]Loading:  88%|████████▊ | 43298/49018 [00:20<00:02, 2235.08it/s]Loading:  89%|████████▉ | 43522/49018 [00:20<00:02, 2236.20it/s]Loading:  89%|████████▉ | 43746/49018 [00:20<00:02, 2235.42it/s]Loading:  90%|████████▉ | 43970/49018 [00:20<00:02, 2234.64it/s]Loading:  90%|█████████ | 44195/49018 [00:20<00:02, 2237.23it/s]Loading:  91%|█████████ | 44419/49018 [00:20<00:02, 2233.89it/s]Loading:  91%|█████████ | 44644/49018 [00:20<00:01, 2235.73it/s]Loading:  92%|█████████▏| 44869/49018 [00:20<00:01, 2239.83it/s]Loading:  92%|█████████▏| 45093/49018 [00:21<00:01, 2237.69it/s]Loading:  92%|█████████▏| 45318/49018 [00:21<00:01, 2239.58it/s]Loading:  93%|█████████▎| 45542/49018 [00:21<00:01, 2236.72it/s]Loading:  93%|█████████▎| 45767/49018 [00:21<00:01, 2238.88it/s]Loading:  94%|█████████▍| 45992/49018 [00:21<00:01, 2240.10it/s]Loading:  94%|█████████▍| 46217/49018 [00:21<00:01, 2233.59it/s]Loading:  95%|█████████▍| 46441/49018 [00:21<00:01, 2234.99it/s]Loading:  95%|█████████▌| 46665/49018 [00:21<00:01, 2233.22it/s]Loading:  96%|█████████▌| 46890/49018 [00:21<00:00, 2236.50it/s]Loading:  96%|█████████▌| 47115/49018 [00:21<00:00, 2239.46it/s]Loading:  97%|█████████▋| 47339/49018 [00:22<00:00, 2235.51it/s]Loading:  97%|█████████▋| 47564/49018 [00:22<00:00, 2237.51it/s]Loading:  97%|█████████▋| 47788/49018 [00:22<00:00, 2237.62it/s]Loading:  98%|█████████▊| 48014/49018 [00:22<00:00, 2241.86it/s]Loading:  98%|█████████▊| 48239/49018 [00:22<00:00, 2242.71it/s]Loading:  99%|█████████▉| 48464/49018 [00:22<00:00, 2237.20it/s]Loading:  99%|█████████▉| 48688/49018 [00:22<00:00, 2235.05it/s]Loading: 100%|█████████▉| 48912/49018 [00:22<00:00, 2234.05it/s]Loading: 100%|██████████| 49018/49018 [00:22<00:00, 2147.57it/s]
0: DDStore add: ('testset/dataset_name', 0, dtype('int64'), (41304, 1), 0, 0.0003077387809753418)
0: DDStore add: ('testset/edge_attr', 0, dtype('float32'), (8750992, 3), 0.001914978, 0.09779995679855347)
0: DDStore add: ('testset/edge_index', 1, dtype('int64'), (8750992, 2), 162631536, 0.1303999423980713)
0: DDStore add: ('testset/energy', 0, dtype('float32'), (41304, 1), -16276681.0, 0.0001538693904876709)
0: DDStore add: ('testset/forces', 0, dtype('float32'), (633414, 3), -3.3974648e-06, 0.0070789530873298645)
0: DDStore add: ('testset/pos', 0, dtype('float32'), (633414, 3), 33641.5, 0.0070789530873298645)
0: DDStore add: ('testset/x', 0, dtype('float32'), (633414, 4), 2520950.0, 0.00943860411643982)
0: DDStore add: ('testset/y', 0, dtype('float32'), (1941546, 1), -32.35925, 0.007232822477817535)
0: DDStore add: ('testset/y_loc', 0, dtype('int64'), (41304, 3), 1982850, 0.0009232163429260254)
0: DDStore total (GB): 0.2604140564799309
0: trainset,valset,testset size: 2560000 1767766 1767836
0: Using GPU
0: Using GPU
0:    module.graph_convs.0.module_0.edge_mlp.0.weight	            [866, 9]	      7794
0:      module.graph_convs.0.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.0.weight	          [866, 870]	    753420
0:      module.graph_convs.0.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.0.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.0.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.0.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.0.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.1.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.1.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.1.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.1.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.1.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.1.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.2.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.2.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.2.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.2.module_0.node_mlp.2.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.0.weight	          [866, 866]	    749956
0:     module.graph_convs.2.module_0.coord_mlp.0.bias	               [866]	       866
0:   module.graph_convs.2.module_0.coord_mlp.2.weight	            [1, 866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.0.weight	         [866, 1733]	   1500778
0:      module.graph_convs.3.module_0.edge_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.edge_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.edge_mlp.2.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.0.weight	         [866, 1732]	   1499912
0:      module.graph_convs.3.module_0.node_mlp.0.bias	               [866]	       866
0:    module.graph_convs.3.module_0.node_mlp.2.weight	          [866, 866]	    749956
0:      module.graph_convs.3.module_0.node_mlp.2.bias	               [866]	       866
0:                module.heads_NN.0.branch-0.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-0.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-0.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-0.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-0.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-1.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-1.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-1.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-1.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-1.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-2.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-2.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-2.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-2.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-2.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-3.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-3.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-3.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-3.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-3.6.bias	                 [1]	         1
0:                module.heads_NN.0.branch-4.0.weight	           [889, 50]	     44450
0:                  module.heads_NN.0.branch-4.0.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.2.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.2.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.4.weight	          [889, 889]	    790321
0:                  module.heads_NN.0.branch-4.4.bias	               [889]	       889
0:                module.heads_NN.0.branch-4.6.weight	            [1, 889]	       889
0:                  module.heads_NN.0.branch-4.6.bias	                 [1]	         1
0:          module.heads_NN.1.branch-0.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-0.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-0.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-0.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-0.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-1.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-1.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-1.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-1.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-1.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-2.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-2.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-2.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-2.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-2.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-3.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-3.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-3.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-3.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-3.mlp.0.6.bias	                 [3]	         3
0:          module.heads_NN.1.branch-4.mlp.0.0.weight	          [889, 866]	    769874
0:            module.heads_NN.1.branch-4.mlp.0.0.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.2.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.2.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.4.weight	          [889, 889]	    790321
0:            module.heads_NN.1.branch-4.mlp.0.4.bias	               [889]	       889
0:          module.heads_NN.1.branch-4.mlp.0.6.weight	            [3, 889]	      2667
0:            module.heads_NN.1.branch-4.mlp.0.6.bias	                 [3]	         3
0:              module.graph_shared.branch-0.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-0.0.bias	                [50]	        50
0:              module.graph_shared.branch-0.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-0.2.bias	                [50]	        50
0:              module.graph_shared.branch-1.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-1.0.bias	                [50]	        50
0:              module.graph_shared.branch-1.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-1.2.bias	                [50]	        50
0:              module.graph_shared.branch-2.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-2.0.bias	                [50]	        50
0:              module.graph_shared.branch-2.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-2.2.bias	                [50]	        50
0:              module.graph_shared.branch-3.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-3.0.bias	                [50]	        50
0:              module.graph_shared.branch-3.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-3.2.bias	                [50]	        50
0:              module.graph_shared.branch-4.0.weight	           [50, 866]	     43300
0:                module.graph_shared.branch-4.0.bias	                [50]	        50
0:              module.graph_shared.branch-4.2.weight	            [50, 50]	      2500
0:                module.graph_shared.branch-4.2.bias	                [50]	        50
0: --------------------------------------------------
0:                                              Total	                    	  38183862
0: All (total, MB): 38183862 145.66
[rank24]:[W424 18:21:29.912009682 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank31]:[W424 18:21:29.943283074 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank25]:[W424 18:21:29.961894712 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank27]:[W424 18:21:29.964557266 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank28]:[W424 18:21:29.978958191 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank26]:[W424 18:21:29.983001295 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank30]:[W424 18:21:29.983005714 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank16]:[W424 18:21:29.350091245 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank22]:[W424 18:21:29.376829876 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank19]:[W424 18:21:29.377776081 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank17]:[W424 18:21:29.380768926 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank21]:[W424 18:21:29.383541223 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank18]:[W424 18:21:29.384125290 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank36]:[W424 18:21:30.226417385 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank8]:[W424 18:21:30.886920800 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank38]:[W424 18:21:30.228876363 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank34]:[W424 18:21:30.229643526 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank32]:[W424 18:21:30.233705269 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank37]:[W424 18:21:30.237532459 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank39]:[W424 18:21:30.242409687 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank33]:[W424 18:21:30.248975844 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank35]:[W424 18:21:30.256651123 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank10]:[W424 18:21:30.927900751 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank9]:[W424 18:21:30.934822402 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank13]:[W424 18:21:30.942192135 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W424 18:22:17.748394409 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W424 18:22:17.748862317 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank6]:[W424 18:22:17.749345724 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W424 18:22:17.749541996 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W424 18:22:17.750078183 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   0%|          | 0/50 [00:00<?, ?it/s][rank0]:[W424 18:22:17.750260589 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W424 18:22:17.190186992 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank12]:[W424 18:22:17.450662506 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank20]:[W424 18:22:17.079558893 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank14]:[W424 18:22:17.451416771 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank23]:[W424 18:22:17.080572707 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank15]:[W424 18:22:17.451877216 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W424 18:22:17.403123117 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank11]:[W424 18:22:17.664549027 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank29]:[W424 18:22:18.600222658 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Train:   2%|▏         | 1/50 [01:11<58:07, 71.17s/it]Train:   4%|▍         | 2/50 [01:11<23:32, 29.44s/it]Train:   6%|▌         | 3/50 [01:11<12:36, 16.09s/it]Train:   8%|▊         | 4/50 [01:11<07:31,  9.82s/it]Train:  10%|█         | 5/50 [01:12<04:46,  6.36s/it]Train:  12%|█▏        | 6/50 [01:12<03:07,  4.27s/it]Train:  14%|█▍        | 7/50 [01:12<02:06,  2.94s/it]Train:  16%|█▌        | 8/50 [01:12<01:27,  2.07s/it]Train:  18%|█▊        | 9/50 [01:12<01:01,  1.49s/it]Train:  20%|██        | 10/50 [01:13<00:43,  1.10s/it]Train:  22%|██▏       | 11/50 [01:13<00:32,  1.21it/s]Train:  24%|██▍       | 12/50 [01:13<00:24,  1.56it/s]Train:  26%|██▌       | 13/50 [01:13<00:18,  1.96it/s]Train:  28%|██▊       | 14/50 [01:13<00:15,  2.38it/s]Train:  30%|███       | 15/50 [01:14<00:12,  2.80it/s]Train:  32%|███▏      | 16/50 [01:14<00:10,  3.19it/s]Train:  34%|███▍      | 17/50 [01:14<00:09,  3.51it/s]Train:  36%|███▌      | 18/50 [01:14<00:08,  3.82it/s]Train:  38%|███▊      | 19/50 [01:15<00:07,  4.05it/s]Train:  40%|████      | 20/50 [01:15<00:07,  4.22it/s]Train:  42%|████▏     | 21/50 [01:15<00:06,  4.36it/s]Train:  44%|████▍     | 22/50 [01:15<00:06,  4.47it/s]Train:  46%|████▌     | 23/50 [01:15<00:05,  4.55it/s]Train:  48%|████▊     | 24/50 [01:16<00:05,  4.59it/s]Train:  50%|█████     | 25/50 [01:16<00:05,  4.63it/s]Train:  52%|█████▏    | 26/50 [01:16<00:05,  4.63it/s]Train:  54%|█████▍    | 27/50 [01:16<00:04,  4.65it/s]Train:  56%|█████▌    | 28/50 [01:16<00:04,  4.67it/s]Train:  58%|█████▊    | 29/50 [01:17<00:04,  4.69it/s]Train:  60%|██████    | 30/50 [01:17<00:04,  4.68it/s]Train:  62%|██████▏   | 31/50 [01:17<00:04,  4.69it/s]Train:  64%|██████▍   | 32/50 [01:17<00:03,  4.71it/s]Train:  66%|██████▌   | 33/50 [01:17<00:03,  4.69it/s]Train:  68%|██████▊   | 34/50 [01:18<00:03,  4.71it/s]Train:  70%|███████   | 35/50 [01:18<00:03,  4.71it/s]Train:  72%|███████▏  | 36/50 [01:18<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [01:18<00:02,  4.73it/s]Train:  76%|███████▌  | 38/50 [01:19<00:02,  4.71it/s]Train:  78%|███████▊  | 39/50 [01:19<00:02,  4.73it/s]Train:  80%|████████  | 40/50 [01:19<00:02,  4.73it/s]Train:  82%|████████▏ | 41/50 [01:19<00:01,  4.71it/s]Train:  84%|████████▍ | 42/50 [01:19<00:01,  4.73it/s]Train:  86%|████████▌ | 43/50 [01:20<00:01,  4.74it/s]Train:  88%|████████▊ | 44/50 [01:20<00:01,  4.75it/s]Train:  90%|█████████ | 45/50 [01:20<00:01,  4.74it/s]Train:  92%|█████████▏| 46/50 [01:20<00:00,  4.76it/s]Train:  94%|█████████▍| 47/50 [01:20<00:00,  4.54it/s]Train:  96%|█████████▌| 48/50 [01:21<00:00,  4.59it/s]Train:  98%|█████████▊| 49/50 [01:21<00:00,  4.62it/s]Train: 100%|██████████| 50/50 [01:21<00:00,  4.65it/s]Train: 100%|██████████| 50/50 [01:21<00:00,  1.63s/it]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:17,  2.77it/s]Train:   4%|▍         | 2/50 [00:00<00:13,  3.68it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.10it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.31it/s]Train:  10%|█         | 5/50 [00:01<00:10,  4.47it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.57it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.61it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.65it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.68it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.71it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.70it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.71it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.71it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.70it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.69it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.70it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.71it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.73it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.72it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.70it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.70it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.72it/s]Train:  46%|████▌     | 23/50 [00:05<00:05,  4.71it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.73it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.73it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.73it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.69it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.69it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.70it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.71it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.72it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.74it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.73it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.70it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.70it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.70it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.70it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.68it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.69it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.70it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.70it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.70it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.72it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.72it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.71it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.70it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.70it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.68it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.63it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.21it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.98it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.26it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.44it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.58it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.64it/s]Train:  16%|█▌        | 8/50 [00:01<00:08,  4.67it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.70it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.72it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.71it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.70it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.69it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.70it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.70it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.72it/s]Train:  34%|███▍      | 17/50 [00:03<00:06,  4.72it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.72it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.73it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.72it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.70it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.69it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.71it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.69it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.71it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.70it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.71it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.70it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.70it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.72it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.73it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.73it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.71it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.72it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.72it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.72it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.72it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.72it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.72it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.72it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.71it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.71it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.74it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.73it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.74it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.23it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.93it/s]Train:   6%|▌         | 3/50 [00:00<00:11,  4.25it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.42it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.53it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.59it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.63it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.62it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.65it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.68it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.69it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.70it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.70it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.70it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.69it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.67it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.69it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.71it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.71it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.71it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.73it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.74it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.71it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.71it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.70it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.71it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.70it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.70it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.69it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.72it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.73it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.75it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.76it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.75it/s]Train:  72%|███████▏  | 36/50 [00:07<00:02,  4.73it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.72it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.74it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.74it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.74it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.74it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.74it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.72it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.72it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.73it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.76it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.75it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.74it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.73it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.66it/s]
Train:   0%|          | 0/50 [00:00<?, ?it/s]Train:   2%|▏         | 1/50 [00:00<00:15,  3.19it/s]Train:   4%|▍         | 2/50 [00:00<00:12,  3.97it/s]Train:   6%|▌         | 3/50 [00:00<00:10,  4.30it/s]Train:   8%|▊         | 4/50 [00:00<00:10,  4.44it/s]Train:  10%|█         | 5/50 [00:01<00:09,  4.51it/s]Train:  12%|█▏        | 6/50 [00:01<00:09,  4.58it/s]Train:  14%|█▍        | 7/50 [00:01<00:09,  4.61it/s]Train:  16%|█▌        | 8/50 [00:01<00:09,  4.65it/s]Train:  18%|█▊        | 9/50 [00:02<00:08,  4.67it/s]Train:  20%|██        | 10/50 [00:02<00:08,  4.69it/s]Train:  22%|██▏       | 11/50 [00:02<00:08,  4.71it/s]Train:  24%|██▍       | 12/50 [00:02<00:08,  4.72it/s]Train:  26%|██▌       | 13/50 [00:02<00:07,  4.71it/s]Train:  28%|██▊       | 14/50 [00:03<00:07,  4.73it/s]Train:  30%|███       | 15/50 [00:03<00:07,  4.70it/s]Train:  32%|███▏      | 16/50 [00:03<00:07,  4.72it/s]Train:  34%|███▍      | 17/50 [00:03<00:07,  4.71it/s]Train:  36%|███▌      | 18/50 [00:03<00:06,  4.72it/s]Train:  38%|███▊      | 19/50 [00:04<00:06,  4.75it/s]Train:  40%|████      | 20/50 [00:04<00:06,  4.74it/s]Train:  42%|████▏     | 21/50 [00:04<00:06,  4.75it/s]Train:  44%|████▍     | 22/50 [00:04<00:05,  4.74it/s]Train:  46%|████▌     | 23/50 [00:04<00:05,  4.74it/s]Train:  48%|████▊     | 24/50 [00:05<00:05,  4.75it/s]Train:  50%|█████     | 25/50 [00:05<00:05,  4.75it/s]Train:  52%|█████▏    | 26/50 [00:05<00:05,  4.76it/s]Train:  54%|█████▍    | 27/50 [00:05<00:04,  4.76it/s]Train:  56%|█████▌    | 28/50 [00:06<00:04,  4.73it/s]Train:  58%|█████▊    | 29/50 [00:06<00:04,  4.74it/s]Train:  60%|██████    | 30/50 [00:06<00:04,  4.73it/s]Train:  62%|██████▏   | 31/50 [00:06<00:04,  4.75it/s]Train:  64%|██████▍   | 32/50 [00:06<00:03,  4.75it/s]Train:  66%|██████▌   | 33/50 [00:07<00:03,  4.73it/s]Train:  68%|██████▊   | 34/50 [00:07<00:03,  4.70it/s]Train:  70%|███████   | 35/50 [00:07<00:03,  4.39it/s]Train:  72%|███████▏  | 36/50 [00:07<00:03,  4.47it/s]Train:  74%|███████▍  | 37/50 [00:07<00:02,  4.55it/s]Train:  76%|███████▌  | 38/50 [00:08<00:02,  4.59it/s]Train:  78%|███████▊  | 39/50 [00:08<00:02,  4.63it/s]Train:  80%|████████  | 40/50 [00:08<00:02,  4.67it/s]Train:  82%|████████▏ | 41/50 [00:08<00:01,  4.70it/s]Train:  84%|████████▍ | 42/50 [00:09<00:01,  4.71it/s]Train:  86%|████████▌ | 43/50 [00:09<00:01,  4.70it/s]Train:  88%|████████▊ | 44/50 [00:09<00:01,  4.72it/s]Train:  90%|█████████ | 45/50 [00:09<00:01,  4.67it/s]Train:  92%|█████████▏| 46/50 [00:09<00:00,  4.65it/s]Train:  94%|█████████▍| 47/50 [00:10<00:00,  4.67it/s]Train:  96%|█████████▌| 48/50 [00:10<00:00,  4.68it/s]Train:  98%|█████████▊| 49/50 [00:10<00:00,  4.69it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.72it/s]Train: 100%|██████████| 50/50 [00:10<00:00,  4.64it/s]
0: Process 0 - Local timer:  train_validate_test  :  124.84
0: Process 0 - Local timer:  load_data  :  97.17
0: Process 0 - Local timer:  create_model  :  1.33
0: Minimum timers: 
0: train_validate_test  :  124.78
0: load_data  :  97.17
0: create_model  :  1.32
0: Maximum timers: 
0: train_validate_test  :  124.84
0: load_data  :  98.18
0: create_model  :  1.41
0: Average timers: 
0: train_validate_test  :  124.8
0: load_data  :  97.79
0: create_model  :  1.38
0: Number of calls to timers: 
0: {'load_data': 1, 'create_model': 1, 'train_validate_test': 1}
[rank0]:[W424 18:23:13.726396557 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W424 18:23:14.404456299 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W424 18:23:14.404851479 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W424 18:23:14.405357839 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W424 18:23:14.405506932 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W424 18:23:14.405581303 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank26]:[W424 18:23:14.962684100 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank29]:[W424 18:23:14.962711673 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank31]:[W424 18:23:14.962756869 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank25]:[W424 18:23:14.962772448 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank30]:[W424 18:23:14.962863521 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank27]:[W424 18:23:14.962873851 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank24]:[W424 18:23:14.962944354 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank28]:[W424 18:23:14.962781054 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W424 18:23:14.406207401 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W424 18:23:14.406330744 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank17]:[W424 18:23:14.294924790 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank19]:[W424 18:23:14.294926183 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank20]:[W424 18:23:14.294933597 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank21]:[W424 18:23:14.294960488 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank22]:[W424 18:23:14.294988381 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank23]:[W424 18:23:14.294980436 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank34]:[W424 18:23:14.007381586 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank18]:[W424 18:23:14.295089182 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank35]:[W424 18:23:14.007395112 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank36]:[W424 18:23:14.007413106 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank37]:[W424 18:23:14.007413967 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank32]:[W424 18:23:14.007463752 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank39]:[W424 18:23:14.007442531 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank33]:[W424 18:23:14.007383419 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank16]:[W424 18:23:14.295847289 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank38]:[W424 18:23:14.008285298 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank10]:[W424 18:23:14.667617885 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank11]:[W424 18:23:14.667637302 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank14]:[W424 18:23:14.667655427 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank8]:[W424 18:23:14.667695282 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank13]:[W424 18:23:14.667616493 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank15]:[W424 18:23:14.667708267 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank9]:[W424 18:23:14.667762971 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank12]:[W424 18:23:14.667791445 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ set +x
Experiment ended at Thu Apr 24 06:23:17 PM EDT 2025
Interference tests with HydraGNN complete at Thu Apr 24 06:23:17 PM EDT 2025
Killing gaussian
ALL DONE
